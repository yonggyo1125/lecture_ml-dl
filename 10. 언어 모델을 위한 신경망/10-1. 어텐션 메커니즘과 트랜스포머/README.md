# 어텐션 매커니즘과 트랜스포머

## 키워드 정리
- **시퀀스-투-시퀀스 작업**은 시퀀스 데이터를 입력받아 다시 시퀀스 데이터를 출력하는 작업입니다. 자연어 처리 분야에서는 텍스트를 입력받아 또 다른 텍스트를 출력해야 하는 요약이나 번역 등의 작업이 이에 해당됩니다. 전통적으로 시퀀스-투-시퀀스 작업에는 두 개의 신경망을 사용한 인코더-디코더 구조가 널리 사용됩니다. 일반적으로 순환 신경망이 인코더와 디코더에 각각 사용되었지만, 긴 텍스트에서 문맥을 감지하는데 어려움이 있습니다. 
- **어텐션 매커니즘**은 인코더-디코더 구조에 사용된 순환 신경망의 성능을 향상시키기 위해 고안되었습니다. 기존에는 인코더의 마지막 타임스텝에서 출력한 은닉 상태만을 사용해 디코더가 새로운 텍스트를 생성했습니다. 어텐션 매커니즘은 이를 해결하기 위해 모든 타임스텝에서 인코더가 출력한 은닉 상태를 참조합니다. 이를 통해 디코더가 새로운 토큰을 생성할 때 인코더에서 처리한 토큰 중 어떤 토큰에 주의를 기울일지 결정할 수 있습니다.
- **트랜스포머** 모델은 어텐션 매커니즘을 기반으로 한 인코더-디코더 구조에서 순환층을 완전히 제거했습니다. 이를 통해 인코더에서 한 번에 하나의 토큰을 처리하지 않고 입력 텍스트 전체를 한 번에 처리할 수 있습니다. 트랜스포머의 인코더와 디코더는 비슷한 구조를 가지고 있으며 핵심 구성 요소는 멀티 헤드 어텐션, 층 정규화, 잔차 연결, 피드포워드 네트워크입니다. 인코더와 디코더는 각각 동일한 블록을 반복적으로 여러 개 쌓아서 구성됩니다. 인코더와 디코더는 각각 동일한 블록을 반복적으로 여러 개 쌓아서 구성됩니다. 인코더에서 최종적으로 출력한 토큰의 은닉 벡터는 모든 디코더의 두 번째 멀티 헤드 어텐션 층에 키워 값으로 전달됩니다. 이 층을 크로스 어텐션이라고도 부릅니다.
- **멀티 헤드 어텐션**은 트랜스포머 모델의 핵심 구성 요소입니다. 어텐션 메커니즘을 계산하는 헤드를 여러 개 병렬로 구성하고 마지막에 밀집층을 두어 원래 임베딩 차원으로 복원하는 구조를 가집니다. 이 어텐션 메커니즘은 입력 텍스트에 있는 토큰 간의 어텐션 점수를 계산하기 때문에 셀프 어텐션 메커니즘이라고도 부릅니다. 디코더에서 사용하는 첫 번째 멀티헤드 어텐션층은 훈련 시에 미래의 토큰을 사용해 어텐션 점수를 계산 할 수 없도록 마스킹하는 기법을 사용합니다. 그래서 이 층을 마스크드 멀티 헤드 어텐션 층이라고도 부릅니다.

## 순환 신경망을 사용한 인코더-디코더 네트워크
- 9장에서 배운 순환 신경망은 텍스트와 같은 시퀀스 데이터를 효과적으로 처리할 수 있지만 여러 가지 한계를 가지고 있습니다. 대표적으로 시퀀스가 길어질수록 이전에 처리한 데이터를 기억하기 어렵다는 것이죠. 이 문제를 해결하기 위해 `LSTM`과 `GRU` 같은 구조가 개발되었지만, 완벽한 해결책이 되지는 못했습니다. 
- 이러한 한계는 **기계 번역**<sup>machine translation</sup> 애플리케이션에서도 두드러졌습니다. 번역할 문장이 길어질 수록 기존 **RNN** 기반 모델의 번역 품질을 유지하기 어렵습니다. 기계 번역에 사용되는 신경망 구조는 전형적으로 **시퀀스-투-시퀀스**<sup>sequence-to-sequence</sup> 구조를 가지고 있습니다.
- 시퀀스-투-시퀀스 작업은 텍스트를 입력받아 텍스트를 출력하는 작업입니다. 대표적인 예로는 기계 번역과 문서 요약이 있습니다. 이런 작업을 수행하기 위해 보통 인코더-디코더 구조를 사용하며, 다음 그림과 같이 인코더와 디코더에 각각 순환 신경망을 적용합니다.

![스크린샷 2025-04-12 오후 9 28 40](https://github.com/user-attachments/assets/acc8ed58-65fa-48ef-9e09-6239ec1e71d1)

> 그림에서는 이해를 돕기 위해 인코더에 입력되는 텍스트와 디코더에서 출력되는 텍스트의 길이를 동일하게 표현했습니다. 하지만 일반적으로 이 두 텍스트의 길이는 다를 수 있습니다. 

- 위 그림은 기계 번역을 수행하는 인코더-디코더 구조를 보여줍니다. 그림을 보면 인코더 신경망은 입력된 문장을 단어(토큰) 단위로 하나씩 처리하면서 전체 정보를 하나의 은닉 상태에 압축합니다. 그런 다음, 디코더 신경망이 이 은닉 상태를 받아 마찬가지로 한 단어씩 번역된 문장을 생성합니다. 

> 위 그림은 인코더와 디코더가 타임스텝에 따라 순차적으로 동작하는 과정을 펼쳐서 나타낸 것입니다. 세 개의 셀(층)을 가진 신경망을 나타내는 것이 아니니 오해하지 마세요.

- 이런 구조에서는 번역할 문장이 길어질수록 초기에 입력된 내용을 기억하기 어려워집니다. 특히, 디코더 신경망은 인코더의 마지막 은닉 상태만 참고하여 번역을 수행하기 때문에 이런 문제가 더 심해집니다.
- 또한 인코더와 디코더는 텍스트를 한 토큰씩 처리합니다. 입력할 때도 한 토큰씩 받고, 출력할 때도 한 토큰씩 생성해야 하므로 속도가 느립니다. 앞의 그림의 디코더는 "I"라는 토큰을 생성한 후, 인코더의 마지막 은닉 상태와 자신의 은닉 상태를 활용해 "love"를 만듭니다. "I love"를 생성한 후 같은 방식으로 인코더의 마지막 은닉상태와 자신의 은닉 상태를 활용해 "you"를 출력합니다.
- 이처럼 디코더는 이전에 생성한 토큰을 참고하면서 다음 토큰을 생성하는데 이를 **자기회귀 모델**<sup>autoregressive model</sup>이라고 합니다. 이 개념은 순환 신경망을 사용하는 인코더-디코더 구조뿐만 아니라 앞으로 배울 다른 구조에서도 동일하게 적용됩니다. 
- 그런데 2014년, 어텐션 메커니즘이 등장하면서 순환 신경망 기반 기계 번역 애플리케이션의 성능이 크게 개선되었습니다.

## 어텐션 메커니즘

- **어텐션 매커니즘**<sup>attention mechanism</sup>은 순환 신경망 기반 인코더-디코더 모델의 성능을 크게 향상시킨 기술입니다. 기존에는 디코더가 인코더의 마지막 은닉 상태만 참고하여 번역을 수행했지만, 어텐션 메커니즘을 사용하면 인코더의 모든 타임스텝에서 계산된 은닉 상태를 활용할 수 있습니다. 
- 아래 그림을 보면서 어텐션 메커니즘이 어떻게 동작하는지 자세히 살펴보겠습니다.

![스크린샷 2025-04-12 오후 9 52 20](https://github.com/user-attachments/assets/50813d4c-a575-418d-bb71-a6608744bab9)

