# 어텐션 매커니즘과 트랜스포머

## 키워드 정리
- **시퀀스-투-시퀀스 작업**은 시퀀스 데이터를 입력받아 다시 시퀀스 데이터를 출력하는 작업입니다. 자연어 처리 분야에서는 텍스트를 입력받아 또 다른 텍스트를 출력해야 하는 요약이나 번역 등의 작업이 이에 해당됩니다. 전통적으로 시퀀스-투-시퀀스 작업에는 두 개의 신경망을 사용한 인코더-디코더 구조가 널리 사용됩니다. 일반적으로 순환 신경망이 인코더와 디코더에 각각 사용되었지만, 긴 텍스트에서 문맥을 감지하는데 어려움이 있습니다. 
- **어텐션 매커니즘**은 인코더-디코더 구조에 사용된 순환 신경망의 성능을 향상시키기 위해 고안되었습니다. 기존에는 인코더의 마지막 타임스텝에서 출력한 은닉 상태만을 사용해 디코더가 새로운 텍스트를 생성했습니다. 어텐션 매커니즘은 이를 해결하기 위해 모든 타임스텝에서 인코더가 출력한 은닉 상태를 참조합니다. 이를 통해 디코더가 새로운 토큰을 생성할 때 인코더에서 처리한 토큰 중 어떤 토큰에 주의를 기울일지 결정할 수 있습니다.
- **트랜스포머** 모델은 어텐션 매커니즘을 기반으로 한 인코더-디코더 구조에서 순환층을 완전히 제거했습니다. 이를 통해 인코더에서 한 번에 하나의 토큰을 처리하지 않고 입력 텍스트 전체를 한 번에 처리할 수 있습니다. 트랜스포머의 인코더와 디코더는 비슷한 구조를 가지고 있으며 핵심 구성 요소는 멀티 헤드 어텐션, 층 정규화, 잔차 연결, 피드포워드 네트워크입니다. 인코더와 디코더는 각각 동일한 블록을 반복적으로 여러 개 쌓아서 구성됩니다. 인코더와 디코더는 각각 동일한 블록을 반복적으로 여러 개 쌓아서 구성됩니다. 인코더에서 최종적으로 출력한 토큰의 은닉 벡터는 모든 디코더의 두 번째 멀티 헤드 어텐션 층에 키워 값으로 전달됩니다. 이 층을 크로스 어텐션이라고도 부릅니다.
- **멀티 헤드 어텐션**은 트랜스포머 모델의 핵심 구성 요소입니다. 어텐션 메커니즘을 계산하는 헤드를 여러 개 병렬로 구성하고 마지막에 밀집층을 두어 원래 임베딩 차원으로 복원하는 구조를 가집니다. 이 어텐션 메커니즘은 입력 텍스트에 있는 토큰 간의 어텐션 점수를 계산하기 때문에 셀프 어텐션 메커니즘이라고도 부릅니다. 디코더에서 사용하는 첫 번째 멀티헤드 어텐션층은 훈련 시에 미래의 토큰을 사용해 어텐션 점수를 계산 할 수 없도록 마스킹하는 기법을 사용합니다. 그래서 이 층을 마스크드 멀티 헤드 어텐션 층이라고도 부릅니다.

## 순환 신경망을 사용한 인코더-디코더 네트워크
- 9장에서 배운 순환 신경망은 텍스트와 같은 시퀀스 데이터를 효과적으로 처리할 수 있지만 여러 가지 한계를 가지고 있습니다. 대표적으로 시퀀스가 길어질수록 이전에 처리한 데이터를 기억하기 어렵다는 것이죠. 이 문제를 해결하기 위해 `LSTM`과 `GRU` 같은 구조가 개발되었지만, 완벽한 해결책이 되지는 못했습니다. 
- 이러한 한계는 **기계 번역**<sup>machine translation</sup> 애플리케이션에서도 두드러졌습니다. 번역할 문장이 길어질 수록 기존 **RNN** 기반 모델의 번역 품질을 유지하기 어렵습니다. 기계 번역에 사용되는 신경망 구조는 전형적으로 **시퀀스-투-시퀀스**<sup>sequence-to-sequence</sup> 구조를 가지고 있습니다.
- 시퀀스-투-시퀀스 작업은 텍스트를 입력받아 텍스트를 출력하는 작업입니다. 대표적인 예로는 기계 번역과 문서 요약이 있습니다. 이런 작업을 수행하기 위해 보통 인코더-디코더 구조를 사용하며, 다음 그림과 같이 인코더와 디코더에 각각 순환 신경망을 적용합니다.

![스크린샷 2025-04-12 오후 9 28 40](https://github.com/user-attachments/assets/acc8ed58-65fa-48ef-9e09-6239ec1e71d1)

> 그림에서는 이해를 돕기 위해 인코더에 입력되는 텍스트와 디코더에서 출력되는 텍스트의 길이를 동일하게 표현했습니다. 하지만 일반적으로 이 두 텍스트의 길이는 다를 수 있습니다. 

- 위 그림은 기계 번역을 수행하는 인코더-디코더 구조를 보여줍니다. 그림을 보면 인코더 신경망은 입력된 문장을 단어(토큰) 단위로 하나씩 처리하면서 전체 정보를 하나의 은닉 상태에 압축합니다. 그런 다음, 디코더 신경망이 이 은닉 상태를 받아 마찬가지로 한 단어씩 번역된 문장을 생성합니다. 

> 위 그림은 인코더와 디코더가 타임스텝에 따라 순차적으로 동작하는 과정을 펼쳐서 나타낸 것입니다. 세 개의 셀(층)을 가진 신경망을 나타내는 것이 아니니 오해하지 마세요.

- 이런 구조에서는 번역할 문장이 길어질수록 초기에 입력된 내용을 기억하기 어려워집니다. 특히, 디코더 신경망은 인코더의 마지막 은닉 상태만 참고하여 번역을 수행하기 때문에 이런 문제가 더 심해집니다.
- 또한 인코더와 디코더는 텍스트를 한 토큰씩 처리합니다. 입력할 때도 한 토큰씩 받고, 출력할 때도 한 토큰씩 생성해야 하므로 속도가 느립니다. 앞의 그림의 디코더는 "I"라는 토큰을 생성한 후, 인코더의 마지막 은닉 상태와 자신의 은닉 상태를 활용해 "love"를 만듭니다. "I love"를 생성한 후 같은 방식으로 인코더의 마지막 은닉상태와 자신의 은닉 상태를 활용해 "you"를 출력합니다.
- 이처럼 디코더는 이전에 생성한 토큰을 참고하면서 다음 토큰을 생성하는데 이를 **자기회귀 모델**<sup>autoregressive model</sup>이라고 합니다. 이 개념은 순환 신경망을 사용하는 인코더-디코더 구조뿐만 아니라 앞으로 배울 다른 구조에서도 동일하게 적용됩니다. 
- 그런데 2014년, 어텐션 메커니즘이 등장하면서 순환 신경망 기반 기계 번역 애플리케이션의 성능이 크게 개선되었습니다.

## 어텐션 메커니즘

- **어텐션 매커니즘**<sup>attention mechanism</sup>은 순환 신경망 기반 인코더-디코더 모델의 성능을 크게 향상시킨 기술입니다. 기존에는 디코더가 인코더의 마지막 은닉 상태만 참고하여 번역을 수행했지만, 어텐션 메커니즘을 사용하면 인코더의 모든 타임스텝에서 계산된 은닉 상태를 활용할 수 있습니다. 
- 아래 그림을 보면서 어텐션 메커니즘이 어떻게 동작하는지 자세히 살펴보겠습니다.

![스크린샷 2025-04-12 오후 9 52 20](https://github.com/user-attachments/assets/50813d4c-a575-418d-bb71-a6608744bab9)

- 디코더가 두 번째 타임스텝에서 "love"라는 토큰을 생성할 때, 인코더의 모든 타임스텝에서 출력된 은닉 상태를 참고합니다. 즉, 디코더는 단순히 인코더의 마지막 은닉 상태만 활용하는 것이 아니라 각각의 타임스텝에서 생성된 모든 은닉 상태를 참조하여 출력을 만듭니다.

### 어텐션 가중치
- 디코더가 인코더의 은닉 상태를 활용하는 방식은 가중치를 곱하는 형태로 이루어집니다. 디코더는 인코더의 모든 타임스텝에서 생성된 은닉 상태를 동일하게 참고하는 것이 아니라, 각 은닉 상태마다 가중치를 다르게 적용하여 더 중요한 정보를 강조합니다. 이러한 가중치는 다른 모델 파라미터와 마찬가지로 신경망을 훈련하면서 함께 학습됩니다.
- 위 그림에서는 어텐션 가중치를 **a**<sub>1</sub>, **a**<sub>2</sub>, **a**<sub>3</sub>으로 나타냈습니다. 이 값들은 각각 다른 값을 가지며, 디코더의 타임스텝마다 달라질 수 있습니다. 이를 디코더가 타임스텝에서 인코더의 각기 다른 은닉 상태에 주의를 기울인다고 이해할 수 있습니다. 디코더가 입력 토큰마다 중요도를 다르게 부여하는 이런 방식을 어텐션 메커니즘이라고 합니다.

### 어텐션 매커니즘의 장점과 단점
- 어텐션 메커니즘은 긴 텍스트를 처리할 때 정보 손실을 줄이는 데 매우 효과적입니다. 인코더의 모든 타임스텝에서 생성된 은닉 상태를 참고하여 더 정확한 출력을 생성할 수 있기 때문입니다.
- 하지만 이 방식에도 단점이 있습니다. 어텐션 가중치를 계산하기 위해 인코더의 모든 타임스텝에서 생성된 은닉 상태를 저장해야 하므로 연산량이 증가합니다. 따라서 인코더가 처리할 수 있는 타임스텝의 최대 개수를 정해야 하며, 이로 인해 입력 텍스트의 길이가 제한될 수 있습니다. 또한, 어텐션을 사용해도 여전히 한 번에 한 토큰씩 처리해야 하는 한계는 남아 있습니다. 이러한 문제에도 불구하고, 어텐션 매커니즘을 사용한 획기적인 모델이 등장하면서 기계 번역은 물론, 자연어 처리 전 분야에 혁명을 일으켰습니다.

## 트랜스포머
- 어텐션 메커니즘의 효과를 극대화하기 위해 2017년 구글 연구팀은 **트랜스포머**<sup>Transformer</sup>라는 새로운 신경망 구조를 발표했습니다. 이 연구는 "Attention Is All You Need"라는 제목의 논문에서 소개되었으며, 트랜스포머는 논문의 제목처럼 어텐션 메커니즘을 적극적으로 활용합니다. 하지만 어텐션 메커니즘만 사용하는 것이 아니며 다양한 기술을 조합하여 구성됩니다.
- 트랜스포머는 기존의 인코더-디코더 구조를 유지하면서도 순환 신경망을 완전히 제거했다는 특징을 가지고 있습니다. 따라서 입력 텍스트를 한 토큰씩 처리할 필요 없이 한 번에 모두 처리할 수 있습니다.
- 다음 그림은 트랜스포머의 전체적인 구조를 단순화하여 나타낸 것입니다.

![스크린샷 2025-04-12 오후 10 04 36](https://github.com/user-attachments/assets/d18a08d0-1209-4a83-a807-e591a83a4972)

> 설명의 편의를 위해 인코더를 디코더보다 크게 그렸습니다. 앞으로 보게 되겠지만 실제로는 디코더의 구성 요소가 조금 더 많습니다.

- 그림에서 볼 수 있듯이 **인코더**와 **디코더**는 기존의 순환 신경망과 구분하기 위해 사각형으로 표시했습니다. 트랜스포머의 기본 작동 방식을 먼저 살펴보고, 이후에 내부 구조를 더 자세히 알아보겠습니다.

### 트랜스포머의 작동 방식

- 트랜스포머의 인코더는 입력된 텍스트를 한 번에 모두 처리합니다. 기존의 순환 신경망과 달리, 타입 스텝의 개념이 필요하지 않습니다. 어텐션 메커니즘으로 인해 사실상 입력 텍스트의 길이에 제한이 있는 점은 아쉽지만, 한 번에 모두 처리할 수 있어 모델의 처리 속도가 크게 향상됩니다. 

> 앞으로 알아보겠지만, 최근에는 트랜스포머 기반 모델들이 발전하면서 더 긴 문장이나 문서 전체를 처리할 수 있는 방식이 개발되고 있습니다. 

- 인코더에서 처리된 결과는 디코더에 전달되며, 디코더는 이를 바탕으로 번역된 문장을 생성합니다. 기존의 순환 신경망을 사용한 인코더-디코더 구조에서는 디코더가 인코더의 마지막 은닉 상태를 받아 번역을 수행했습니다. 하지만 트랜스포머는 순환 신경망을 사용하지 않으므로, 더 이상 은닉 상태라는 개념을 사용하지 않습니다. 대신, **은닉 벡터**<sup>hidden vector</sup> 또는 **단어 벡터**<sup>word vector</sup>, **임베딩 벡터**<sup>embedding vector</sup>라는 표현을 사용합니다.
- 디코더는 인코더에서 전달받은 은닉 벡터를 활용해 각 타임스텝에서 출력할 토큰을 생성합니다. 기존의 인코더-디코더 모델처럼, 디코더는 이전에 생성된 토큰을 참고하면서 새로운 토큰을 만듭니다. 하지만, 순환 신경망 없이도 이전 출력값을 반영할 수 있는 구조를 갖추고 있습니다.
- 예를 들어, 디코더가 두 번째 타임스텝에서 "love"를 출력하려면, 앞서 생성한 텍스트 "I"를 입력으로 받아야 합니다. 세 번째 타임스텝에서는 "I love"를 입력으로 받아 "you"를 출력하난 식입니다.
- 그림에서 볼 수 있듯이 디코더가 자기회귀 방식으로 작동하는 것은 기존의 순환 신경망을 사용한 인코더-디코더 구조와 동일합니다. 하지만 디코더가 이전 출력값을 활용하는 방식이 다릅니다. 
- 트랜스포머의 전체적인 구조를 알아보았으니, 이제 인코더 내부에서 어떻게 입력 텍스트를 한 번에 처리하는지 알아보겠습니다.


## 셀프 어텐션 메커니즘
- 기존 어텐션 매커니즘은 인코더의 은닉 상태와 디코더의 은닉 상태를 비교해 디코더가 특정 타임스텝에서 어떤 입력 토큰에 집중해야 하는지를 학습합니다. 하지만 트랜스포머에서는 이와 다르게 인코더에 입력되는 토큰만으로 어텐션 가중치를 학습하도록 만들었습니다. 이를 **셀프 어텐션**<sup>self-attention</sup>이라고 합니다.

### 셀프 어텐션의 계산 과정

- 그림으로 차근차근 알아보죠. 먼저 입력 텍스트의 각 토큰을 밀집층에 통과시킵니다. 아래 그림에 있는 밀집층은 모두 같은 층입니다. 7장에서 배웠듯이 밀집층은 한 번에 여러 개의 샘플을 처리할 수 있습니다. 여기서는 이해를 돕기 위해 각각의 토큰이 밀집층을 통과하는 것처럼 그려졌지만, 사실 전체 토큰이 한 번에 밀집층을 통과합니다.

![스크린샷 2025-04-12 오후 10 19 18](https://github.com/user-attachments/assets/3abf0eb7-42be-4073-b556-08f4a4fddec8)

> 사실상 토큰이 바로 입력되는 것이 아니라, 9장에서 배운 단어 임베딩 과정을 거친 후 어텐션 매커니즘에 전달됩니다. 이에 대해서는 뒤에서 더 자세히 알아보겠습니다.

- 밀집층을 통과한 벡터를 **쿼리**<sup>Query</sup> 벡터라고 합니다. 같은 입력 텍스트를 두 번 밀집층에 통과시켜 **키**<sup>key</sup> 벡터를 만듭니다. 여기서 중요한 점은 쿼리를 생성하는 밀집층과 키를 생성하는 밀집층이 서로 다른 층이라는 점입니다. 이를 강조하기 위해 그림에서도 다른 색으로 구분했습니다.




