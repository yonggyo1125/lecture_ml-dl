# 대규모 언어 모델로 텍스트 생성하기

## 키워드 정리

- **EXAONE**은 LG AI 연구원에서 만든 트랜스포머 디코더 기반의 대규모 언어 모델입니다. 3.5버전은 한국어와 영어를 잘 이해하며 비교적 적은 모델 파라미터를 가진 모델 중에서 경쟁력이 있습니다. 최신 LLM에서 채택하는 여러 기술을 사용하고 있습니다. 그룹 쿼리 어텐션, 실루 활성화 함수, RMS 정규화, 로터리 위치 임베딩 등입니다. 
- **토큰 디코딩**은 대규모 언어 모델이 출력한 로짓을 바탕으로 다음 토큰을 선택하는 과정입니다. 가장 기본적인 방법은 로짓을 소프트맥스 함수에 통과시켜 확률로 바꾼 후 이 확률을 기반으로 다음 토큰을 선택합니다. 온도 파라미터를 높이면 비교적 낮은 확률의 토큰이 선택될 가능성을 높일 수 있습니다. 최상위 로짓의 일부 토큰을 선택하는 `top-k` 방식과 누적 확률의 임곗값으로 토큰을 선택하는 `top-p` 방식이 널리 사용됩니다.
- **GPT**는 오픈 AI에서 개발된 트랜스포머 디코더 기반의 대규모 언어 모델입니다. `GPT-2`는 공개 되어 있지만 `GPT-3` 부터는 클로즈드 소스 정책을 유지하고 있습니다. `GPT-3.5`를 기반으로 하는 `ChatGPT`는 인공지능 분야에 큰 영향을 미쳤습니다. 최신 `GPT-4o` 모델은 다양한 작업에서 뛰어난 성능을 내는 모델 중 하나입니다. `ChatGPT` 웹 인터페이스 또는 파이썬 같은 프로그래밍 언어를 위해 제공되는 API를 통해 이런 모델을 사용할 수 있습니다. 

### transformers

- **AutoTokenizer**는 허깅페이스에서 제공하는 사전 훈련된 LLM 모델의 토크나이저를 직접 로드하기 위한 클래스입니다. `from_pretrained()` 클래스 메서드에 허깅페이스 모델 경로를 전달하여 불러올 수 있습니다. 비슷하게 트랜스포머 디코더 기반의 LLM 모델을 불러오려면 `AutoModelForCausalLM` 클래스를 사용합니다. 
- 파이프라인 객체를 호출할 때 다음과 같은 매개변수를 사용할 수 있습니다.
    - `max_new_tokens` 매개변수는 모델이 생성할 최대 토큰 개수를 설정합니다.
    - `return_full_text` 매개변수를 `False`로 지정하면 모델이 생성한 텍스트만 반환합니다. 기본값은 `True` 입니다.
    - `do_sample` 매개변수를 `True`로 지정하면 토큰 확률을 기반으로 다음 토큰을 선택합니다. 기본값은 `False` 입니다.
    - `temperature` 매개변수는 모델이 출력한 로짓의 분포를 조정하는 온도 파라미터입니다. 1.0보다 크면 토큰의 선택 가능성을 고르게 만들고 1.0보다 작으면 높은 확률의 토큰이 선택될 가능성이 더 커집니다. 기본값은 1.0입니다.
    - `top_k` 매개변수는 가장 큰 확률을 가진 토큰 k개를 다음 토큰의 후보로 설정합니다. 기본값은 50이며 이런 디코딩 전략을 `top-k` 샘플링이라고 합니다.
    - `top_p` 매개변수를 1.0보다 작게 설정하면 확률 크기 순으로 토큰을 나열했을 때 누적 확률이 지정한 값을 넘기지 않을 때까지 후보 토큰으로 설정합니다. 기본값은 1.0이며 이런 디코딩 전략을 `top_p` 샘플링이라고 합니다.

### openai

- **OpenAI** 클래스는 오픈 AI의 API호출을 위한 클라이언트 객체를 만듭니다./
    - `api_key` 매개변수에는 오픈 AI에서 발급받은 API 키를 지정합니다. 이 매개변수를 지정하지 않으면 `OPENAI_API_KEY` 환경 변수에 저장되어 값을 이용합니다.
- `OpenAI.chat.completion.create()` 메서드는 채팅 완성 API를 호출하고 GPT 모델의 응답을 반환합니다.
    - `model` 매개변수에 사용할 모델 아이디를 지정합니다.
    - `messages` 매개변수에 모델에게 전달할 대화 메세지를 입력합니다. 멀티모달 모델일 경우 텍스트 외에 이미지나 오디오 등을 전달할 수 있습니다. 
    - `temperature` 매개변수로 0\~2 사이의 온도 파라미터를 조정합니다. 기본값은 1입니다. 
    - `top_p` 매개변수로 `top-p` 샘플링을 설정합니다. 기본값은 1입니다. 
    - `max_completion_tokens` 매개변수로 모델이 생성할 최대 토큰 수를 지정합니다.

## 디코더 기반의 대규모 언어 모델

- 앞에서 보았듯이 `ChatGPT`의 등장 이후 디코더 기반의 대규모 언어 모델(LLM)이 큰 인기를 얻고 있습니다. 디코더 기반 LLM은 텍스트 생성 능력이 특히 뛰어나기 때문에 종종 생성 언어 모델 또는 생성 언어 AI라고도 불립니다. 이러한 모델은 **오픈 소스**<sup>open source</sup> **모델**과 **클로즈드 소스**<sup>closed source</sup> **모델**로 나뉩니다.
- 대표적인 오픈 소스 모델은 다음과 같습니다.
  - 메타의 Llama : https://www.llama.com/
  - 구글의 Gemma : https://ai.google.dev/gemma
  - 마이크로소프트의 Phi : https://azure.microsoft.com/en-us/products/phi/
  - 알리바바의 Qwen : https://qwenlm.github.io/

- 사실 오픈 소스 LLM은 워낙 많아 모두 나열하기 힘듭니다. 특히 인기 있는 모델을 특정 데이터셋으로 다시 미세 튜닝한 변형 모델들도 많습니다. 하지만 메타나 구글과 같은 큰 규모의 회사에서 제공하는 언어 모델은 비교적 높은 성능과 지속적인 지원을 기대할 수 있습니다. 아마도 많은 경우에 여기서 소개하는 모델들이 좋은 출발접이 될 것입니다. 
- 대표적인 클로즈드 소스 모델은 다음과 같습니다.
    - 오픈 AI의 GPT-4 : https://chat.com
    - 엔트로픽(Anthropic)의 Claude : https://claude.ai/
    - 구글의 Gemini : https://gemini.google.com/ 
- 이런 클로즈드 소스 모델들은 모두 웹 인터페이스를 제공하므로, 브라우저에서 다른 사람과 채팅하듯이 모델에게 질문을 하거나 다양한 작업을 요청할 수 있습니다. 클로즈드 소스 LLM은 API 방식도 지원하기 때문에, 이를 활용하면 쉽게 연동하여 자동으로 댓글을 달 수 있습니다.
- 본격적으로 디코더 기반의 모델을 살펴보기 전에, 먼저 가장 높은 성능을 내는 모델을 찾는 방법을 알아보겠습니다. 

## LLM 리더보드
- 대규모 언어 모델의 성능을 비교하는 서비스가 많이 있습니다. 그중에서도 **오픈 LLM 리더보드**<sup>Open LLM leaderboard</sup>와 **LMSYS 챗봇 아레나 리더보드**<sup>LMSYS Chatbot Arena Leaderboard</sup>가 가장 널리 알려져 있습니다.
- 오픈 LLM 리더보드는 허깅페이스에서 제공하는 서비스로, 허깅페이스에 등록된 오픈 소스 LLM의 성능을 비교합니다. 오픈 소스 LLM 리더보드 사이트(https://bit.ly/4gylmPz)에 접속하면 다음과 같은 화면을 확인할 수 있습니다. 

![스크린샷 2025-04-19 오후 10 51 45](https://github.com/user-attachments/assets/ab6e23c3-f68e-48c8-9be2-eae5341a4f38)


- 이 리더보드를 잘 활용하면 특정 작업에 적합한 최신 모델을 쉽게 찾을 수 있습니다. 먼저 화면 아래에 나타난 순위를 살펴보죠. 이 글을 작성하는 시점에는 `calm-3.2-instruct-78b`가 1등을 차지하고 있군요. 모델을 클릭해서 상세 정보를 확인해 보면, 사실 이 모델은 `Qwen 72B` 모델을 미세 튜닝한 버전이라는 점도 알 수 있습니다. 모델의 순위가 어떤 기준으로 매겨졌는지 이해하면 적절한 모델을 선택하는 데 도움이 됩니다. 이 리더보드의 구성 요소를 하나씩 살펴보겠습니다.
- 먼저 목록의 왼쪽의 Type 열에 나타난 아이콘은 모델의 종류를 보여줍니다. 각 아이콘에 대한 설명은 다음과 같습니다.

![스크린샷 2025-04-19 오후 10 51 54](https://github.com/user-attachments/assets/f4a867cb-1df9-47b1-9f47-e53e02d67050)

- Average 열은 모든 벤치마크에서 얻은 점수를 평균한 값입니다. 그 옆의 벤치마크 지표를 차례로 살펴보죠.
  - **IFEval**<sup>Instruction-Following Evaluation</sup>: 약 500개 프롬프트를 선택하여 언어 모델이 프롬프트의 지시를 얼마나 잘 따르는지 평가한 값입니다. 
  - **BBH**<sup>Big Bench Hard</sup>: **빅 벤치**<sup>Big-Bench</sup> 평가의 하위 집합으로 다단계 추론 능력을 평가하는 어려운 과제로 구성되어 있습니다.
  - **MATH**<sup>Mathematics Aptitude Test of Heuristics</sup>: 이름에서 알 수 있듯이 수학 문제 해결 능력을 평가하는 벤치마크 입니다. 12,500개의 문제로 구성되어 있고 대수학, 정수론, 조합, 확률, 미적분 등의 문제를 풀어야 합니다.
  - **GPQA**<sup>Graduate-Level Google-Proof Q&A</sup>: 화학, 생물학, 물리학 분야에서 박사 수준의 448개의 객관식 문제를 푸는 벤치마크입니다.
  - **MUSR**<sup>Multistep Soft Reasoning</sup>: 자연어로 묘사된 추론 문제를 푸는 벤치마크입니다. 예를 들면, 1,000단어 길이의 미스터리 문제를 풀어야 하는 과제입니다.
  - **MMLU-Pro**<sup>Massive Multitask Language Understanding - Professional</sup>: 대규모 언어 모델의 언어 이해와 추론 능력을 평가하기 위한 벤치마크로, 기존의 MMLU보다 더 복잡하고 어려운 12,000개의 문제로 구성되어 있습니다.
  - CO<sub>2</sub> Cost: 모델을 평가하는 데 사용된 전력을 생산하기 위해 배출된 탄소의 양(Kg)입니다.
- 이 리더보드는 기본적으로 Average 열의 값을 기준으로 정렬되어 있습니다. 다른 열을 기준으로 정렬하려면 열 이름 옆에 있는 아이콘을 클릭하여 내림차순이나 오름차순으로 표시할 수 있습니다. 테이블 오른쪽 위에는 `column visibility` 아이콘이 있습니다. 이 아이콘을 클릭하면 표시하고 싶은 열을 추가하거나 제외시킬 수 있습니다. 아이콘을 클릭한 다음 Architecture 버튼을 클릭해 보세요.

![스크린샷 2025-04-20 오후 3 04 47](https://github.com/user-attachments/assets/18fea975-cbcd-49b6-a121-3aaf74d39422)

- 클릭하면 목록에 있는 모델이 사용한 파운데이션 모델 또는 베이스 모델 Architecture 열에 보여줍니다.

![스크린샷 2025-04-20 오후 3 04 54](https://github.com/user-attachments/assets/b214f537-a70d-4389-80b6-0316185d34c8)

- 놀랍게도 이 글을 작성하는 시점에 상위 10개의 모델 모두 Qwen을 사용하고 있습니다. Qwen의 성능이 뛰어나 다양한 작업에서 파운데이션 모델로 채택되고 있다는 것을 잘 보여줍니다.
- 우리는 코랩에서 예제를 실행해야 하므로, 파라미터 개수가 작은 모델을 사용하는 것이 좋습니다. 리더보드 중앙 검색 창 아래에는 `Quick Fliters` 버튼이 있습니다. 모델 파라미터 크기에 따라 순서대로 네 개의 버튼이 제공됩니다.
  - For Edge Devices: 30억 개 이하
  - For Customers: 30억 개 \~ 70억 개
  - Mid-range: 70억 개 \~ 650억 개
  - For the GPU-rich: 650억 개 이상
- 이 중 'For Edge Devices'를 선택하면 다음과 같은 목록이 보여집니다.

![스크린샷 2025-04-20 오후 3 21 55](https://github.com/user-attachments/assets/bf228161-f518-4637-a4ca-dd1ca36dfae2)


- 놀랍게도 이 글을 작성하는 시점에 1위를 차지한 모델은 이 절의 서두에서 소개한 대표적인 네 개의 오픈 소스 LLM이 아닙니다. 가장 높은 점수를 낸 EXAONE-3.5는 2024년 후반 LG AI 연구원에서 공개한 디코더 기반 LLM입니다. 24억 개의 파라미터를 가진 비교적 작은 모델임에도 불구하고 IFEval 점수가 79.5%에 달해서 비슷한 크기의 다른 모델의 성능을 압도하고 있습니다. 이제 EXAONE 모델의 구조적인 특징을 알아보겠습니다. 

> 2025년 3월에 추론 능력이 강화된 EXAONE Deep이 공개되었습니다. 

## EXAONE의 특징

- EXAONE은 국내에서 오픈소스로 공개된 파운데이션 모델로, 한국어와 영여를 잘 이해하며 다양한 작업을 수행할 수 있는 모델로 알려져 있습니다. 이 모델의 3.5 버전은 최신 LLM에서 채택하는 여러 기술을 사용하고 있습니다. 이제 이를 차례대로 살펴보고 전체 모델의 구조를 그림으로 그려보겠습니다. 
- EXAONE은 디코더 기반 트랜스포머 모델입니다. BART처럼 인코더의 출력을 전달받는 크로스 어텐션 층이 없다는 점을 유념하세요.
> 이후에 언급되는 EXAONE은 EXAONE-3.5 버전을 의미합니다. 

- EXAONE은 최신 LLM에서 널리 사용하는 **그룹 쿼리 어텐션**<sup>grouped query attention</sup>을 사용합니다. 이는 멀티 헤드 어텐션의 변형으로, 이를 이해하기 전에 먼저 디코더 토큰을 생성하는 과정을 잠시 되짚어 보죠.
- 디코더는 하나의 토큰을 생성한 후, 그 토큰을 입력의 끝에 이어 붙인 다음 다시 모델에 입력해 다음 토큰을 생성합니다. 이전 절에서 설명했듯이, 이를 자기회귀 모델이라 부릅니다. 이 방식에서는 디코더가 하나의 토큰을 생성할 때마다 이전에 처리했던 토큰들을 매번 다시 계산해야 하므로, 언뜻 보면 계산 낭비처럼 보입니다. 이 문제를 해결하기 위해 어텐션 층에서 키와 값을 캐시에 저장하고 다음 토큰을 생성할 때 재사용하는 기법이 등장했습니다. 그러나 트랜스포머에 입력할 수 있는 최대 입력길이를 늘리려면 캐시의 크기도 자연스럽게 커질 수 밖에 없습니다. 
- 이러한 문제를 해결하기 위해 멀티 헤드 어텐션에서 키와 값을 모든 헤드에서 공유하는 방식이 등장했습니다. 이를 **멀티 쿼리 어텐션**<sup>multi-query attention</sup>이라 부릅니다. 그리고 모든 헤드에서 키와 값을 공유하지 않고, 몇 개의 헤드씩 나눠서 공유하는 방식이 그룹 쿼리 어텐션입니다. 두 방식을 그림으로 나타내면 다음과 같습니다.

![스크린샷 2025-04-20 오후 5 09 14](https://github.com/user-attachments/assets/763d840e-3b93-4aa6-a037-7508e09e5bc8)

- 멀티 쿼리 어텐션과 그룹 쿼리 어텐션은 키와 값을 만드는 밀집층의 개수가 줄어들기 때문에 전체 모델의 파라미터 개수를 줄이는 효과도 있습니다. 그래서 상대적으로 크기가 작은 LLM에서 널리 사용됩니다. EXAONE은 24억, 78억, 320억 파라미터 버전에서 모두 그룹 쿼리 어텐션을 사용합니다.
- 어텐션 층 다음에 등장하는 피드포워드 네트워크에서는 최근 LLM에서 많이 사용되는 **실루**<sup>SiLU</sup> 활성화 함수를 사용합니다. 이 함수는 밀집층의 출력에 시그모이드 함수를 적용한 다음 이 결과에 원래 출력을 다시 곱합니다. 이를 수식으로 나타내면 다음과 같습니다.

![스크린샷 2025-04-20 오후 5 09 22](https://github.com/user-attachments/assets/cf6fae11-2948-4a4d-b6c8-bf9852538478)

- 조금 복잡해 보이지만 다행히 대부분의 딥러닝 프레임워크에서 실루 함수를 제공하고 있기 때문에 다른 활성화 함수를 적용하는 것처럼 손쉽게 적용할 수 있습니다. 실루 함수는 렐루와 비슷한 형태를 가지며, 젤루와 마찬가지로 원점에서도 미분이 가능합니다.
- 일반적으로 실루 함수를 적용할 때 피드포워드 네트워크의 첫 번째 밀집층을 두 개로 나누어 하나는 실루 함수를 적용하고, 다른 하나는 활성화 함수를 적용하지 않습니다. 그 후, 이 두 출력을 곱합니다. 이를 그림으로 나타내면 다음과 같습니다.

![스크린샷 2025-04-20 오후 5 09 28](https://github.com/user-attachments/assets/21bd0abd-2bc8-4424-8a4a-c20acc1dbc7c)

- 또한 층 정규화의 변종인 **RMS 정규화**<sup>root mean square normalization</sup>를 사용합니다. RMS 정규화는 정규화를 할 때 평균을 구하지 않는 방법입니다. 2장에서 표준점수를 계산할 때 입력에서 평균을 빼고 표준편차로 나누었던 것을 기억하시죠? 층 정규화도 기본적으로 이와 같은 방식을 사용합니다. 그런데 입력에서 평균을 빼지 않고, 표준편차를 구할 때도 평균을 사용하지 않는 방법이 RMS 정규화입니다. 간단한 예를 들어 설명해 보죠.
- 다음과 같이 다섯 개의 원소를 가진 배열을 정규화한다고 가정해 보겠습니다. 1에서 5까지의 값으로 이루어져 있으므로 평균은 3입니다. 분산은 각 원소에서 평균을 뺀 후 제곱한 후, 전체 원소 개수로 나누어 계산합니다. 이렇게 계산한 분산을 제곱근하면 표준편차가 됩니다. 정규화를 할 때는 각 원소에서 평균을 빼고 표준편차로 나누어 주면 됩니다.

![스크린샷 2025-04-20 오후 5 26 30](https://github.com/user-attachments/assets/a0987199-97ab-4181-8448-372668294a3d)

- RMS 정규화는 앞의 과정에서 평균을 사용하지 않는 방법입니다. 따라서 그림처럼 분산은 각 원소의 제곱을 모두 더한 다음 전체 원소 개수로 나누면 됩니다. 정규화를 할 떄는 각 원소에서 평균을 빼지 않고 원소에 그대로 표준편차를 나누어 줍니다.

![스크린샷 2025-04-20 오후 5 26 35](https://github.com/user-attachments/assets/630f0757-2bc8-43fa-86aa-89d33b758308)

- RMS 정규화를 사용하면 평균을 계산하지 않아도 되므로 계산 속도가 빠릅니다. 또 평균을 사용해 데이터 중심에 맞추지 않아도 모델의 성능에 큰 영향을 미치지 않는다고 알려져 있습니다. 
- 최근 LLM은 이런 정규화를 어텐션 층 다음이 아니라 어텐션 이전에 두는 경향이 있습니다. EXAONE도 마찬가지로 어텐션 층 이전과 피드포워드 네트워크 이전에 RMS 정규화를 적용합니다. 24억 파라미터 버전을 기준으로 이런 요소를 디코더 블록에 나타내면 다음 그림과 같습니다. 

![스크린샷 2025-04-20 오후 5 30 13](https://github.com/user-attachments/assets/14d42e86-f9db-4fa6-825f-c83fef515d17)

- 디코더 블록은 층 정규화부터 시작됩니다. 24억 파라미터 버전의 은닉 벡터 크기는 2,560입니다. 이를 80개씩 나누어 32개의 헤드에 입력됩니다. 그룹 쿼리 어텐션을 사용하며 키와 값의 헤드는 8개입니다. 어텐션 층이 출력한 벡터의 크기는 다시 2,560이 되고 잔차 연결을 지나 다시 층 정규화를 거칩니다. 
- 이어서 피드포워드 네트워크가 등장합니다. 앞서 설명한 것처럼 실루 활성화 함수를 사용하며 두 개의 밀집층 중에서 하나에만 적용합니다. 두 밀집층은 은닉 벡터의 크기를 늘려 7,168로 만듭니다. 두 밀집층의 결과를 곱한 후 마지막 밀집층에서 은닉 벡터의 크기는 다시 2,560으로 줄어듭니다. 이런 디코더 블록을 30개 쌓습니다. 드럼 24억 파라미터 버전의 EXAONE 전체 구조를 그림으로 그려보죠.

![스크린샷 2025-04-20 오후 5 30 22](https://github.com/user-attachments/assets/3cd5c112-ca88-497c-84dc-d8a98c586b4b)

- 토큰 아이디가 임베딩 층을 통과해 2,560 차원의 벡터로 변환되고 드롭아웃 층을 지납니다. 그다음 앞서 소개한 디코더 블록 30개를 통과합니다. 마지막 층 정규화를 거치고 밀집층을 통과하면서 어휘 사전의 크기인 102,400 크기의 벡터를 출력합니다. 이 출력에 소프트맥스 함수를 적용하면 어휘 사전에 있는 모든 토큰에 대한 확률처럼 생각할 수 있습니다.
- 이번 절에서 BART의 경우 임베딩 층의 가중치를 활용하여 각 토큰에 대한 확률을 구했습니다. 하지만 EXAONE은 별도의 밀집층을 사용하고 있습니다. 또 하나 다른 점을 눈치했나요? 네, 위치 임베딩이 없죠. 사실 EXAONE이 사용하는 위치 임베딩은 디코더 블록의 어텐션 층 안에 포함되어 있습니다.
- EXAONE을 비롯해 최근 LLM에서 널리 사용되는 위치 임베딩 방식은 **로터리 위치 임베딩**<sup>rotary position embedding, RoPE</sup>입니다. 로터리 위치 임베딩은 쿼리와 키 벡터를 서로 다른 각도로 회전합니다. 이렇게 회전시킨 두 벡터를 곱하면 그 결과에 두 벡터의 상대적인 각도 차이를 인코딩할 수 있습니다.
- 기존의 위치 인코딩과 위치 임베딩은 토큰의 절대적인 위치 인덱스를 벡터로 변환하기 때문에 절대 위치 인코딩이라고 부르며 로터리 위치 임베딩은 쿼리와 키의 상대적인 각도 차이를 표현하기 때문에 상대 위치 인코딩이라 부릅니다. 로터리 위치 임베딩은 별도의 위치 임베딩 벡터를 만들지 않기 때문에 계산이 간단하고 트랜스포머 모델의 성능도 향상시킨다고 알려져 있습니다.
- EXAONE의 주요 특징을 알아보았습니다. 그럼 이제 허깅페이스의 `transformers` 패키지를 사용해 코랩에서 EXAONE의 24억 파라미터 버전을 로드해서 사용해 보겠습니다.

## EXAONE-3.5로 상품 질문에 대한 대답 생성하기

- 이번에는 EXAONE-3.5의 24억 파라미터 버전을 사용해 보겠습니다. EXAONE-3.5의 전체 모델은 허깅페이스 사이트(https://bit.ly/4gsiHHF)를 참고하세요.
- EXAONE 모델은 채팅 템플릿을 활용할 때 좋은 결과를 얻을 수 있습니다. 허깅페이스의 `transformers` 패키지를 사용할 때 채팅 템플릿으로 프롬프트를 구성하는 방법은 잠시 후에 소개하겠습니다. 먼저 EXAONE 모델에서 채팅 템플릿을 사용하라면 토크나이저를 별도로 로드해야 한다는 점을 알아 두어야 합니다.
- `pipeline()` 함수를 사용해 모델을 로드할 때 토크나이저도 자동으로 포함되었습니다. 덕분에 파이프라인 객체의 `tokenizer` 속성으로 참조할 수 있었죠. 하지만 토크나이저를 명시적으로 로드하여 `pipeline()` 함수에 전달할 수도 있습니다. 이럴 때 `AutoTokenizer` 클래스를 사용합니다. 이 클래스의 `from_pretrained()` 클래스 메서드를 사용해 `EXAONE`의 토크나이저를 로드해 보겠습니다. 

> AutoModelForCausalLM 클래스를 사용하여 모델도 직접 로드할 수 있습니다. 허깅페이스 transformers 패키지는 각 작업에 맞는 다양한 Auto 클래스를 제공합니다. 전체 목록은 https://huggingface.co/docs/transformers/model_doc/auto 를 참고하세요.

```python
from transformers import AutoTokenizer

exaone_tokenizer = AutoTokenizer.from_pretrained(
    "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct")
```

- 불러올 토크나이저 이름은 모델 이름을 지정할 때와 동일하게 허깅페이스의 경로를 전달하면 됩니다. 그다음 `pipeline()` 함수의 `tokenizer` 매개변수로 `exaone_tokenizer`를 전달합니다.

```python
from transformers import pipeline

pipe = pipeline(task="text-generation",
                model="LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct",
                tokenizer=exaone_tokenizer,
                device=0, trust_remote_code=True)
```

- `tokenizer` 외에도 `trust_remote_code` 매개변수가 추가되었습니다. 허깅페이스에 있는 모델은 `transformers` 패키지의 코드 외에 독자적인 코드로 모델을 정의할 수 있습니다. EXAONE이 바로 이런 경우에 해당합니다. 이런 코드는 실행하기 전에 사용자에게 실행 여부를 뭍게 되는데요. `trust_remote_code`를 `True`로 설정하면 코드를 신뢰한다고 일일이 실행할지 여부를 묻지 않습니다.
- 모델을 로드했으니 이제 채팅 템플릿을 만들어 보겠습니다. 채팅 템플릿은 딕셔너리의 리스트로 구성됩니다. 딕셔너리의 키로는 "role"과 "content"가 있습니다. "role"에는 "system"와 "user"와 같은 대화 상대의 역할을 지정합니다. "content"에는 실제 메세지 내용을 적습니다.
- 예를 들어 다음과 같이 "role"을 "system"으로 지정하고 EXAONE 모델이 어떤 역할을 맡게 되는지를 기술해 보죠. 여기에서는 한빛 마켓에 올라온 문의 글에 자동으로 답변하는 역할입니다. 그다음 "role"을 "user"로 지정하고 실제 상품 문의와 같은 메세지를 적습니다. 이렇게 두 개의 딕셔너리를 만든 다음 리스트로 연결하여 채팅 템플릿 구성을 마칩니다. 

```python
messages = [
    {"role": "system",
     "content": "너는 쇼핑몰 홈페이지에 올라온 질문에 대답하는 Q&A 챗봇이야. \
                 확정적인 답변을 하지 말고 제품 담당자가 정확한 답변을 하기 위해 \
                 시간이 필요하다는 간단하고 친절한 답변을 생성해줘."},
    {"role": "user", "content": "이 다이어리에 내년도 공휴일이 표시되어 있나요?"}
]
``` 

- 이제 파이프라인 객체를 호출할 차례입니다. 호출 방법은 이전 절과 동일하지만 이번에는 너무 긴 텍스트가 생성되지 않도록 `max_new_tokens`를 200으로 지정합니다. 

```python
pipe(messages, max_new_tokens=200)
```

```python
[{'generated_text': [{'role': 'system',
    'content': '너는 쇼핑몰 홈페이지에 올라온 질문에 대답하는 Q&A 챗봇이야.                  확정적인 답변을 하지 말고 제품 담당자가 정확한 답변을 하기 위해                  시간이 필요하다는 간단하고 친절한 답변을 생성해줘.'},
   {'role': 'user', 'content': '이 다이어리에 내년도 공휴일이 표시되어 있나요?'},
   {'role': 'assistant',
    'content': '안녕하세요! 다이어리에 내년의 공휴일이 미리 표시되어 있는지에 대해 정확한 답변을 드리기 위해서는 제품 담당자에게 확인이 필요합니다. 현재로선 직접 확인이 어려우니, 저희가 안내드릴 수 있는 방법으로는 고객센터에 연락하시거나, 제품 페이지 내의 문의 게시판을 통해 질문해 보시는 것이 좋을 것 같습니다. 담당자분께서 빠르게 답변해 주실 거예요! 감사합니다.'}]}]
```

- 생성된 텍스트는 딕셔너리의 리스트 형태로 반환됩니다. 여기서는 하나의 프롬프트만 전달했기 때문에 리스트 안에 하나의 딕셔너리만 담겨 있습니다. 이 딕셔너리 안에는 `generated_text`키와 이 키의 값으로 채팅 템플릿에 포함된 메세지와 함께 `role`이 `assistant`인 항목이 추가되었습니다. 이 항목의 `content` 키에 담긴 내용이 EXAONE이 만든 답변입니다. 생성된 답변을 보면 꽤 놀랍습니다. 텍스트가 자연스러울 뿐만 아니라 시스템 메세지를 잘 이해하고 담당자의 확인이 필요하다는 답변과 함께 고객센터나 제품 페이지 내의 게시판을 활용해 보라고 안내까지 하고 있습니다. 
- 보통 프롬프트로 입력한 내용을 다시 확인할 필요는 없으니 모델이 생성한 텍스트만 출력하려면 `return_full_text` 매개변수를 `False`로 지정합니다.

```python
pipe(messages, max_new_tokens=200, return_full_text=False)
```

```python
[{'generated_text': '안녕하세요! 다이어리에 내년의 공휴일이 미리 표시되어 있는지에 대해 정확한 답변을 드리기 위해서는 제품 담당자에게 확인이 필요합니다. 현재로선 직접 확인이 어려우니, 저희가 안내드릴 수 있는 방법으로는 고객센터에 연락하시거나, 제품 페이지 내의 문의 게시판을 통해 질문해 보시는 것이 좋을 것 같습니다. 담당자분께서 빠르게 답변해 주실 거예요! 감사합니다.'}]
```

- 앞의 두 결과를 보면 출력 내용이 동일합니다. 이는 다음 토큰을 선택할 때 무조건 가장 높은 확률의 토큰을 선택하기 때문입니다. 조금 확률적으로 토큰을 선택하고 싶다면 do_sample 매개변수를 `True`로 지정합니다. 그리고 반환된 결과에서 `generated_text`키만 출력해 보겠습니다. 

> do_sample 매개변수의 기본값은 False입니다.

```python
output = pipe(messages, max_new_tokens=200, return_full_text=False, do_sample=True)
print(output[0]['generated_text'])
```

```python
안녕하세요! 다이어리에 내년의 공휴일 정보가 포함되어 있는지에 대해 자세히 알려드리는 데 시간이 조금 걸릴 수 있습니다. 정확한 내용을 확인하려면 제조사 측이나 쇼핑몰의 고객센터에 연락하시는 것이 가장 확실할 것 같아요. 그럼 편하실 때 다시 연락주시면 도와드리겠습니다! 도움이 되셨길 바랍니다. 😊
```

- do_sample 매개변수를 True로 지정하니 답변이 조금 더 자연스럽게 생성된 것 같습니다. 사람이 직접 대답한 것 같네요. 앞서 모델 구조에서 살펴보았듯 대규모 언어 모델은 마지막에 어휘 사전 크기만큼 확률을 출력합니다. 이 확률을 바탕으로 어떤 토큰을 선택할지 결정하는 과정을 샘플링 전략 또는 디코딩 전략이라고 합니다. 그렇다면 `do_sample` 매개변수는 어떤 역할을 할까요? 자세히 알아보겠습니다.  

> do_sample 매개변수를 사용하면 코드를 실행할 때마다 다른 토큰이 선택될 수 있습니다. 그래서 책에 나온 출력 결과가 깃허브의 노트북과 다를 수 있습니다. 만약 실행할 때마다 동일한 결과를 얻고 싶다면 다음처럼 `set_seed()` 함수를 사용하여 난수 발생의 시드 값을 지정하세요.

```python
from transformers import set_seed

set_seed(42)
```

## 토큰 디코딩 전략
- 지금까지 이해하기 쉽도록 LLM 모델의 디코더가 출력하는 값을 확률이라 설명했습니다. 하지만 실제로 디코더가 출력하는 것은 확률이 아니라, 어휘 사전에 들어 있는 각 토큰에 대한 점수입니다. 4장에서 배운 로지스틱 회귀처럼, 이 점수에 소프트맥스 함수를 적용하면 확률로 변환할 수 있습니다. 보통 소프트맥스 함수를 적용하기 전의 값을 **로짓**<sup>logit</sup>이라 부릅니다.
- 디코더가 출력한 로짓 중에서 어떤 토큰을 선택할지는 LLM 모델이 아닌 `transformers` 같은 딥러닝 프레임워크의 몫입니다. 프레임워크마다 이 과정은 조금씩 다를 수 있기 때문에 같은 모델, 같은 프롬프트, 같은 디코딩 옵션을 사용하더라도 출력된 텍스트가 다를 수 있습니다. 
- 여기에서는 `transformers` 패키지를 기준으로 디코딩 전략을 소개하겠습니다. 다른 패키지에서는 디코딩 순서나 적용 방식이 다를 수 있습니다.
- 가장 간단한 디코딩 방식은 `do_sample` 매개변수가 기본값 `False`일 때입니다. 이 경우 가장 높은 확률을 가진 토큰 하나를 선택합니다. 그렇기 때문에 프롬프트가 같으면 여러 번 실행해도 항상 같은 대답을 얻을 수 있습니다. 이런 전력을 **그리디 서치**<sup>greedy search</sup>라 부릅니다. 
- `transformers` 패키지는 기본적으로 그리디 서치를 사용하며 `do_sample` 매개변수를 `True`로 설정하면 샘플링 전략을 사용합니다. 가장 널리 사용하는 방식인 `top-k` 샘플링과 `top-p` 샘플링을 차례대로 소개하겠습니다. 

### 기본 샘플링
- 디코더가 출력한 로짓 중에서 한 토큰의 로짓이 아주 크다고 생각해 보죠. 예를 들어 다음처럼 다섯개의 토큰에 대한 로짓을 얻었다고 가정해 보겠습니다. 

```python
import numpy as np

logits = np.array([1, 2, 3, 4, 100])
```

- 마지막 값이 다른 네 개의 값보다 월등히 크군요. 이 배열을 소프트맥스 함수에 통과시켜 보겠습니다. 4장에서 했던 것처럼 사이파이의 `softmax()` 함수를 사용해 보죠.

```python
from scipy.special import softmax

probas = softmax(logits)
print(probas)
```

```python
[1.01122149e-43 2.74878501e-43 7.47197234e-43 2.03109266e-42
 1.00000000e+00]
```

- 소프트맥스 함수의 결과를 보면 마지막 원소의 확률이 거의 1에 가깝고 나머지 원소의 확률은 0에 가깝습니다. 이런 확률 분포에서 하나의 토큰을 랜덤하게 선택하면 거의 항상 마지막 원소가 선택될 것 같군요. 넘파이의 `random.multinomial()` 함수를 사용하면 주어진 확률 분포를 바탕으로 샘플링을 실험해 볼 수 있습니다. 여기서는 100번을 실행해 보겠습니다. 

```python
np.random.multinomial(100, probas)
```

```python
array([  0,   0,   0,   0, 100])
```

- 100번을 시도했는데 100번 모두 마지막 원소가 선택되었군요. 처음 네 개의 원소가 선택된 횟수는 모두 0입니다. 이렇게 되면 그리디 서치를 적용하는 것과 별반 다르지 않습니다. 그래서 소프트맥스 함수가 만드는 확률 분포를 조금 변형시켜 주는 기법을 종종 사용합니다. 다음처럼 `logits`을 100으로 나눈 다음 다시 실행해 보겠습니다.

```python
probas = softmax(logits/100)
np.random.multinomial(100, probas)
```

```python
array([22, 13, 10, 12, 43])
```
- 여전히 마지막 원소가 선택되는 경우가 높지만, 이전과 달리 처음 네 개의 원소도 어느 정도 선택됩니다. 이렇게 로직을 1보다 큰 값으로 나누면 확률 붆포가 조금 더 부드러워져 다른 원소가 선택될 가능성이 높아지기 때문입니다. 반대로 1보다 작은 값으로 나누어 주면 확률 분포가 더 결정적으로 바뀌고 가장 큰 로짓을 선택하는 그리디 서치와 비슷하게 동작하게 됩니다.
- 이렇게 선택의 다양성을 증가시키거나 줄이는 역할을 하는 이 값을 온도 파라미터라고 부릅니다(이 용어는 열역학에서 유래되었습니다). 파이프라인 객체를 사용할 때 `temperature` 매개변수로 온도값을 조절할 수 있으며, 기본값은 1입니다. 온도를 높였을 때 어떤 텍스트가 생성되는지 확인해 보겠습니다. 

> `temperature` 매개변수는 0보다 큰 실수여야 합니다. 

```python
output = pipe(messages, max_new_tokens=200, return_full_text=False,
              do_sample=True, temperature=10.0)
print(output[0]['generated_text'])
```

```
저는 해당Qy 관리자 입장이지않그래서요qing 특정 브랜드 또는 제조업의 공휴일 포함사항들 자세힌 조회 권한 없으므로 공식 판매처인 판매처혹은회사 페이지의 Calendar 섹션까지 직접 검색recommension 부탁할답니다 그런 디테일 확인해봐 보아지에 확실이 정확히 될 수 밖니를 알 수도요... 거기엔 언제나 전문가, 예 쇼핑몰측이나 해당 제작 다이어리 관계자하시분들 의견들이 담겨볼 시간 필요 하셨겠다 이해 부탁드립니다 감사바겠! 😉🏰💫♖❕◫★.answer.note#DiaryContent#ChRIONlogicalEntries 정확 내용 제공 위해 담당자 요청 대기 부탁드립니다 🤕‍♀⚄ #NeedAssistantRightApprovalsFirst.handle.💰 ⁳
```

- 출력 내용이 정신이 없네요. 한글과 영어가 섞여 쓰이고, 이모티콘에 특수문자도 너무 많습니다. 아무래도 토큰의 선택 가능성을 고르게 부여하다 보니 적절하지 않은 토큰이 많이 선택된 것 같습니다. 그럼 이번에는 온도 파라미터를 낮춰서 가장 큰 로짓을 가진 토큰에 높은 가능성을 부여해 보겠습닌다. 

```python
output = pipe(messages, max_new_tokens=200, return_full_text=False,
              do_sample=True, temperature=0.001)
print(output[0]['generated_text'])
```

```python
안녕하세요! 다이어리에 내년의 공휴일이 미리 표시되어 있는지에 대해 정확한 답변을 드리기 위해서는 제품 담당자에게 확인이 필요합니다. 현재로선 직접 확인이 어려우니, 저희가 안내드릴 수 있는 방법으로는 고객센터에 연락하시거나, 제품 페이지 내의 문의 게시판을 통해 질문해 보시는 것이 좋을 것 같습니다. 담당자분께서 빠르게 답변해 주실 거예요! 감사합니다.
```

- 온도 파라미터를 1보다 많이 작게 하니 앞서 `do_sample=False`로 생성한 결과와 동일하군요! 마치 그리디 서치를 수행한 것과 다름이 없습니다. `transformers` 패키지는 이렇게 온도 파라미터만으로도 출력의 결과에 영향을 줄 수 있습니다. 이어서 자주 사용되는 두 개의 샘플링 전략을 차례대로 알아보겠습니다. 

### top-k 샘플링

- `top-k` 샘플링은 모델이 출력한 로짓을 기준으로 최상위 k개의 토큰을 선택하는 방법입니다. 이후 선택된 토큰의 로짓만 소프트맥스 함수에 통과시킵니다. 바꾸어 말하면 가능성이 높은 몇 개의 토큰중에서 하나를 선택하는 방법입니다.
- `top-k` 샘플링을 사용하려면 파이프라인 객체를 호출할 때 `top-k` 매개변수를 지정하면 됩니다. 이 매개변수에는 1보다 큰 정수를 지정해야 합니다. 예를 들어 다음처럼 상위 10개의 토큰을 선택하도록 지정해 보겠습니다.

> 일반적으로 top_k는 5\~50 사이의 값을 많이 사용합니다.

```python
output = pipe(messages, max_new_tokens=200, return_full_text=False,
              do_sample=True, top_k=10)
print(output[0]['generated_text'])
```

```
안녕하세요! 다이어리 정보에 대해 궁금하시군요. 죄송하지만, 실시간으로 모든 디테일을 확인하기는 어렵습니다. 정확한 답변을 드리려면 제품 담당자님께 직접 문의하시는 게 가장 좋을 것 같아요. 그분께서는 다이어리의 최신 정보와 공휴일 관련 내용을 자세히 알려드릴 수 있을 거예요. 혹시 연락처나 담당자 정보가 있으시다면 함께 공유해 주시면 더 도움을 드릴 수 있을 것 같습니다! 감사합니다.
```

- `transformers` 패키지는 온도 파라미터를 `top-k` 샘플링이나 이어서 설명할 `top-p` 샘플링보다 먼저 적용합니다. 즉, 모델이 생성한 로짓에 대해 `temperature`를 적용해 분포를 조정한 다음 `top_k`매개변수 값만큼 최상위 k개 로짓을 선택하고, 마지막으로 소프트맥스 함수가 적용됩니다. 
- 앞선 코드에서는 `temperature` 매개변수를 지정하지 않았기 때문에 기본값 1.0이 적용되었습니다. `temperature` 값을 조정하면 모델이 생성하는 텍스트의 다양성을 조정할 수 있습니다. 이번에는 이전과 동일하게 `temperature` 매개변수를 10.0으로 높여 보겠습니다. 

```python
output = pipe(messages, max_new_tokens=200, return_full_text=False,
              do_sample=True, top_k=10, temperature=10.0)
print(output[0]['generated_text'])
```

```
안녕세요. 해당 다이어리 내에 내년도 전체 날짜 범위는 표시되고 그 속에서 정확한 각 국가별로 정해지지 공휴일은 아직 정확하기 확인을 못한 상황임을 죄송히 안내 드리지만용도는 충분히 활용하기 위함이기에 공휴일 날짜는 미리 알려드리는 서비스가 아직 적용되어있지 마세요._Please check directly in the diary by the year specified inside or confirm on one-to-[Your Country/Time Period]: 우리 고객지원으로 바로 도움주시거나 공식 고객서비스 번호에서 정확함 정보 확인 부탁드리요 감사함니다.] 이 정도의 응답 형식으로 이해하였으니 도움되신길 바랍니까?! 궁금하신 건 있으세요 더 물어 보시고 필요 시 전문가님께 연락 부탁드립니다. 🤩감사히!
```

- `temperature` 매개변수를 10.0으로 지정했을 때는 영어와 특수 문자가 섞인 이상한 결과가 나왔습니다. 하지만 이번에는 맞춤법이 좀 틀리긴 했지만 어느 정도 이해할 수 있는 문장이 생성됐습니다. 이는 `temperature` 매개변수로 로짓이 작은 토큰들의 선택 가능성을 높였지만, `top_k` 매개변수를 함께 사용해 최상위 토큰만 선택한 결과로 볼 수 있습니다. 이어서 `top-p` 샘플링 방식을 알아보겠습니다. 

### top-p 샘플링

- top-p 샘플링은 top-k 샘플링과 다르게 최상위 토큰의 개수를 고정하는 것이 아니라 확률 순으로 토큰을 나열한 후 사전에 지정한 확률만큼만 최상위 토큰을 선택하는 방식입니다. 예를 들어, 상위 90%의 확률까지만 선택한다고 가정해 보겠습니다.

> top-p 샘플링을 뉴클리어스 샘플링(nucleus sampling)이라고도 부릅니다.

![스크린샷 2025-04-20 오후 7 22 40](https://github.com/user-attachments/assets/49c54e64-11d2-4608-b7d3-0053509d1c5d)

- top-p 샘플링을 적용하려면 top_p 매개변수에 0.0보다 크고 1.0보다 작은 실숫값을 지정해야 합니다. 
> 일반적으로 top_p는 0.9\~0.95 사이의 값을 많이 사용합니다.

```python
output = pipe(messages, max_new_tokens=200, return_full_text=False,
              do_sample=True, top_p=0.9)
print(output[0]['generated_text'])
```

```
네, 맞습니다! 다이어리에 내년의 공휴일들이 표시되어 있어 날짜를 쉽게 기억하고 계획을 세울 수 있게 도와주고 있어요.  하지만 가장 정확한 정보는 올해의 해당 공휴일 일정을 확인해보시는 게 좋을 것 같아요. 혹시 특정 날짜나 지역에 대한 자세한 내용이 필요하시다면, 직접 고객 서비스팀에 문의해 보시는 것도 좋은 방법이 될 거예요. 저희가 바로 확인해 드리진 못하지만, 필요한 도움을 드릴 수 있도록 최선을 다하겠습니다!
```

- 네, 만족할만한 텍스트가 생성되었군요. top_p=0.9로 지정했다는 것은 LLM이 다음 토큰을 선택할 때 모두 합쳐서 90% 확률 안에 드는 토큰만 사용했다는 의미입니다. top-k 방식은 최상위 토큰 개수를 고정하기 때문에 비교적 높은 확률을 가진 토큰입에도 불구하고 선택에서 제외될 가능성이 있습니다. 아에 반해 top-p 샘플링은 대상 토큰을 누적 확률로 지정하기 때문에 다양한 확률 분포에 유연하게 대처할 수 있습니다. 
- 그런데 `top-p` 샘플링을 하려면 소프트맥스 함수를 사용해 로짓을 확률로 바꿔야 합니다. 그다음 확률을 기준으로 토큰을 선택하고, 이 토큰들의 로짓을 다시 소프트맥스 함수에 통과시켜서 최종 토큰 확률을 계산합니다. 복잡하군요. `transformers` 패키지에서 `top-p` 샘플링을 사용했을 때 LLM의 출력부터 최종 토큰 확률을 만드는 것까지 그림으로 나타내면 다음과 같습니다.

![스크린샷 2025-04-20 오후 7 30 06](https://github.com/user-attachments/assets/94f263f4-f9ab-4324-8e86-4c0bb7917788)


- 엇, 그런데 전체 토큰의 로짓에 소프트맥스 함수를 적용하는 것이 왠지 계산량이 많이 들 것 같군요.

- EXAONE의 경우 출력되는 로짓 개수는 어휘 사전의 크기와 같아서 무려 102,400개나 되니까요. top-p 샘플링이 유용하지만 top-k 방식에 비해 계산량이 늘어나는 단점이 있습니다. 이런 경우 top-k와 함께 사용하면 좋습니다. 예를 들어 top-k로 먼저 최상위 로짓을 일부 선택한 다음 top-p 방식을 적용하는 거죠.
- 다행이 `transformers` 패키지는 `top-k`와 `top-p`방식을 동시에 사용할 수 있으며 그 중에 `top-k`가 로짓에 먼저 적용됩니다. `top-k` 방식을 포함하여 전체 진행 과정을 그림으로 나타내면 다음과 같습니다. 

> 다시 언급하지만, 이 그림은 `transdormers` 패키지가 수행하는 방식을 나타낸 것입니다. 패키지마다 샘플링 순서나 방식이 다를 수 있습니다.

![스크린샷 2025-04-20 오후 7 30 18](https://github.com/user-attachments/assets/28bb9afe-eb75-4135-b4b7-26a8861aae83)

- 그럼 top_k 매개변수와 top_p 매개변수를 함께 사용해 보겠습니다.

```python
output = pipe(messages, max_new_tokens=200, return_full_text=False,
              do_sample=True, temperature=0.8, top_k=100, top_p=0.9)
print(output[0]['generated_text'])
```

```
안녕하세요! 다이어리의 공휴일 정보에 대해 궁금하시군요. 정확한 답변을 드리기 위해서는 제품 담당자가 해당 다이어리의 상세 내용을 확인해야 합니다. 아마도 내년의 공휴일 정보가 다이어리 내부에 포함되어 있을 수 있지만, 가장 확실한 답변을 위해서는 저희가 직접 확인해야 할 것 같아요. 담당자분께 문의하시면 더 빠르게 도와드리실 수 있을 것 같습니다. 감사합니다!
```
- 위 코드에서는 `temperature` 매개변수로 로짓의 값을 먼저 조정한 후, top_k 매개변수를 사용해 비교적 많은 개수의 토큰을 먼저 선택합니다. 그다음 top-p 샘플링을 적용한 것입니다.