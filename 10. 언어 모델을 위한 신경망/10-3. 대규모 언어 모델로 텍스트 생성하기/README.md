# 대규모 언어 모델로 텍스트 생성하기

## 키워드 정리

- **EXAONE**은 LG AI 연구원에서 만든 트랜스포머 디코더 기반의 대규모 언어 모델입니다. 3.5버전은 한국어와 영어를 잘 이해하며 비교적 적은 모델 파라미터를 가진 모델 중에서 경쟁력이 있습니다. 최신 LLM에서 채택하는 여러 기술을 사용하고 있습니다. 그룹 쿼리 어텐션, 실루 활성화 함수, RMS 정규화, 로터리 위치 임베딩 등입니다. 
- **토큰 디코딩**은 대규모 언어 모델이 출력한 로짓을 바탕으로 다음 토큰을 선택하는 과정입니다. 가장 기본적인 방법은 로짓을 소프트맥스 함수에 통과시켜 확률로 바꾼 후 이 확률을 기반으로 다음 토큰을 선택합니다. 온도 파라미터를 높이면 비교적 낮은 확률의 토큰이 선택될 가능성을 높일 수 있습니다. 최상위 로짓의 일부 토큰을 선택하는 `top-k` 방식과 누적 확률의 임곗값으로 토큰을 선택하는 `top-p` 방식이 널리 사용됩니다.
- **GPT**는 오픈 AI에서 개발된 트랜스포머 디코더 기반의 대규모 언어 모델입니다. `GPT-2`는 공개 되어 있지만 `GPT-3` 부터는 클로즈드 소스 정책을 유지하고 있습니다. `GPT-3.5`를 기반으로 하는 `ChatGPT`는 인공지능 분야에 큰 영향을 미쳤습니다. 최신 `GPT-4o` 모델은 다양한 작업에서 뛰어난 성능을 내는 모델 중 하나입니다. `ChatGPT` 웹 인터페이스 또는 파이썬 같은 프로그래밍 언어를 위해 제공되는 API를 통해 이런 모델을 사용할 수 있습니다. 

### transformers

- **AutoTokenizer**는 허깅페이스에서 제공하는 사전 훈련된 LLM 모델의 토크나이저를 직접 로드하기 위한 클래스입니다. `from_pretrained()` 클래스 메서드에 허깅페이스 모델 경로를 전달하여 불러올 수 있습니다. 비슷하게 트랜스포머 디코더 기반의 LLM 모델을 불러오려면 `AutoModelForCausalLM` 클래스를 사용합니다. 
- 파이프라인 객체를 호출할 때 다음과 같은 매개변수를 사용할 수 있습니다.
    - `max_new_tokens` 매개변수는 모델이 생성할 최대 토큰 개수를 설정합니다.
    - `return_full_text` 매개변수를 `False`로 지정하면 모델이 생성한 텍스트만 반환합니다. 기본값은 `True` 입니다.
    - `do_sample` 매개변수를 `True`로 지정하면 토큰 확률을 기반으로 다음 토큰을 선택합니다. 기본값은 `False` 입니다.
    - `temperature` 매개변수는 모델이 출력한 로짓의 분포를 조정하는 온도 파라미터입니다. 1.0보다 크면 토큰의 선택 가능성을 고르게 만들고 1.0보다 작으면 높은 확률의 토큰이 선택될 가능성이 더 커집니다. 기본값은 1.0입니다.
    - `top_k` 매개변수는 가장 큰 확률을 가진 토큰 k개를 다음 토큰의 후보로 설정합니다. 기본값은 50이며 이런 디코딩 전략을 `top-k` 샘플링이라고 합니다.
    - `top_p` 매개변수를 1.0보다 작게 설정하면 확률 크기 순으로 토큰을 나열했을 때 누적 확률이 지정한 값을 넘기지 않을 때까지 후보 토큰으로 설정합니다. 기본값은 1.0이며 이런 디코딩 전략을 `top_p` 샘플링이라고 합니다.

### openai

- **OpenAI** 클래스는 오픈 AI의 API호출을 위한 클라이언트 객체를 만듭니다./
    - `api_key` 매개변수에는 오픈 AI에서 발급받은 API 키를 지정합니다. 이 매개변수를 지정하지 않으면 `OPENAI_API_KEY` 환경 변수에 저장되어 값을 이용합니다.
- `OpenAI.chat.completion.create()` 메서드는 채팅 완성 API를 호출하고 GPT 모델의 응답을 반환합니다.
    - `model` 매개변수에 사용할 모델 아이디를 지정합니다.
    - `messages` 매개변수에 모델에게 전달할 대화 메세지를 입력합니다. 멀티모달 모델일 경우 텍스트 외에 이미지나 오디오 등을 전달할 수 있습니다. 
    - `temperature` 매개변수로 0\~2 사이의 온도 파라미터를 조정합니다. 기본값은 1입니다. 
    - `top_p` 매개변수로 `top-p` 샘플링을 설정합니다. 기본값은 1입니다. 
    - `max_completion_tokens` 매개변수로 모델이 생성할 최대 토큰 수를 지정합니다.

## 디코더 기반의 대규모 언어 모델

- 앞에서 보았듯이 `ChatGPT`의 등장 이후 디코더 기반의 대규모 언어 모델(LLM)이 큰 인기를 얻고 있습니다. 디코더 기반 LLM은 텍스트 생성 능력이 특히 뛰어나기 때문에 종종 생성 언어 모델 또는 생성 언어 AI라고도 불립니다. 이러한 모델은 **오픈 소스**<sup>open source</sup> **모델**과 **클로즈드 소스**<sup>closed source</sup> **모델**로 나뉩니다.
- 대표적인 오픈 소스 모델은 다음과 같습니다.
  - 메타의 Llama : https://www.llama.com/
  - 구글의 Gemma : https://ai.google.dev/gemma
  - 마이크로소프트의 Phi : https://azure.microsoft.com/en-us/products/phi/
  - 알리바바의 Qwen : https://qwenlm.github.io/

- 사실 오픈 소스 LLM은 워낙 많아 모두 나열하기 힘듭니다. 특히 인기 있는 모델을 특정 데이터셋으로 다시 미세 튜닝한 변형 모델들도 많습니다. 하지만 메타나 구글과 같은 큰 규모의 회사에서 제공하는 언어 모델은 비교적 높은 성능과 지속적인 지원을 기대할 수 있습니다. 아마도 많은 경우에 여기서 소개하는 모델들이 좋은 출발접이 될 것입니다. 
- 대표적인 클로즈드 소스 모델은 다음과 같습니다.
    - 오픈 AI의 GPT-4 : https://chat.com
    - 엔트로픽(Anthropic)의 Claude : https://claude.ai/
    - 구글의 Gemini : https://gemini.google.com/ 
- 이런 클로즈드 소스 모델들은 모두 웹 인터페이스를 제공하므로, 브라우저에서 다른 사람과 채팅하듯이 모델에게 질문을 하거나 다양한 작업을 요청할 수 있습니다. 클로즈드 소스 LLM은 API 방식도 지원하기 때문에, 이를 활용하면 쉽게 연동하여 자동으로 댓글을 달 수 있습니다.
- 본격적으로 디코더 기반의 모델을 살펴보기 전에, 먼저 가장 높은 성능을 내는 모델을 찾는 방법을 알아보겠습니다. 

## LLM 리더보드
- 대규모 언어 모델의 성능을 비교하는 서비스가 많이 있습니다. 그중에서도 **오픈 LLM 리더보드**<sup>Open LLM leaderboard</sup>와 **LMSYS 챗봇 아레나 리더보드**<sup>LMSYS Chatbot Arena Leaderboard</sup>가 가장 널리 알려져 있습니다.
- 오픈 LLM 리더보드는 허깅페이스에서 제공하는 서비스로, 허깅페이스에 등록된 오픈 소스 LLM의 성능을 비교합니다. 오픈 소스 LLM 리더보드 사이트(https://bit.ly/4gylmPz)에 접속하면 다음과 같은 화면을 확인할 수 있습니다. 

![스크린샷 2025-04-19 오후 10 51 45](https://github.com/user-attachments/assets/ab6e23c3-f68e-48c8-9be2-eae5341a4f38)


- 이 리더보드를 잘 활용하면 특정 작업에 적합한 최신 모델을 쉽게 찾을 수 있습니다. 먼저 화면 아래에 나타난 순위를 살펴보죠. 이 글을 작성하는 시점에는 `calm-3.2-instruct-78b`가 1등을 차지하고 있군요. 모델을 클릭해서 상세 정보를 확인해 보면, 사실 이 모델은 `Qwen 72B` 모델을 미세 튜닝한 버전이라는 점도 알 수 있습니다. 모델의 순위가 어떤 기준으로 매겨졌는지 이해하면 적절한 모델을 선택하는 데 도움이 됩니다. 이 리더보드의 구성 요소를 하나씩 살펴보겠습니다.
- 먼저 목록의 왼쪽의 Type 열에 나타난 아이콘은 모델의 종류를 보여줍니다. 각 아이콘에 대한 설명은 다음과 같습니다.

![스크린샷 2025-04-19 오후 10 51 54](https://github.com/user-attachments/assets/f4a867cb-1df9-47b1-9f47-e53e02d67050)

- Average 열은 모든 벤치마크에서 얻은 점수를 평균한 값입니다. 그 옆의 벤치마크 지표를 차례로 살펴보죠.
  - **IFEval**<sup>Instruction-Following Evaluation</sup>: 약 500개 프롬프트를 선택하여 언어 모델이 프롬프트의 지시를 얼마나 잘 따르는지 평가한 값입니다. 
  - **BBH**<sup>Big Bench Hard</sup>: **빅 벤치**<sup>Big-Bench</sup> 평가의 하위 집합으로 다단계 추론 능력을 평가하는 어려운 과제로 구성되어 있습니다.
  - **MATH**<sup>Mathematics Aptitude Test of Heuristics</sup>: 이름에서 알 수 있듯이 수학 문제 해결 능력을 평가하는 벤치마크 입니다. 12,500개의 문제로 구성되어 있고 대수학, 정수론, 조합, 확률, 미적분 등의 문제를 풀어야 합니다.
  - **GPQA**<sup>Graduate-Level Google-Proof Q&A</sup>: 화학, 생물학, 물리학 분야에서 박사 수준의 448개의 객관식 문제를 푸는 벤치마크입니다.
  - **MUSR**<sup>Multistep Soft Reasoning</sup>: 자연어로 묘사된 추론 문제를 푸는 벤치마크입니다. 예를 들면, 1,000단어 길이의 미스터리 문제를 풀어야 하는 과제입니다.
  - **MMLU-Pro**<sup>Massive Multitask Language Understanding - Professional</sup>: 대규모 언어 모델의 언어 이해와 추론 능력을 평가하기 위한 벤치마크로, 기존의 MMLU보다 더 복잡하고 어려운 12,000개의 문제로 구성되어 있습니다.
  - CO<sub>2</sub> Cost: 모델을 평가하는 데 사용된 전력을 생산하기 위해 배출된 탄소의 양(Kg)입니다.
- 이 리더보드는 기본적으로 Average 열의 값을 기준으로 정렬되어 있습니다. 다른 열을 기준으로 정렬하려면 열 이름 옆에 있는 아이콘을 클릭하여 내림차순이나 오름차순으로 표시할 수 있습니다. 테이블 오른쪽 위에는 `column visibility` 아이콘이 있습니다. 이 아이콘을 클릭하면 표시하고 싶은 열을 추가하거나 제외시킬 수 있습니다. 아이콘을 클릭한 다음 Architecture 버튼을 클릭해 보세요.

![스크린샷 2025-04-20 오후 3 04 47](https://github.com/user-attachments/assets/18fea975-cbcd-49b6-a121-3aaf74d39422)

- 클릭하면 목록에 있는 모델이 사용한 파운데이션 모델 또는 베이스 모델 Architecture 열에 보여줍니다.

![스크린샷 2025-04-20 오후 3 04 54](https://github.com/user-attachments/assets/b214f537-a70d-4389-80b6-0316185d34c8)

- 놀랍게도 이 글을 작성하는 시점에 상위 10개의 모델 모두 Qwen을 사용하고 있습니다. Qwen의 성능이 뛰어나 다양한 작업에서 파운데이션 모델로 채택되고 있다는 것을 잘 보여줍니다.
- 우리는 코랩에서 예제를 실행해야 하므로, 파라미터 개수가 작은 모델을 사용하는 것이 좋습니다. 리더보드 중앙 검색 창 아래에는 `Quick Fliters` 버튼이 있습니다. 모델 파라미터 크기에 따라 순서대로 네 개의 버튼이 제공됩니다.
  - For Edge Devices: 30억 개 이하
  - For Customers: 30억 개 \~ 70억 개
  - Mid-range: 70억 개 \~ 650억 개
  - For the GPU-rich: 650억 개 이상
- 이 중 'For Edge Devices'를 선택하면 다음과 같은 목록이 보여집니다.

![스크린샷 2025-04-20 오후 3 21 55](https://github.com/user-attachments/assets/bf228161-f518-4637-a4ca-dd1ca36dfae2)


- 놀랍게도 이 글을 작성하는 시점에 1위를 차지한 모델은 이 절의 서두에서 소개한 대표적인 네 개의 오픈 소스 LLM이 아닙니다. 가장 높은 점수를 낸 EXAONE-3.5는 2024년 후반 LG AI 연구원에서 공개한 디코더 기반 LLM입니다. 24억 개의 파라미터를 가진 비교적 작은 모델임에도 불구하고 IFEval 점수가 79.5%에 달해서 비슷한 크기의 다른 모델의 성능을 압도하고 있습니다. 이제 EXAONE 모델의 구조적인 특징을 알아보겠습니다. 

> 2025년 3월에 추론 능력이 강화된 EXAONE Deep이 공개되었습니다. 

## EXAONE의 특징

- EXAONE은 국내에서 오픈소스로 공개된 파운데이션 모델로, 한국어와 영여를 잘 이해하며 다양한 작업을 수행할 수 있는 모델로 알려져 있습니다. 이 모델의 3.5 버전은 최신 LLM에서 채택하는 여러 기술을 사용하고 있습니다. 이제 이를 차례대로 살펴보고 전체 모델의 구조를 그림으로 그려보겠습니다. 
- EXAONE은 디코더 기반 트랜스포머 모델입니다. BART처럼 인코더의 출력을 전달받는 크로스 어텐션 층이 없다는 점을 유념하세요.
> 이후에 언급되는 EXAONE은 EXAONE-3.5 버전을 의미합니다. 

- EXAONE은 최신 LLM에서 널리 사용하는 **그룹 쿼리 어텐션**<sup>grouped query attention</sup>을 사용합니다. 이는 멀티 헤드 어텐션의 변형으로, 이를 이해하기 전에 먼저 디코더 토큰을 생성하는 과정을 잠시 되짚어 보죠.
- 디코더는 하나의 토큰을 생성한 후, 그 토큰을 입력의 끝에 이어 붙인 다음 다시 모델에 입력해 다음 토큰을 생성합니다. 이전 절에서 설명했듯이, 이를 자기회귀 모델이라 부릅니다. 이 방식에서는 디코더가 하나의 토큰을 생성할 때마다 이전에 처리했던 토큰들을 매번 다시 계산해야 하므로, 언뜻 보면 계산 낭비처럼 보입니다. 이 문제를 해결하기 위해 어텐션 층에서 키와 값을 캐시에 저장하고 다음 토큰을 생성할 때 재사용하는 기법이 등장했습니다. 그러나 트랜스포머에 입력할 수 있는 최대 입력길이를 늘리려면 캐시의 크기도 자연스럽게 커질 수 밖에 없습니다. 
- 이러한 문제를 해결하기 위해 멀티 헤드 어텐션에서 키와 값을 모든 헤드에서 공유하는 방식이 등장했습니다. 이를 **멀티 쿼리 어텐션**<sup>multi-query attention</sup>이라 부릅니다. 그리고 모든 헤드에서 키와 값을 공유하지 않고, 몇 개의 헤드씩 나눠서 공유하는 방식이 그룹 쿼리 어텐션입니다. 두 방식을 그림으로 나타내면 다음과 같습니다.

![스크린샷 2025-04-20 오후 5 09 14](https://github.com/user-attachments/assets/763d840e-3b93-4aa6-a037-7508e09e5bc8)

- 멀티 쿼리 어텐션과 그룹 쿼리 어텐션은 키와 값을 만드는 밀집층의 개수가 줄어들기 때문에 전체 모델의 파라미터 개수를 줄이는 효과도 있습니다. 그래서 상대적으로 크기가 작은 LLM에서 널리 사용됩니다. EXAONE은 24억, 78억, 320억 파라미터 버전에서 모두 그룹 쿼리 어텐션을 사용합니다.
- 어텐션 층 다음에 등장하는 피드포워드 네트워크에서는 최근 LLM에서 많이 사용되는 **실루**<sup>SiLU</sup> 활성화 함수를 사용합니다. 이 함수는 밀집층의 출력에 시그모이드 함수를 적용한 다음 이 결과에 원래 출력을 다시 곱합니다. 이를 수식으로 나타내면 다음과 같습니다.

![스크린샷 2025-04-20 오후 5 09 22](https://github.com/user-attachments/assets/cf6fae11-2948-4a4d-b6c8-bf9852538478)

- 조금 복잡해 보이지만 다행히 대부분의 딥러닝 프레임워크에서 실루 함수를 제공하고 있기 때문에 다른 활성화 함수를 적용하는 것처럼 손쉽게 적용할 수 있습니다. 실루 함수는 렐루와 비슷한 형태를 가지며, 젤루와 마찬가지로 원점에서도 미분이 가능합니다.
- 일반적으로 실루 함수를 적용할 때 피드포워드 네트워크의 첫 번째 밀집층을 두 개로 나누어 하나는 실루 함수를 적용하고, 다른 하나는 활성화 함수를 적용하지 않습니다. 그 후, 이 두 출력을 곱합니다. 이를 그림으로 나타내면 다음과 같습니다.

![스크린샷 2025-04-20 오후 5 09 28](https://github.com/user-attachments/assets/21bd0abd-2bc8-4424-8a4a-c20acc1dbc7c)

- 또한 층 정규화의 변종인 **RMS 정규화**<sup>root mean square normalization</sup>를 사용합니다. RMS 정규화는 정규화를 할 때 평균을 구하지 않는 방법입니다. 2장에서 표준점수를 계산할 때 입력에서 평균을 빼고 표준편차로 나누었던 것을 기억하시죠? 층 정규화도 기본적으로 이와 같은 방식을 사용합니다. 그런데 입력에서 평균을 빼지 않고, 표준편차를 구할 때도 평균을 사용하지 않는 방법이 RMS 정규화입니다. 간단한 예를 들어 설명해 보죠.
- 다음과 같이 다섯 개의 원소를 가진 배열을 정규화한다고 가정해 보겠습니다. 1에서 5까지의 값으로 이루어져 있으므로 평균은 3입니다. 분산은 각 원소에서 평균을 뺀 후 제곱한 후, 전체 원소 개수로 나누어 계산합니다. 이렇게 계산한 분산을 제곱근하면 표준편차가 됩니다. 정규화를 할 때는 각 원소에서 평균을 빼고 표준편차로 나누어 주면 됩니다.

![스크린샷 2025-04-20 오후 5 26 30](https://github.com/user-attachments/assets/a0987199-97ab-4181-8448-372668294a3d)

- RMS 정규화는 앞의 과정에서 평균을 사용하지 않는 방법입니다. 따라서 그림처럼 분산은 각 원소의 제곱을 모두 더한 다음 전체 원소 개수로 나누면 됩니다. 정규화를 할 떄는 각 원소에서 평균을 빼지 않고 원소에 그대로 표준편차를 나누어 줍니다.

![스크린샷 2025-04-20 오후 5 26 35](https://github.com/user-attachments/assets/630f0757-2bc8-43fa-86aa-89d33b758308)

- RMS 정규화를 사용하면 평균을 계산하지 않아도 되므로 계산 속도가 빠릅니다. 또 평균을 사용해 데이터 중심에 맞추지 않아도 모델의 성능에 큰 영향을 미치지 않는다고 알려져 있습니다. 
- 최근 LLM은 이런 정규화를 어텐션 층 다음이 아니라 어텐션 이전에 두는 경향이 있습니다. EXAONE도 마찬가지로 어텐션 층 이전과 피드포워드 네트워크 이전에 RMS 정규화를 적용합니다. 24억 파라미터 버전을 기준으로 이런 요소를 디코더 블록에 나타내면 다음 그림과 같습니다. 

![스크린샷 2025-04-20 오후 5 30 13](https://github.com/user-attachments/assets/14d42e86-f9db-4fa6-825f-c83fef515d17)

- 디코더 블록은 층 정규화부터 시작됩니다. 24억 파라미터 버전의 은닉 벡터 크기는 2,560입니다. 이를 80개씩 나누어 32개의 헤드에 입력됩니다. 그룹 쿼리 어텐션을 사용하며 키와 값의 헤드는 8개입니다. 어텐션 층이 출력한 벡터의 크기는 다시 2,560이 되고 잔차 연결을 지나 다시 층 정규화를 거칩니다. 
- 이어서 피드포워드 네트워크가 등장합니다. 앞서 설명한 것처럼 실루 활성화 함수를 사용하며 두 개의 밀집층 중에서 하나에만 적용합니다. 두 밀집층은 은닉 벡터의 크기를 늘려 7,168로 만듭니다. 두 밀집층의 결과를 곱한 후 마지막 밀집층에서 은닉 벡터의 크기는 다시 2,560으로 줄어듭니다. 이런 디코더 블록을 30개 쌓습니다. 드럼 24억 파라미터 버전의 EXAONE 전체 구조를 그림으로 그려보죠.

![스크린샷 2025-04-20 오후 5 30 22](https://github.com/user-attachments/assets/3cd5c112-ca88-497c-84dc-d8a98c586b4b)

- 토큰 아이디가 임베딩 층을 통과해 2,560 차원의 벡터로 변환되고 드롭아웃 층을 지납니다. 그다음 앞서 소개한 디코더 블록 30개를 통과합니다. 마지막 층 정규화를 거치고 밀집층을 통과하면서 어휘 사전의 크기인 102,400 크기의 벡터를 출력합니다. 이 출력에 소프트맥스 함수를 적용하면 어휘 사전에 있는 모든 토큰에 대한 확률처럼 생각할 수 있습니다.
- 이번 절에서 BART의 경우 임베딩 층의 가중치를 활용하여 각 토큰에 대한 확률을 구했습니다. 하지만 EXAONE은 별도의 밀집층을 사용하고 있습니다. 또 하나 다른 점을 눈치했나요? 네, 위치 임베딩이 없죠. 사실 EXAONE이 사용하는 위치 임베딩은 디코더 블록의 어텐션 층 안에 포함되어 있습니다.
- EXAONE을 비롯해 최근 LLM에서 널리 사용되는 위치 임베딩 방식은 **로터리 위치 임베딩**<sup>rotary position embedding, RoPE</sup>입니다. 로터리 위치 임베딩은 쿼리와 키 벡터를 서로 다른 각도로 회전합니다. 이렇게 회전시킨 두 벡터를 곱하면 그 결과에 두 벡터의 상대적인 각도 차이를 인코딩할 수 있습니다.
- 기존의 위치 인코딩과 위치 임베딩은 토큰의 절대적인 위치 인덱스를 벡터로 변환하기 때문에 절대 위치 인코딩이라고 부르며 로터리 위치 임베딩은 쿼리와 키의 상대적인 각도 차이를 표현하기 때문에 상대 위치 인코딩이라 부릅니다. 로터리 위치 임베딩은 별도의 위치 임베딩 벡터를 만들지 않기 때문에 계산이 간단하고 트랜스포머 모델의 성능도 향상시킨다고 알려져 있습니다.
- EXAONE의 주요 특징을 알아보았습니다. 그럼 이제 허깅페이스의 `transformers` 패키지를 사용해 코랩에서 EXAONE의 24억 파라미터 버전을 로드해서 사용해 보겠습니다.

## EXAONE-3.5로 상품 질문에 대한 대답 생성하기

- 이번에는 EXAONE-3.5의 24억 파라미터 버전을 사용해 보겠습니다. EXAONE-3.5의 전체 모델은 허깅페이스 사이트(https://bit.ly/4gsiHHF)를 참고하세요.
- EXAONE 모델은 채팅 템플릿을 활용할 때 좋은 결과를 얻을 수 있습니다. 허깅페이스의 `transformers` 패키지를 사용할 때 채팅 템플릿으로 프롬프트를 구성하는 방법은 잠시 후에 소개하겠습니다. 먼저 EXAONE 모델에서 채팅 템플릿을 사용하라면 토크나이저를 별도로 로드해야 한다는 점을 알아 두어야 합니다.
- `pipeline()` 함수를 사용해 모델을 로드할 때 토크나이저도 자동으로 포함되었습니다. 덕분에 파이프라인 객체의 `tokenizer` 속성으로 참조할 수 있었죠. 하지만 토크나이저를 명시적으로 로드하여 `pipeline()` 함수에 전달할 수도 있습니다. 이럴 때 `AutoTokenizer` 클래스를 사용합니다. 이 클래스의 `from_pretrained()` 클래스 메서드를 사용해 `EXAONE`의 토크나이저를 로드해 보겠습니다. 

> AutoModelForCausalLM 클래스를 사용하여 모델도 직접 로드할 수 있습니다. 허깅페이스 transformers 패키지는 각 작업에 맞는 다양한 Auto 클래스를 제공합니다. 전체 목록은 https://huggingface.co/docs/transformers/model_doc/auto 를 참고하세요.

```python
from transformers import AutoTokenizer

exaone_tokenizer = AutoTokenizer.from_pretrained(
    "LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct")
```

- 불러올 토크나이저 이름은 모델 이름을 지정할 때와 동일하게 허깅페이스의 경로를 전달하면 됩니다. 그다음 `pipeline()` 함수의 `tokenizer` 매개변수로 `exaone_tokenizer`를 전달합니다.

```python
from transformers import pipeline

pipe = pipeline(task="text-generation",
                model="LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct",
                tokenizer=exaone_tokenizer,
                device=0, trust_remote_code=True)
```

- `tokenizer` 외에도 `trust_remote_code` 매개변수가 추가되었습니다. 허깅페이스에 있는 모델은 `transformers` 패키지의 코드 외에 독자적인 코드로 모델을 정의할 수 있습니다. EXAONE이 바로 이런 경우에 해당합니다. 이런 코드는 실행하기 전에 사용자에게 실행 여부를 뭍게 되는데요. `trust_remote_code`를 `True`로 설정하면 코드를 신뢰한다고 일일이 실행할지 여부를 묻지 않습니다.
- 모델을 로드했으니 이제 채팅 템플릿을 만들어 보겠습니다. 채팅 템플릿은 딕셔너리의 리스트로 구성됩니다. 딕셔너리의 키로는 "role"과 "content"가 있습니다. "role"에는 "system"와 "user"와 같은 대화 상대의 역할을 지정합니다. "content"에는 실제 메세지 내용을 적습니다.
- 예를 들어 다음과 같이 "role"을 "system"으로 지정하고 EXAONE 모델이 어떤 역할을 맡게 되는지를 기술해 보죠. 여기에서는 한빛 마켓에 올라온 문의 글에 자동으로 답변하는 역할입니다. 그다음 "role"을 "user"로 지정하고 실제 상품 문의와 같은 메세지를 적습니다. 이렇게 두 개의 딕셔너리를 만든 다음 리스트로 연결하여 채팅 템플릿 구성을 마칩니다. 

```python
messages = [
    {"role": "system",
     "content": "너는 쇼핑몰 홈페이지에 올라온 질문에 대답하는 Q&A 챗봇이야. \
                 확정적인 답변을 하지 말고 제품 담당자가 정확한 답변을 하기 위해 \
                 시간이 필요하다는 간단하고 친절한 답변을 생성해줘."},
    {"role": "user", "content": "이 다이어리에 내년도 공휴일이 표시되어 있나요?"}
]
``` 

- 이제 파이프라인 객체를 호출할 차례입니다. 호출 방법은 이전 절과 동일하지만 이번에는 너무 긴 텍스트가 생성되지 않도록 `max_new_tokens`를 200으로 지정합니다. 

```python
pipe(messages, max_new_tokens=200)
```

```python
[{'generated_text': [{'role': 'system',
    'content': '너는 쇼핑몰 홈페이지에 올라온 질문에 대답하는 Q&A 챗봇이야.                  확정적인 답변을 하지 말고 제품 담당자가 정확한 답변을 하기 위해                  시간이 필요하다는 간단하고 친절한 답변을 생성해줘.'},
   {'role': 'user', 'content': '이 다이어리에 내년도 공휴일이 표시되어 있나요?'},
   {'role': 'assistant',
    'content': '안녕하세요! 다이어리에 내년의 공휴일이 미리 표시되어 있는지에 대해 정확한 답변을 드리기 위해서는 제품 담당자에게 확인이 필요합니다. 현재로선 직접 확인이 어려우니, 저희가 안내드릴 수 있는 방법으로는 고객센터에 연락하시거나, 제품 페이지 내의 문의 게시판을 통해 질문해 보시는 것이 좋을 것 같습니다. 담당자분께서 빠르게 답변해 주실 거예요! 감사합니다.'}]}]
```

- 생성된 텍스트는 딕셔너리의 리스트 형태로 반환됩니다. 여기서는 하나의 프롬프트만 전달했기 때문에 리스트 안에 하나의 딕셔너리만 담겨 있습니다. 이 딕셔너리 안에는 `generated_text`키와 이 키의 값으로 채팅 템플릿에 포함된 메세지와 함께 `role`이 `assistant`인 항목이 추가되었습니다. 이 항목의 `content` 키에 담긴 내용이 EXAONE이 만든 답변입니다. 생성된 답변을 보면 꽤 놀랍습니다. 텍스트가 자연스러울 뿐만 아니라 시스템 메세지를 잘 이해하고 담당자의 확인이 필요하다는 답변과 함께 고객센터나 제품 페이지 내의 게시판을 활용해 보라고 안내까지 하고 있습니다. 
- 보통 프롬프트로 입력한 내용을 다시 확인할 필요는 없으니 모델이 생성한 텍스트만 출력하려면 `return_full_text` 매개변수를 `False`로 지정합니다.
