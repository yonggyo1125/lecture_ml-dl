# 대규모 언어 모델로 텍스트 생성하기

## 키워드 정리

- **EXAONE**은 LG AI 연구원에서 만든 트랜스포머 디코더 기반의 대규모 언어 모델입니다. 3.5버전은 한국어와 영어를 잘 이해하며 비교적 적은 모델 파라미터를 가진 모델 중에서 경쟁력이 있습니다. 최신 LLM에서 채택하는 여러 기술을 사용하고 있습니다. 그룹 쿼리 어텐션, 실루 활성화 함수, RMS 정규화, 로터리 위치 임베딩 등입니다. 
- **토큰 디코딩**은 대규모 언어 모델이 출력한 로짓을 바탕으로 다음 토큰을 선택하는 과정입니다. 가장 기본적인 방법은 로짓을 소프트맥스 함수에 통과시켜 확률로 바꾼 후 이 확률을 기반으로 다음 토큰을 선택합니다. 온도 파라미터를 높이면 비교적 낮은 확률의 토큰이 선택될 가능성을 높일 수 있습니다. 최상위 로짓의 일부 토큰을 선택하는 `top-k` 방식과 누적 확률의 임곗값으로 토큰을 선택하는 `top-p` 방식이 널리 사용됩니다.
- **GPT**는 오픈 AI에서 개발된 트랜스포머 디코더 기반의 대규모 언어 모델입니다. `GPT-2`는 공개 되어 있지만 `GPT-3` 부터는 클로즈드 소스 정책을 유지하고 있습니다. `GPT-3.5`를 기반으로 하는 `ChatGPT`는 인공지능 분야에 큰 영향을 미쳤습니다. 최신 `GPT-4o` 모델은 다양한 작업에서 뛰어난 성능을 내는 모델 중 하나입니다. `ChatGPT` 웹 인터페이스 또는 파이썬 같은 프로그래밍 언어를 위해 제공되는 API를 통해 이런 모델을 사용할 수 있습니다. 

### transformers

- **AutoTokenizer**는 허깅페이스에서 제공하는 사전 훈련된 LLM 모델의 토크나이저를 직접 로드하기 위한 클래스입니다. `from_pretrained()` 클래스 메서드에 허깅페이스 모델 경로를 전달하여 불러올 수 있습니다. 비슷하게 트랜스포머 디코더 기반의 LLM 모델을 불러오려면 `AutoModelForCausalLM` 클래스를 사용합니다. 
- 파이프라인 객체를 호출할 때 다음과 같은 매개변수를 사용할 수 있습니다.
    - `max_new_tokens` 매개변수는 모델이 생성할 최대 토큰 개수를 설정합니다.
    - `return_full_text` 매개변수를 `False`로 지정하면 모델이 생성한 텍스트만 반환합니다. 기본값은 `True` 입니다.
    - `do_sample` 매개변수를 `True`로 지정하면 토큰 확률을 기반으로 다음 토큰을 선택합니다. 기본값은 `False` 입니다.
    - `temperature` 매개변수는 모델이 출력한 로짓의 분포를 조정하는 온도 파라미터입니다. 1.0보다 크면 토큰의 선택 가능성을 고르게 만들고 1.0보다 작으면 높은 확률의 토큰이 선택될 가능성이 더 커집니다. 기본값은 1.0입니다.
    - `top_k` 매개변수는 가장 큰 확률을 가진 토큰 k개를 다음 토큰의 후보로 설정합니다. 기본값은 50이며 이런 디코딩 전략을 `top-k` 샘플링이라고 합니다.
    - `top_p` 매개변수를 1.0보다 작게 설정하면 확률 크기 순으로 토큰을 나열했을 때 누적 확률이 지정한 값을 넘기지 않을 때까지 후보 토큰으로 설정합니다. 기본값은 1.0이며 이런 디코딩 전략을 `top_p` 샘플링이라고 합니다.

### openai

- **OpenAI** 클래스는 오픈 AI의 API호출을 위한 클라이언트 객체를 만듭니다./
    - `api_key` 매개변수에는 오픈 AI에서 발급받은 API 키를 지정합니다. 이 매개변수를 지정하지 않으면 `OPENAI_API_KEY` 환경 변수에 저장되어 값을 이용합니다.
- `OpenAI.chat.completion.create()` 메서드는 채팅 완성 API를 호출하고 GPT 모델의 응답을 반환합니다.
    - `model` 매개변수에 사용할 모델 아이디를 지정합니다.
    - `messages` 매개변수에 모델에게 전달할 대화 메세지를 입력합니다. 멀티모달 모델일 경우 텍스트 외에 이미지나 오디오 등을 전달할 수 있습니다. 
    - `temperature` 매개변수로 0\~2 사이의 온도 파라미터를 조정합니다. 기본값은 1입니다. 
    - `top_p` 매개변수로 `top-p` 샘플링을 설정합니다. 기본값은 1입니다. 
    - `max_completion_tokens` 매개변수로 모델이 생성할 최대 토큰 수를 지정합니다.

## 디코더 기반의 대규모 언어 모델

- 앞에서 보았듯이 `ChatGPT`의 등장 이후 디코더 기반의 대규모 언어 모델(LLM)이 큰 인기를 얻고 있습니다. 디코더 기반 LLM은 텍스트 생성 능력이 특히 뛰어나기 때문에 종종 생성 언어 모델 또는 생성 언어 AI라고도 불립니다. 이러한 모델은 **오픈 소스**<sup>open source</sup> **모델**과 **클로즈드 소스**<sup>closed source</sup> **모델**로 나뉩니다.
- 대표적인 오픈 소스 모델은 다음과 같습니다.
  - 메타의 Llama : https://www.llama.com/
  - 구글의 Gemma : https://ai.google.dev/gemma
  - 마이크로소프트의 Phi : https://azure.microsoft.com/en-us/products/phi/
  - 알리바바의 Qwen : https://qwenlm.github.io/

- 사실 오픈 소스 LLM은 워낙 많아 모두 나열하기 힘듭니다. 특히 인기 있는 모델을 특정 데이터셋으로 다시 미세 튜닝한 변형 모델들도 많습니다. 하지만 메타나 구글과 같은 큰 규모의 회사에서 제공하는 언어 모델은 비교적 높은 성능과 지속적인 지원을 기대할 수 있습니다. 아마도 많은 경우에 여기서 소개하는 모델들이 좋은 출발접이 될 것입니다. 
- 대표적인 클로즈드 소스 모델은 다음과 같습니다.
    - 오픈 AI의 GPT-4 : https://chat.com
    - 엔트로픽(Anthropic)의 Claude : https://claude.ai/
    - 구글의 Gemini : https://gemini.google.com/ 
- 이런 클로즈드 소스 모델들은 모두 웹 인터페이스를 제공하므로, 브라우저에서 다른 사람과 채팅하듯이 모델에게 질문을 하거나 다양한 작업을 요청할 수 있습니다. 클로즈드 소스 LLM은 API 방식도 지원하기 때문에, 이를 활용하면 쉽게 연동하여 자동으로 댓글을 달 수 있습니다.
- 본격적으로 디코더 기반의 모델을 살펴보기 전에, 먼저 가장 높은 성능을 내는 모델을 찾는 방법을 알아보겠습니다. 

## LLM 리더보드
- 대규모 언어 모델의 성능을 비교하는 서비스가 많이 있습니다. 그중에서도 **오픈 LLM 리더보드**<sup>Open LLM leaderboard</sup>와 **LMSYS 챗봇 아레나 리더보드**<sup>LMSYS Chatbot Arena Leaderboard</sup>가 가장 널리 알려져 있습니다.
- 오픈 LLM 리더보드는 허깅페이스에서 제공하는 서비스로, 허깅페이스에 등록된 오픈 소스 LLM의 성능을 비교합니다. 오픈 소스 LLM 리더보드 사이트(https://bit.ly/4gylmPz)에 접속하면 다음과 같은 화면을 확인할 수 있습니다. 

![스크린샷 2025-04-19 오후 10 51 45](https://github.com/user-attachments/assets/ab6e23c3-f68e-48c8-9be2-eae5341a4f38)


- 이 리더보드를 잘 활용하면 특정 작업에 적합한 최신 모델을 쉽게 찾을 수 있습니다. 먼저 화면 아래에 나타난 순위를 살펴보죠. 이 글을 작성하는 시점에는 `calm-3.2-instruct-78b`가 1등을 차지하고 있군요. 모델을 클릭해서 상세 정보를 확인해 보면, 사실 이 모델은 `Qwen 72B` 모델을 미세 튜닝한 버전이라는 점도 알 수 있습니다. 모델의 순위가 어떤 기준으로 매겨졌는지 이해하면 적절한 모델을 선택하는 데 도움이 됩니다. 이 리더보드의 구성 요소를 하나씩 살펴보겠습니다.
- 먼저 목록의 왼쪽의 Type 열에 나타난 아이콘은 모델의 종류를 보여줍니다. 각 아이콘에 대한 설명은 다음과 같습니다.

![스크린샷 2025-04-19 오후 10 51 54](https://github.com/user-attachments/assets/f4a867cb-1df9-47b1-9f47-e53e02d67050)

- Average 열은 모든 벤치마크에서 얻은 점수를 평균한 값입니다. 그 옆의 벤치마크 지표를 차례로 살펴보죠.
  - **IFEval**<sup>Instruction-Following Evaluation</sup>: 약 500개 프롬프트를 선택하여 언어 모델이 프롬프트의 지시를 얼마나 잘 따르는지 평가한 값입니다. 
  - **BBH**<sup>Big Bench Hard</sup>: **빅 벤치**<sup>Big-Bench</sup> 평가의 하위 집합으로 다단계 추론 능력을 평가하는 어려운 과제로 구성되어 있습니다.
  - **MATH**<sup>Mathematics Aptitude Test of Heuristics</sup>: 이름에서 알 수 있듯이 수학 문제 해결 능력을 평가하는 벤치마크 입니다. 12,500개의 문제로 구성되어 있고 대수학, 정수론, 조합, 확률, 미적분 등의 문제를 풀어야 합니다.
  - **GPQA**<sup>Graduate-Level Google-Proof Q&A</sup>: 화학, 생물학, 물리학 분야에서 박사 수준의 448개의 객관식 문제를 푸는 벤치마크입니다.
  - **MUSR**<sup>Multistep Soft Reasoning</sup>: 자연어로 묘사된 추론 문제를 푸는 벤치마크입니다. 예를 들면, 1,000단어 길이의 미스터리 문제를 풀어야 하는 과제입니다.
  - **MMLU-Pro**<sup>Massive Multitask Language Understanding - Professional</sup>: 대규모 언어 모델의 언어 이해와 추론 능력을 평가하기 위한 벤치마크로, 기존의 MMLU보다 더 복잡하고 어려운 12,000개의 문제로 구성되어 있습니다.
  - CO<sub>2</sub> Cost: 모델을 평가하는 데 사용된 전력을 생산하기 위해 배출된 탄소의 양(Kg)입니다.
- 이 리더보드는 기본적으로 Average 열의 값을 기준으로 정렬되어 있습니다. 다른 열을 기준으로 정렬하려면 열 이름 옆에 있는 아이콘을 클릭하여 내림차순이나 오름차순으로 표시할 수 있습니다. 테이블 오른쪽 위에는 `column visibility` 아이콘이 있습니다. 이 아이콘을 클릭하면 표시하고 싶은 열을 추가하거나 제외시킬 수 있습니다. 아이콘을 클릭한 다음 Architecture 버튼을 클릭해 보세요.

![스크린샷 2025-04-20 오후 3 04 47](https://github.com/user-attachments/assets/18fea975-cbcd-49b6-a121-3aaf74d39422)

- 클릭하면 목록에 있는 모델이 사용한 파운데이션 모델 또는 베이스 모델 Architecture 열에 보여줍니다.

![스크린샷 2025-04-20 오후 3 04 54](https://github.com/user-attachments/assets/b214f537-a70d-4389-80b6-0316185d34c8)

- 놀랍게도 이 글을 작성하는 시점에 상위 10개의 모델 모두 Qwen을 사용하고 있습니다. Qwen의 성능이 뛰어나 다양한 작업에서 파운데이션 모델로 채택되고 있다는 것을 잘 보여줍니다.
- 우리는 코랩에서 예제를 실행해야 하므로, 파라미터 개수가 작은 모델을 사용하는 것이 좋습니다. 리더보드 중앙 검색 창 아래에는 `Quick Fliters` 버튼이 있습니다. 모델 파라미터 크기에 따라 순서대로 네 개의 버튼이 제공됩니다.
  - For Edge Devices: 30억 개 이하
  - For Customers: 30억 개 \~ 70억 개
  - Mid-range: 70억 개 \~ 650억 개
  - For the GPU-rich: 650억 개 이상
- 이 중 'For Edge Devices'를 선택하면 다음과 같은 목록이 보여집니다.

![스크린샷 2025-04-20 오후 3 21 55](https://github.com/user-attachments/assets/bf228161-f518-4637-a4ca-dd1ca36dfae2)


- 놀랍게도 이 글을 작성하는 시점에 1위를 차지한 모델은 이 절의 서두에서 소개한 대표적인 네 개의 오픈 소스 LLM이 아닙니다. 가장 높은 점수를 낸 EXAONE-3.5는 2024년 후반 LG AI 연구원에서 공개한 디코더 기반 LLM입니다. 24억 개의 파라미터를 가진 비교적 작은 모델임에도 불구하고 IFEval 점수가 79.5%에 달해서 비슷한 크기의 다른 모델의 성능을 압도하고 있습니다. 이제 EXAONE 모델의 구조적인 특징을 알아보겠습니다. 

> 2025년 3월에 추론 능력이 강화된 EXAONE Deep이 공개되었습니다. 

## EXAONE의 특징

- EXAONE은 국내에서 오픈소스로 공개된 파운데이션 모델로, 한국어와 영여를 잘 이해하며 다양한 작업을 수행할 수 있는 모델로 알려져 있습니다. 이 모델의 3.5 버전은 최신 LLM에서 채택하는 여러 기술을 사용하고 있습니다. 이제 이를 차례대로 살펴보고 전체 모델의 구조를 그림으로 그려보겠습니다. 
- EXAONE은 디코더 기반 트랜스포머 모델입니다. BART처럼 인코더의 출력을 전달받는 크로스 어텐션 층이 없다는 점을 유념하세요.
> 이후에 언급되는 EXAONE은 EXAONE-3.5 버전을 의미합니다. 

- EXAONE은 최신 LLM에서 널리 사용하는 **그룹 쿼리 어텐션**<sup>grouped query attention</sup>을 사용합니다. 이는 멀티 헤드 어텐션의 변형으로, 이를 이해하기 전에 먼저 디코더 토큰을 생성하는 과정을 잠시 되짚어 보죠.
- 디코더는 하나의 토큰을 생성한 후, 그 토큰을 입력의 끝에 이어 붙인 다음 다시 모델에 입력해 다음 토큰을 생성합니다. 이전 절에서 설명했듯이, 이를 자기회귀 모델이라 부릅니다. 이 방식에서는 디코더가 하나의 토큰을 생성할 때마다 이전에 처리했던 토큰들을 매번 다시 계산해야 하므로, 언뜻 보면 계산 낭비처럼 보입니다. 이 문제를 해결하기 위해 어텐션 층에서 키와 값을 캐시에 저장하고 다음 토큰을 생성할 때 재사용하는 기법이 등장했습니다. 그러나 트랜스포머에 입력할 수 있는 최대 입력길이를 늘리려면 캐시의 크기도 자연스럽게 커질 수 밖에 없습니다. 
- 이러한 문제를 해결하기 위해 멀티 헤드 어텐션에서 키와 값을 모든 헤드에서 공유하는 방식이 등장했습니다. 이를 **멀티 쿼리 어텐션**<sup>multi-query attention</sup>이라 부릅니다. 그리고 모든 헤드에서 키와 값을 공유하지 않고, 몇 개의 헤드씩 나눠서 공유하는 방식이 그룹 쿼리 어텐션입니다. 두 방식을 그림으로 나타내면 다음과 같습니다.

![스크린샷 2025-04-20 오후 5 09 14](https://github.com/user-attachments/assets/763d840e-3b93-4aa6-a037-7508e09e5bc8)

- 멀티 쿼리 어텐션과 그룹 쿼리 어텐션은 키와 값을 만드는 밀집층의 개수가 줄어들기 때문에 전체 모델의 파라미터 개수를 줄이는 효과도 있습니다. 그래서 상대적으로 크기가 작은 LLM에서 널리 사용됩니다. EXAONE은 24억, 78억, 320억 파라미터 버전에서 모두 그룹 쿼리 어텐션을 사용합니다.
- 어텐션 층 다음에 등장하는 피드포워드 네트워크에서는 최근 LLM에서 많이 사용되는 **실루**<sup>SiLU</sup> 활성화 함수를 사용합니다. 이 함수는 밀집층의 출력에 시그모이드 함수를 적용한 다음 이 결과에 원래 출력을 다시 곱합니다. 이를 수식으로 나타내면 다음과 같습니다.

![스크린샷 2025-04-20 오후 5 09 22](https://github.com/user-attachments/assets/cf6fae11-2948-4a4d-b6c8-bf9852538478)

- 조금 복잡해 보이지만 다행히 대부분의 딥러닝 프레임워크에서 실루 함수를 제공하고 있기 때문에 다른 활성화 함수를 적용하는 것처럼 손쉽게 적용할 수 있습니다. 실루 함수는 렐루와 비슷한 형태를 가지며, 젤루와 마찬가지로 원점에서도 미분이 가능합니다.
- 일반적으로 실루 함수를 적용할 때 피드포워드 네트워크의 첫 번째 밀집층을 두 개로 나누어 하나는 실루 함수를 적용하고, 다른 하나는 활성화 함수를 적용하지 않습니다. 그 후, 이 두 출력을 곱합니다. 이를 그림으로 나타내면 다음과 같습니다.

![스크린샷 2025-04-20 오후 5 09 28](https://github.com/user-attachments/assets/21bd0abd-2bc8-4424-8a4a-c20acc1dbc7c)

- 또한 층 정규화의 변종인 **RMS 정규화**<sup>root mean square normalization</sup>를 사용합니다. RMS 정규화는 정규화를 할 때 평균을 구하지 않는 방법입니다. 2장에서 표준점수를 계산할 때 입력에서 평균을 빼고 표준편차로 나누었던 것을 기억하시죠? 층 정규화도 기본적으로 이와 같은 방식을 사용합니다. 그런데 입력에서 평균을 빼지 않고, 표준편차를 구할 때도 평균을 사용하지 않는 방법이 RMS 정규화입니다. 간단한 예를 들어 설명해 보죠.
- 다음과 같이 다섯 개의 원소를 가진 배열을 정규화한다고 가정해 보겠습니다. 1에서 5까지의 값으로 이루어져 있으므로 평균은 3입니다. 분산은 각 원소에서 평균을 뺀 후 제곱한 후, 전체 원소 개수로 나누어 계산합니다. 이렇게 계산한 분산을 제곱근하면 표준편차가 됩니다. 정규화를 할 때는 각 원소에서 평균을 빼고 표준편차로 나누어 주면 됩니다.

![스크린샷 2025-04-20 오후 5 26 30](https://github.com/user-attachments/assets/a0987199-97ab-4181-8448-372668294a3d)


![스크린샷 2025-04-20 오후 5 26 35](https://github.com/user-attachments/assets/630f0757-2bc8-43fa-86aa-89d33b758308)



