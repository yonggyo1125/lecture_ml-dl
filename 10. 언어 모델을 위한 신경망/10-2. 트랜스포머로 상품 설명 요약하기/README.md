# 트랜스포머로 상품 설명 요약하기

## 키워드 정리

- **전이 학습**은 한 데이터셋에서 훈련한 모델을 다른 작업 혹은 다른 데이터셋에 적용하는 방법입니다. 이를 위해 훈련된 모델을 그대로 사용하거나 모델의 일부 또는 전체를 미세 튜닝할 수도 있습니다. 전이 학습은 합성곱 신경망에서 널리 사용되면서 컴퓨터 비전 분야를 발전시켰습니다. 트랜스포머 모델이 등장하면서 자연어 처리 분야에도 전이 학습이 널리 적용되기 시작했습니다.
- **BART**는 메타에서 공개한 트랜스포머 기반 인코더-디코더 모델입니다. 원본 트랜스포머 모델과 매우 비슷한 구조를 띠고 있으며 인코더와 디코더 블록을 반복적으로 여러 개 쌓아서 구성합니다. 인코더-디코더 기반 모델은 주로 요약, 번역과 같은 시퀀스-투-시퀀스 작업에 사용됩니다. 한국어 데이터셋에서 훈련된 **KoBART** 모델도 공개되었습니다.
- **허깅페이스**는 트랜스포머 모델을 쉽게 사용하고 훈련하기 위한 `transformers` 패키지를 만든 회사로 유명합니다. 허깅페이스 사이트에는 다양한 트랜스포머 기반 자연어 처리 모델은 물론 비전과 오디오 작업을 위한 모델과 데이터셋을 무료로 제공합니다. 만약 자연어 처리 문제를 위한 데이터셋이나 모델이 필요하다면 가장 먼저 찾아볼 곳 중 하나입니다.
- **토큰화**는 대규모 언어 모델에 입력하기 위해 텍스트를 더 작은 단어로 쪼개는 과정입니다. 이런 작업을 처리하는 모델을 토크나이저라고 하며 `LLM` 모델 자체와는 별도로 훈련됩니다. 허깅페이스의 파이프라인 함수를 사용하면 토크나이저와 `LLM`을 한 번에 로드하여 사용할 수 있습니다. 대표적인 토큰화 방법으로는 `BPE`, `워드피스`, `유니그램`, `센텐스피스`등이 있습니다.

## 핵심 패지지와 함수
