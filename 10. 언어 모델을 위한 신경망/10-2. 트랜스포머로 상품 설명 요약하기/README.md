# 트랜스포머로 상품 설명 요약하기

## 키워드 정리

- **전이 학습**은 한 데이터셋에서 훈련한 모델을 다른 작업 혹은 다른 데이터셋에 적용하는 방법입니다. 이를 위해 훈련된 모델을 그대로 사용하거나 모델의 일부 또는 전체를 미세 튜닝할 수도 있습니다. 전이 학습은 합성곱 신경망에서 널리 사용되면서 컴퓨터 비전 분야를 발전시켰습니다. 트랜스포머 모델이 등장하면서 자연어 처리 분야에도 전이 학습이 널리 적용되기 시작했습니다.
- **BART**는 메타에서 공개한 트랜스포머 기반 인코더-디코더 모델입니다. 원본 트랜스포머 모델과 매우 비슷한 구조를 띠고 있으며 인코더와 디코더 블록을 반복적으로 여러 개 쌓아서 구성합니다. 인코더-디코더 기반 모델은 주로 요약, 번역과 같은 시퀀스-투-시퀀스 작업에 사용됩니다. 한국어 데이터셋에서 훈련된 **KoBART** 모델도 공개되었습니다.
- **허깅페이스**는 트랜스포머 모델을 쉽게 사용하고 훈련하기 위한 `transformers` 패키지를 만든 회사로 유명합니다. 허깅페이스 사이트에는 다양한 트랜스포머 기반 자연어 처리 모델은 물론 비전과 오디오 작업을 위한 모델과 데이터셋을 무료로 제공합니다. 만약 자연어 처리 문제를 위한 데이터셋이나 모델이 필요하다면 가장 먼저 찾아볼 곳 중 하나입니다.
- **토큰화**는 대규모 언어 모델에 입력하기 위해 텍스트를 더 작은 단어로 쪼개는 과정입니다. 이런 작업을 처리하는 모델을 토크나이저라고 하며 `LLM` 모델 자체와는 별도로 훈련됩니다. 허깅페이스의 파이프라인 함수를 사용하면 토크나이저와 `LLM`을 한 번에 로드하여 사용할 수 있습니다. 대표적인 토큰화 방법으로는 `BPE`, `워드피스`, `유니그램`, `센텐스피스`등이 있습니다.

## 핵심 패지지와 함수

### transformers
- `pipeline()`은 허깅페이스 `transformers` 라이브러리로 간편하게 모델 추론을 할 수 있는 파이프라인 객체를 만들어 주는 함수입니다. 
- `task` 매개변수에 파이프라인 객체로 수행할 작업을 지정합니다. 대표적으로 텍스트를 분류하는 `text-classification`과 요약을 위한 `summarization`등이 있습니다.
- `model` 매개변수에는 작업 수행에 사용할 모델을 지정합니다. 이 매개변수를 지정하지 않을 경우 각 작업의 기본 모델이 사용됩니다.
- `device` 매개변수에는 추론에 사용할 하드웨어 장치를 지정합니다. 기본값은 -1로 `CPU`를 사용합니다. 0부터 시작해서 컴퓨터에 설치된 `GPU` 장치를 순서대로 지정할 수 있습니다.
- 파이프라인 객체의 `model` 속성은 `pipeline()` 함수를 호출할 떄 로드한 모델 객체를 담고 있습니다. 이 모델 객체의 `config` 속성은 인코더와 디코더 개수, 임베딩 크기 등 `LLM` 모델을 위한 다양한 설정을 포함하고 있습니다. 
- 파이프라인 객체의 `tokenizer` 속성은 `pipeline()` 함수를 호출할 때 로드한 토크나이저 객체를 담고 있습니다.

### 허깅페이스 토크나이저 객체는 다음과 같은 속성과 메서드를 제공합니다.
- **vocab_size** 속성은 토크나이저의 어휘 사전 크기를 나타냅니다.
- **vocab** 속성은 토크나이저의 어휘 사전을 반환합니다. 토크나이저의 **get_vocab()** 메서드를 호출하는 것과 동일합니다.
- **tokenize()** 메서드는 문자열을 토큰으로 분할하여 토큰 리스트를 반환합니다.
- **convert_tokens_to_ids()** 메서드는 토큰(또는 토큰의 리스트)을 토큰 아이디(또는 토큰 아이디의 리스트)로 변환합니다.
- **convert_ids_to_tokens()** 메서드는 토큰 아이디(또는 토큰 아이디의 리스트)를 토큰(또는 토큰 리스트)으로 변환합니다.
- **encode()** 메서드는 문자열(또는 문자열 리스트)를 토큰으로 분할하여 토큰 아이디와 리스트를 반환합니다.
- **decode()** 메서드는 토큰 아이디(또는 토큰 아이디의 리스트)를 문자열로 복원합니다.

## 시작하기 전에
- 이전 절에서 트랜스포머 모델의 전체적인 구조를 살펴보았습니다. 자연어 처리 분야에서 트랜스포머가 뛰어난 성능을 발휘하니, 한빛 마켓의 문제에도 적용해 보겠습니다. 그런데 트랜스포머 모델은 놀랍게도 인코더-디코더 구조를 기반으로 하지만, 인코더와 디코더를 각각 떼어내어 독립적으로 사용할 수도 있습니다.
- 이 절에서 트랜스포머 인코더, 디코더, 인코더-디코더 모델의 특징을 알아보고 상품 설명을 요약하기 위해 적합한 모델을 찾아 적용해 보겠습니다.

## 트랜스포머 가계도
- 이전 절에서 살펴보았듯이 트랜스포머 모델은 인코더와 디코더로 구성되어 있습니다. 하지만 인코더만 따로 사용하거나, 디코더만 따로 떼어내서 사용할 수도 있습니다. 어떻게 이런 활용이 가능할까요?
- 이전 절에서 인코더와 디코더가 출력하는 값은 결국 각 토큰에 대한 은닉 벡터라고 설명했던 것을 기억하나요? 즉 인코더, 디코더, 인코더-디코더 구조에서 출력하는 결과는 모두 동일한 형태의 은닉 벡터입니다. 세 구조의 출력 형태가 같기 때문에, 특정 작업에 따라 적절한 구조를 선택할 수 있는 가능성이 열린 셈이죠.
- 먼저 디코더를 생각해 보죠. 디코더는 타임스텝마다 하나의 토큰을 생성하는 역할을 합니다. 만약 어떤 작업이 시퀀스 데이터를 입력받아 하나의 결괏값을 출력해야 한다면, 디코더는 적합하지 않을 수 있습니다. 이런 작업에는 9장에서 다루었던 텍스트 분류가 해당됩니다. 텍스트 분류는 입력된 문장이 긍정적인지 부정적인지 예측합니다. 이러한 작업은 인코더의 출력에 밀집층과 같은 분류를 위한 층을 배치하여 해결할 수 있습니다.
- 인코더를 활용하는 또 다른 직업으로는 **개체명 인식**<sup>named entity</sup>이 있습니다. 개체명 인식은 텍스트에서 사람 이름, 지역명, 회사 이름 등의 고유 명사를 식별하는 작업입니다. 이 경우 입력된 각 토큰마다 개체명 여부를 출력합니다. 또 인코더는 두 텍스트의 유사도를 측정하는 **STS**<sup>sementic textual similarity</sup> 같은 작업에도 사용됩니다. 
- 이전 절에서 설명했듯이 인코더-디코더 모델은 주로 요약과 번역 같은 전형적인 시퀀스-투-시퀀스 작업에 사용됩니다. 이외에도 자연어 처리에 자주 등장하는 또 다른 시퀀스-투-시퀀스 작업으로 **질문-답변**<sup>question answering, QA</sup>이 있습니다. 질문과 문맥 텍스트가 주어졌을 때, 문맥 속에서 답을 찾아 응답을 생성합니다.
- 그럼 디코더만 따로 사용하는 경우는 어떤 작업일까요? 이에 대해 알아보기 전에 용어를 먼저 정리해보죠. 트랜스포머의 인코더만 사용하는 모델은 인코더 기반 모델, 디코더만 사용하는 모델은 디코더 기반 모델이라 부르겠습니다. 그리고 인코더와 디코더를 모두 사용하는 모델을 인코더-디코더 모델이라 부르겠습니다. 디코더 기반 모델을 설명하기 전에 이 세 종류의 트랜스포머 모델을 개발 순서대로 나타낸 그림을 살펴보겠습니다.

![스크린샷 2025-04-16 오후 9 40 32](https://github.com/user-attachments/assets/8db9dfa4-1d1b-4f1d-9be8-b8ca5cfe46db)


