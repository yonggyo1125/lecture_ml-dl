# 트랜스포머로 상품 설명 요약하기

## 키워드 정리

- **전이 학습**은 한 데이터셋에서 훈련한 모델을 다른 작업 혹은 다른 데이터셋에 적용하는 방법입니다. 이를 위해 훈련된 모델을 그대로 사용하거나 모델의 일부 또는 전체를 미세 튜닝할 수도 있습니다. 전이 학습은 합성곱 신경망에서 널리 사용되면서 컴퓨터 비전 분야를 발전시켰습니다. 트랜스포머 모델이 등장하면서 자연어 처리 분야에도 전이 학습이 널리 적용되기 시작했습니다.
- **BART**는 메타에서 공개한 트랜스포머 기반 인코더-디코더 모델입니다. 원본 트랜스포머 모델과 매우 비슷한 구조를 띠고 있으며 인코더와 디코더 블록을 반복적으로 여러 개 쌓아서 구성합니다. 인코더-디코더 기반 모델은 주로 요약, 번역과 같은 시퀀스-투-시퀀스 작업에 사용됩니다. 한국어 데이터셋에서 훈련된 **KoBART** 모델도 공개되었습니다.
- **허깅페이스**는 트랜스포머 모델을 쉽게 사용하고 훈련하기 위한 `transformers` 패키지를 만든 회사로 유명합니다. 허깅페이스 사이트에는 다양한 트랜스포머 기반 자연어 처리 모델은 물론 비전과 오디오 작업을 위한 모델과 데이터셋을 무료로 제공합니다. 만약 자연어 처리 문제를 위한 데이터셋이나 모델이 필요하다면 가장 먼저 찾아볼 곳 중 하나입니다.
- **토큰화**는 대규모 언어 모델에 입력하기 위해 텍스트를 더 작은 단어로 쪼개는 과정입니다. 이런 작업을 처리하는 모델을 토크나이저라고 하며 `LLM` 모델 자체와는 별도로 훈련됩니다. 허깅페이스의 파이프라인 함수를 사용하면 토크나이저와 `LLM`을 한 번에 로드하여 사용할 수 있습니다. 대표적인 토큰화 방법으로는 `BPE`, `워드피스`, `유니그램`, `센텐스피스`등이 있습니다.

## 핵심 패지지와 함수

### transformers
- `pipeline()`은 허깅페이스 `transformers` 라이브러리로 간편하게 모델 추론을 할 수 있는 파이프라인 객체를 만들어 주는 함수입니다. 
- `task` 매개변수에 파이프라인 객체로 수행할 작업을 지정합니다. 대표적으로 텍스트를 분류하는 `text-classification`과 요약을 위한 `summarization`등이 있습니다.
- `model` 매개변수에는 작업 수행에 사용할 모델을 지정합니다. 이 매개변수를 지정하지 않을 경우 각 작업의 기본 모델이 사용됩니다.
- `device` 매개변수에는 추론에 사용할 하드웨어 장치를 지정합니다. 기본값은 -1로 `CPU`를 사용합니다. 0부터 시작해서 컴퓨터에 설치된 `GPU` 장치를 순서대로 지정할 수 있습니다.
- 파이프라인 객체의 `model` 속성은 `pipeline()` 함수를 호출할 떄 로드한 모델 객체를 담고 있습니다. 이 모델 객체의 `config` 속성은 인코더와 디코더 개수, 임베딩 크기 등 `LLM` 모델을 위한 다양한 설정을 포함하고 있습니다. 
- 파이프라인 객체의 `tokenizer` 속성은 `pipeline()` 함수를 호출할 때 로드한 토크나이저 객체를 담고 있습니다.

### 허깅페이스 토크나이저 객체는 다음과 같은 속성과 메서드를 제공합니다.
- **vocab_size** 속성은 토크나이저의 어휘 사전 크기를 나타냅니다.
- **vocab** 속성은 토크나이저의 어휘 사전을 반환합니다. 토크나이저의 **get_vocab()** 메서드를 호출하는 것과 동일합니다.
- **tokenize()** 메서드는 문자열을 토큰으로 분할하여 토큰 리스트를 반환합니다.
- **convert_tokens_to_ids()** 메서드는 토큰(또는 토큰의 리스트)을 토큰 아이디(또는 토큰 아이디의 리스트)로 변환합니다.
- **convert_ids_to_tokens()** 메서드는 토큰 아이디(또는 토큰 아이디의 리스트)를 토큰(또는 토큰 리스트)으로 변환합니다.
- **encode()** 메서드는 문자열(또는 문자열 리스트)를 토큰으로 분할하여 토큰 아이디와 리스트를 반환합니다.
- **decode()** 메서드는 토큰 아이디(또는 토큰 아이디의 리스트)를 문자열로 복원합니다.

## 시작하기 전에
- 이전 절에서 트랜스포머 모델의 전체적인 구조를 살펴보았습니다. 자연어 처리 분야에서 트랜스포머가 뛰어난 성능을 발휘하니, 한빛 마켓의 문제에도 적용해 보겠습니다. 그런데 트랜스포머 모델은 놀랍게도 인코더-디코더 구조를 기반으로 하지만, 인코더와 디코더를 각각 떼어내어 독립적으로 사용할 수도 있습니다.
- 이 절에서 트랜스포머 인코더, 디코더, 인코더-디코더 모델의 특징을 알아보고 상품 설명을 요약하기 위해 적합한 모델을 찾아 적용해 보겠습니다.

## 트랜스포머 가계도
- 이전 절에서 살펴보았듯이 트랜스포머 모델은 인코더와 디코더로 구성되어 있습니다. 하지만 인코더만 따로 사용하거나, 디코더만 따로 떼어내서 사용할 수도 있습니다. 어떻게 이런 활용이 가능할까요?
- 이전 절에서 인코더와 디코더가 출력하는 값은 결국 각 토큰에 대한 은닉 벡터라고 설명했던 것을 기억하나요? 즉 인코더, 디코더, 인코더-디코더 구조에서 출력하는 결과는 모두 동일한 형태의 은닉 벡터입니다. 세 구조의 출력 형태가 같기 때문에, 특정 작업에 따라 적절한 구조를 선택할 수 있는 가능성이 열린 셈이죠.
- 먼저 디코더를 생각해 보죠. 디코더는 타임스텝마다 하나의 토큰을 생성하는 역할을 합니다. 만약 어떤 작업이 시퀀스 데이터를 입력받아 하나의 결괏값을 출력해야 한다면, 디코더는 적합하지 않을 수 있습니다. 이런 작업에는 9장에서 다루었던 텍스트 분류가 해당됩니다. 텍스트 분류는 입력된 문장이 긍정적인지 부정적인지 예측합니다. 이러한 작업은 인코더의 출력에 밀집층과 같은 분류를 위한 층을 배치하여 해결할 수 있습니다.
- 인코더를 활용하는 또 다른 직업으로는 **개체명 인식**<sup>named entity</sup>이 있습니다. 개체명 인식은 텍스트에서 사람 이름, 지역명, 회사 이름 등의 고유 명사를 식별하는 작업입니다. 이 경우 입력된 각 토큰마다 개체명 여부를 출력합니다. 또 인코더는 두 텍스트의 유사도를 측정하는 **STS**<sup>sementic textual similarity</sup> 같은 작업에도 사용됩니다. 
- 이전 절에서 설명했듯이 인코더-디코더 모델은 주로 요약과 번역 같은 전형적인 시퀀스-투-시퀀스 작업에 사용됩니다. 이외에도 자연어 처리에 자주 등장하는 또 다른 시퀀스-투-시퀀스 작업으로 **질문-답변**<sup>question answering, QA</sup>이 있습니다. 질문과 문맥 텍스트가 주어졌을 때, 문맥 속에서 답을 찾아 응답을 생성합니다.
- 그럼 디코더만 따로 사용하는 경우는 어떤 작업일까요? 이에 대해 알아보기 전에 용어를 먼저 정리해보죠. 트랜스포머의 인코더만 사용하는 모델은 인코더 기반 모델, 디코더만 사용하는 모델은 디코더 기반 모델이라 부르겠습니다. 그리고 인코더와 디코더를 모두 사용하는 모델을 인코더-디코더 모델이라 부르겠습니다. 디코더 기반 모델을 설명하기 전에 이 세 종류의 트랜스포머 모델을 개발 순서대로 나타낸 그림을 살펴보겠습니다.

![스크린샷 2025-04-16 오후 9 40 32](https://github.com/user-attachments/assets/8db9dfa4-1d1b-4f1d-9be8-b8ca5cfe46db)

- 이 그림은 2018년 즈음부터 언어 모델의 주요 발전 과정을 보여줍니다. 가장 왼쪽에 회색 부분은 유용한 단어 임베딩 벡터를 만드는 데 초점을 맞춘 모델들입니다. 이 모델들은 트랜스포머 구조를 사용하지 않으며, 초창기 LLM이라고 말할 수 있습니다. 
- 중앙에 크게 세 개의 가지가 위로 뻗어 있는 것을 확인할 수 있습니다. 이 세 개의 가지 위에 있는 모델은 모두 트랜스포머 구조를 사용합니다. 가장 왼쪽의 진보라색 가지는 인코더 기반 모델입니다. 이 가지는 2021년 후에 더 이상 자라지 못하고 있네요. 실제로 인코더 기반 모델에 대한 관심이 줄었기 때문입니다. 대표적인 인코더 기반 모델은 그림에 있듯이 `BERT`, `RoBERT`, `ELECTRA` 등이 있습니다.
- 가운데 연보라색 가지는 인코더-디코더 모델입니다. 가지에 달린 열매가 풍성하지는 않지만 최근까지 여러 모델이 개발되고 있습니다. 대표적인 인코더-디코더 모델은 T5, BART등입니다. 
- 가장 오른쪽에 있는 보래색 가지에는 정말 많은 모델이 달려 있군요. 이 가지의 모델들이 바로 디코더 기반 모델입니다. 이 그림을 보면 디코더 기반 모델의 인기를 쉽게 눈치챌 수 있습니다. 이 그림을 보면 디코더 기반 모델의 인기를 쉽게 눈치챌 수 있습니다. `GPT-3`와 `ChatGPT` 같은 유명한 모델이 여기에 포함됩니다.
- 디코더 기반 모델이 특히 인기가 높은 이유는 뛰어난 텍스트 생성 능력 때문입니다. 텍스트 생성 기능은 챗봇, 질문 답변, 요약, 번역 등에 널리 적용될 수 있기 때문이죠. 그런데 인코더에서 제공하는 은닉 벡터 없이 디코더가 단독으로 텍스트를 생성할 수 있을까요?
- 사실, 디코더는 무에서 유를 창조할 수 없습니다. 디코더의 작동 방식을 다시 떠올려 보세요. 디코더는 이전까지 생성한 텍스트를 입력으로 받아 다음 토큰을 예측하는 식으로 동작합니다. 따라서 이전에 생성한 텍스트도 없고, 인코더로부터의 입력도 없으면 아무것도 생성할 수가 없죠. 반대로 이전에 생성한 텍스트인 것처럼 어떤 텍스트를 입력해 주면 인코더의 도움이 없어도 다음 토큰을 예측할 수 있습니다. 이렇게 이전에 생성한 텍스트인 것처럼 전달하는 초기 텍스트를 **프롬프트**<sup>prompt</sup>라고 부르며 사람이 모델을 실행할 떄 전달하게 됩니다.

> ChatGPT 등과 같은 모델을 사용해 보았다면 프롬프트에 원하는 지시를 내리면 모델이 그에 맞는 응답을 하는 것을 볼수 있습니다. 

- 이번 절에서는 인코더-디코더 기반 모델을 사용해 텍스트를 요약하는 방법을 알아보고, 다음 절에서는 디코더 기반 모델을 사용해 재미있는 텍스트를 생성하는 방법을 알아보겠습니다.
- 다음 주제로 넘어가기 전에 앞 그림에 대해서 추가로 언급할 것이 있습니다. 이 그림에서 보라색으로 바탕이 채워진 모델은 오픈소스 모델입니다. 흰색 바탕은 클로즈드 소스, 즉 독점적인 상업 모델입니다. 초기 대규모 언어 모델은 대부분은의 머신러닝 분야가 그렇듯이 오픈소스였습니다. 하지만 디코더 기반 모델 중에서 GPT-3가 크게 성공하자 클로즈드 소스 정책으로 선회했고 ChatGPT 같은 유료 서비스가 등장했습니다. 이후로 많은 회사들이 디코더 기반의 모델들을 클로즈드 소스로 선보였습니다. 
- 하지만 2022년 부터 오픈소스 모델들이 다시 등장하기 시작해 클로즈드 소스 LLM과 경쟁하기 시작했습니다. 특히 **메타**<sup>Meta</sup>에서 공개한 Llama는 규모와 성능 면에서 클로즈드 소스 LLM에 크게 뒤쳐지지 않습니다. Llama에서 파생된 모델이 2023년을 언어 모델의 해로 만들었다고 해도 과언이 아닙니다.
- 2025년에도 (그리고 아마 그 이후에도) 이런 추세는 계속되고 있습니다. 구글이 오픈소스 LLM인 Gemma를 공개했고, 마이크로소프트는 Phi 모델을 오픈소스로 공개했습니다. 또 알리바바 그룹이 만든 Qwen도 높은 성능으로 큰 인기를 끌고 있습니다. 이외에도 정말 많은 모델이 등장하고 있어 LLM의 진화 트리를 더 이상 그리기 힘들 정도입니다.
- 이런 오픈소스 LLM이 인기가 높은 것은 단순히 구조만 공개된 것이 아니라 대규모 텍스트 말뭉치에서 훈련된 모델 파라미터도 함께 공개되었기 때문입니다. 모델 파라미터가 공개 되었다는 것은 사전에 훈련된 모델을 현재 주어진 작업에 맞게 접목할 수 있다는 의미입니다. 이런 방법을 전이 학습이라고 부릅니다. 이어서 전이 학습에 대해 알아보겠습니다. 

## 전이 학습
- **전이 학습**<sup>transfer learning</sup>은 이미 훈련된 모델을 새로운 작업에 맞춰 재사용하거나 약간 조정하여 사용하는 방법입니다. 최근 신경망 모델은 성능을 높이기 위해 갈수록 점점 더 커지고 있습니다. 새로운 문제마다 모델을 처음부터 다시 훈련해야 한다면, 비용과 시간이 많이 듭니다. 또 어떤 경우에는 사용하고 싶은 모델이 너무 커서 아예 훈련할 수 있는 여력이 없을 수 있습니다.
- 이런 경우 누군가 이미 훈련한 모델을 재사용할 수 있다면 정말 좋지 않을까요? 다행히 신경망 모델의 파라미터는 재사용할 수 있는 특징을 가지고 있습니다. 예를 들어 합성곱 신경망에서는 합성곱층이 학습한 필터가 다양한 시각적 패턴(경계선, 질감, 색상 등)을 감지하는 역할을 합니다. 이런 패턴 감지 능력은 특정 문제에만 국한되지 않고 다른 이미지 인식 작업에서도 사용할 수 있습니다.
- 실제로 **이미지넷 데이터셋**<sup>ImageNet dataset</sup>에서 훈련된 신경망이 많이 공개되어 있습니다. 이런 신경망은 구조와 가중치가 모두 공개되어 있기 때문에 누구나 자신의 작업을 가져다 사용할 수 있죠. 일반적으로 합성곱 신경망의 마지막 부분에 놓인 한 개 이상의 밀집충(이를 분류층이라고 부릅니다)을 버리고 입력부터 마지막 합성곱 층까지만 재사용합니다. 이런 모델을 **베이스 모델**<sup>base model</sup>이라고 부릅니다. 

> 이미지넷 데이터셋은 스탠포드 대학교에서 만든 컴퓨터 비전을 위한 대규모 이미지 데이터베이스입니다. 2010년부터 이미지 분류, 객체 탐지 등의 주제로 경연 대회가 열렸으며, 이를 통해 VGGNet, ResNet 등의 유명한 합성곱 신경망 모델들이 개발되었습니다. 2015년부터 신경망 모델이 사람보다 더 뛰어난 성능을 달성하기 시작했고 이 대회는 2017년을 마지막으로 종료되었습니다.

- 최근 트랜스포머 기반 언어 모델이 인기를 끄는 가장 큰 이유는 전이 학습이 가능하기 때문입니다. 10장 1절에서 언급했듯이 트랜스포머의 인코더와 디코더가 각각 출력하는 것은 토큰의 임베딩 벡터(또는 은닉 벡터)라는 것을 기억하세요. 이 벡터는 토큰의 의미를 잘 표현하기 위한 문맥에 있는 정보를 통합한 벡터입니다. 대규모 말뭉치에서 사전 훈련된 트랜스포머 모델의 파라미터는 이런 벡터를 잘 만들 수 있도록 최적화되어 있을 것입니다. 
- 따라서 모델을 처음부터 훈련할 것이 아니라 대규모 텍스트 데이터셋에서 사전 훈련된 트랜스포머 모델의 인코더와 디코더를 사용해 새로운 작업에 적용할 수 있습니다. 다행히 트랜스포머 모델은 다양한 작업에 맞춰 훈련되거나 전이 학습으로 조정된 모델이 많이 공개되어 있습니다. 이번 절에서는 잘 알려진 인코더-디코더 모델인 BART 모델을 사용해 텍스트를 요약하는 작업을 수행해 보겠습니다.

## BART 모델 소개

- BART는 2019년 메터에서 공개한 트랜스포머 기반의 인코더-디코더 언어 모델로, 160GB에 달하는 대규모 텍스트 데이터셋으로 훈련되었습니다. 이전 절에서 살펴보았듯 인코더-디코더 기반 모델은 텍스트를 입력받아 또 다른 텍스트를 생성하는 작업에 적합합니다. 따라서 텍스트 요약 작업에도 잘 맞습니다. 
- BART 모델은 **베이스**<sup>base</sup>와 **라지**<sup>large</sup> 모델 두 가지 버전이 있습니다. 베이스 모델은 인코더와 디코더 블록을 각각 6개씩 사용하며, 라지 모델은 12개씩 사용합니다. 블록 개수가 많아질수록 모델의 파라미터 수도 증가하는데, 베이스 모델의 파라미터 개수는 약 1억 2천만 개, 라지 모델은 4억 개가 넘습니다. 정말 엄청나게 많군요.
- 먼저 BART 베이스 모델의 전체 구조를 그림으로 살펴보겠습니다. 라지 모델도 블록 개수와 같은 하이퍼파라미터만 다를 뿐 기본 구성은 동일합니다. 

![스크린샷 2025-04-17 오후 8 04 59](https://github.com/user-attachments/assets/3df7465d-e83e-40e3-bc6f-d668df52277d)

- BART 베이스 모델은 인코더와 디코더 블록을 각각 6개씩 사용합니다. 10장 1절에서 설명했듯이 마지막 인코더의 출력이 모든 디코더 블록에 추가됩니다. 이런 모습이 위 그림에 잘 나타나 있습니다. 
- 그런데 10장 1절에서 소개한 것과 조금 다른 부분이 보입니다. 먼저 위치 인코딩이 아니라 위치 임베딩으로 바뀌었습니다. 위치 인코딩에서는 토큰의 위치에 대한 정보를 만들기 위해 삼각 함수를 사용했습니다. 다시 말하면 토큰의 위치에 해당하는 정수값을 삼각 함수로 위치 정보가 담긴 벡터로 바꾼 것이죠. 그렇다면 정수를 실수 벡터 표현으로 바꾸는 데 사용하는 임베딩 층을 위치 인코딩에 사용하면 어떨까요?
- 네, 바로 이런 아이디어가 실제로 널리 사용됩니다. 토큰 아이디어를 실수 벡터 표현으로 바꾸기 위해 임베딩 층을 사용하듯 위치 정수값을 실수 벡터 표현으로 바꾸기 위해 임베딩 층을 사용할 수 있습니다. 토큰을 위한 임베딩 층처럼 위치를 위한 임베딩 층도 처음에는 랜덤하게 초기화되며 훈련을 통한 최적의 값을 학습합니다. 이런 방법을 위치 인코딩과 구분하기 위하여 **위치 임베딩**<sup>positional embedding</sup>이라 부릅니다. 
- 앞의 그림을 보면 인코더와 디코더에 각각 별도의 위치 임베딩 층이 배치되어 있습니다. 하지만 인코더와 디코더에 들어가는 토큰은 하나의 임베딩 층을 통과하는 군요. 이는 인코더-디코더 모델이 하나의 구조로 동작하기 때문입니다. 인코더와 디코더가 서로 다른 단어 임베딩을 학습한다면, 인코더에서 생성한 단어 임베딩을 디코더가 완전히 다르게 해석해 엉뚱한 결과를 출력할 가능성이 있습니다. 따라서 트랜스포머 기반 인코더-디코더 모델은 하나의 공통된 단어 임베딩 층을 사용하여 인코더와 디코더가 동일한 방식으로 토큰을 변환하도록 설계됩니다.
- 토큰 임베딩과 위치 임베딩을 더한 후 층 정규화와 드롭아웃 층을 거칩니다. 그리고 인코더 블록과 디코더 블록을 반복해서 거치게 되는군요. 그런데 디코더 마지막에 세로로 회전시킨 임베딩 층이 보입니다. 이 층을 통과하면 토큰에 대한 확률이 출력됩니다. 이에 대해서 조금 설명이 필요할 것 같습니다. 디코더의 마지막 출력은 토큰에 대한 은닉 벡터 즉, 임베딩 벡터입니다. BART 베이스 모델의 경우 이 벡터의 크기는 768입니다. 이 벡터로 부터 다음 토큰에 대한 확률을 계산해야 합니다. BART모델이 출력할 수 있는 어휘 사전의 토큰 수는 50,265개 입니다. 따라서 768 크기의 벡터를 50,265개의 확률을 담은 또 다른 벡터로 바꾸는 작업이 필요합니다.
- 7장에서 해보았듯이 이는 입력이 768개일 때 50,265개의 출력을 만드는 밀집층으로 쉽게 구현할 수 있습니다. 절편으로 고려하지 않는다 해도 이 밀집층에는 768 X 50,265개의 모델 파라미터가 필요하겠군요! 엄청난 양이 필요합니다. 그런데 이와 동일한 크기의 모델 파라미터가 모델 맨 처음에 이미 사용되고 있습니다. 바로 토큰 정수를 단어 임베딩으로 만드는 임베딩 층입니다. 이 층에는 다음 그림과 같이 50,265개의 토큰에 대한 단어 임베딩이 학습되어 있습니다. 각 단어 임베딩 벡터의 차원은 768입니다.

![스크린샷 2025-04-17 오후 10 05 18](https://github.com/user-attachments/assets/e42e9325-8dc4-47ef-932e-3c732e3d5815)

- 만약 이 행렬을 살짝 회전시킨다면 다음처럼 768 X 50,265 크기의 모델 파라미터를 얻을 수 있습니다. 이것이 가능하다면 별도의 밀집층을 두지 않고 임베딩 층의 모델 파라미터를 사용해 디코더의 출력 벡터를 각 토큰에 대한 확률로 바꿀수 있습니다.

![스크린샷 2025-04-17 오후 10 05 30](https://github.com/user-attachments/assets/e1d98965-aec7-4b84-9585-feeb696ab095)

- 실제 이런 아이디어가 대규모 언어 모델을 만들 때 종종 적용되며 마지막에 밀집층을 따로 둘 필요가 없어 전체 모델 파라미터의 수를 줄이는 데 도움이 됩니다. 조금 더 구체적으로 살펴보면 디코더 층의 마지막 출력을 회전시킨 임베딩 행렬의 첫 번째 열과 곱하면 하나의 실수값을 얻을 수 있습니다. 이 값을 어휘 사전에 있는 첫 번째 토큰이 선택될 확률로 생각할 수 있습니다.

![스크린샷 2025-04-17 오후 10 05 39](https://github.com/user-attachments/assets/6259380a-4884-44f6-8bc7-fba04bf3a32c)

- 그다음 두 번째 열과 곱하면 다음과 같이 두 번째 토큰이 다음 토큰으로 선택될 확률을 출력합니다.

![스크린샷 2025-04-17 오후 10 10 59](https://github.com/user-attachments/assets/89bda7cf-3df6-42b2-831a-a3ee29c9b991)

- 이런 식으로 50,265개의 열과 모두 곱하면 50,265개의 실숫값이 만들어지고 이를 다음 토큰에 대한 확률처럼 생각할 수 있습니다.

> 사실 디코더의 출력을 확률로 생각하려면 마지막 출력값에 소프트맥스 함수를 적용해야 합니다. 이에 대해서는 토큰 샘플링을 설명할 때 자세히 다루겠습니다.

## BART의 인코더와 디코더

- BART의 인코더 블록과 디코더 블록은 이전 절에서 소개한 원본 트랜스포머 구조와 매우 유사합니다. 간단하게 나타내기 위해 인코더 블록과 디코더 블록 하나씩 그림으로 나타내면 다음과 같습니다.

![스크린샷 2025-04-17 오후 10 14 37](https://github.com/user-attachments/assets/474d572c-1c6e-49c7-93e3-9ad1a719a086)

- 10장 1절에서 소개한 원본 트랜스포머 구조와 다른 점은 피드포워드 네트워크에서 렐루 활성화 함수 대신 **젤루**<sup>GeLU</sup> 함수를 사용하는 것입니다. 젤루 함수에 대해서는 잠시 후에 알아보겠습니다. 인코더와 디코더 안의 각 구성요소 위에 입출력의 크기나 관련 파라미터를 붉은 색 글씨로 표시했습니다. 인코더 입력부터 이 값을 하나씩 설명해 보겠습니다.

- (768,) 크기의 임베딩 벡터가 인코딩 블록에 입력되면 12개의 헤드에 나누어 전달됩니다. 따라서 각 헤드에 입력되는 벡터 크기는 (64,)가 됩니다. 어텐션의 마지막 밀집층을 통과하면서 다시 (768,) 크기의 벡터로 변환됩니다.
- BART는 인코더와 디코더에 드롭아웃 비율이 0.1인 드롭아웃 층을 사용합니다. 드롭아웃 층을 지난 후 잔차 연결이 나오고 층 정규화가 이어집니다. 그다음 피드포워드 네트워크가 나옵니다. 피드포워드의 첫 번째 밀집층은 임베딩 벡터 차원의 네 배인 (3072,)의 출력을 만듭니다. 두 번째 밀집층은 다시 원래 임베딩 차원인 (768,)의 출력을 만듭니다. 그다음 앞에서와 동일하게 드롭아웃, 잔차 연결, 층 정규화가 등장합니다. 이 세 층은 벡터의 차원을 바꾸지 않으므로 블록의 최종 출력 크기는 (768,)입니다.

> BART는 드롭아웃 층을 사용하지만 최신 LLM은 모델과 데이터셋이 커서 여러 에포크 동안 훈련하기 힘듭니다. 따라서 과대적합을 막기 위한 드롭아웃을 잘 사용하지 않는 것으로 알려져 있습니다.

> BART를 비롯해 많은 트랜스포머 모델에서 피드포워드 네트워크의 첫 번째 밀집층의 크기가 임베딩 벡터의 네 배입니다. 하지만 항상 이런 것은 아니며 모델에 따라 다를 수 있습니다.

- BART의 피드포워드 네트워크에서 사용하는 활성화 함수는 젤루입니다. 이 함수는 입력에 표준 정규 분포의 **누적 분포 함수**<sup>cumulative distribution function for gaussian distribution</sup>를 곱합니다. 이 누적 분포 함수를 계산하려면 적분이 필요합니다. 그래서 대부분의 딥러닝 프레임워크들은 복잡한 적분 대신 근사값을 구할 수 있는 간단한 공식을 사용합니다.

![스크린샷 2025-04-18 오후 10 08 41](https://github.com/user-attachments/assets/eaeb30a6-4170-4e55-ba97-75bfb6d16bbe)


- 젤루 함수의 그래프는 다음처럼 렐루 함수와 비슷하지만 원점에서 부드럽게 변하기 때문에 미분 가능합니다.

![스크린샷 2025-04-18 오후 10 08 52](https://github.com/user-attachments/assets/4650d9ed-c7bd-44fa-80ae-249fcbbd0d6d)

- 그럼 이제 디코더 블록을 살펴보죠. 디코더 블록도 인코더 블록과 매우 흡사한 처리 과정을 밟습니다. 맨 처음 (768,) 크기의 입력이 마스크드 멀티 헤드 어텐션 층에 12개의 헤드에 나뉘어져 들어갑니다. 어텐션 바로 뒤에 등장하는 밀집층이 출력 차원을 (768,)로 만들고 이어서 드롭아웃, 잔차 연결, 층 정규화가 등장합니다.
- 두 번째 멀티 헤드 어텐션은 인코더의 출력을 사용하는 크로스 어텐션입니다. 인코더의 출력은 키와 값으로 전달되고, 디코더의 벡터를 쿼리로 사용하여 어텐션을 계산합니다. 그다음은 역시 동일하게 드롭아웃, 잔차 연결, 층 정규화가 등장합니다. 디코더의 출력 크기도 입력과 동일하게 (768,)입니다. 
- BART의 전체 구조와 인코더, 디코더 블록의 상세 구성까지 모두 알아보았습니다. 이제 본격적으로 BART 모델을 로드하여 텍스트를 요약해보겠습니다.

## 허깅페이스로 KoBART 모델 로드하기

- BART와 같은 트랜스포머 모델을 사용하려면 **허깅페이스**<sup>HuggingFace</sup>의 `transformers` 패키지를 사용합니다. 허깅페이스는 트랜스포머 기반의 오픈소스 모델을 공유하는 곳으로 유명합니다. `transformers` 패키지를 사용하면 허깅페이스에서 공유되는 사전 훈련된 오픈소스 `LLM`을 쉽게 로드하여 사용할 수 있고, 직접 미세 튜닝한 모델도 허깅페이스를 통해 공유할 수 있습니다.
- `transformers` 패키지의 인기가 높아지자 허깅페이스는 `LLM`의 개발에 필요한 다양한 패키지를 계속 개발하여 공개하고 있습니다. 또한 모델뿐만 아니라 데이터셋과 교육 자료도 풍부합니다. 덕분에 `LLM`에 대해 배우고 활용하고 싶을 때 가장 먼저 찾아볼 곳이 허깅페이스가 되었습니다. 더욱이 이제는 트랜스포머 기반 `LLM` 모델을 넘어서 컴퓨터 비전과 오디오 등 다른 분야의 모델도 제공하고 있습니다. 
- 높은 인기 덕에 구글 코랩에는 `transformers` 패키지가 이미 설치되어 있습니다. 다음처럼 `transformers` 패키지에서 `pipeline()` 함수를 로드하여 텍스트 요약을 위한 파이프라인을 준비해보죠.

> 만약 로컬 컴퓨터에서 코드를 실행한다면 다음 명령으로 `transformers` 패키지를 먼저 설치해 주세요.

```
!pip install transformers
```


```python
from transformers import pipeline
pipe = pipeline(task='summarization', device=0)
```

```
No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).
Using a pipeline without specifying a model name and revision in production is not recommended.
```

```
config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]
pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]
tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]
vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]
merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]
model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]
```

- `pipeline()` 함수를 사용하면 허깅페이스에 있는 LLM을 사용하기 위해 수행할 몇 가지 단계를 한 번에 실행할 수 있어 매우 편리합니다. 이 함수는 매개변수가 많습니다. 가장 기본이 되는 매개변수는 수행할 작업의 종류를 지정하는 task입니다. 앞의 코드에서는 요약 작업을 위해 `summarization`으로 지정했습니다.
- `task` 매개변수에 지정할 수 있는 다른 옵션으로는 텍스트 분류를 위한 `text-classification` 텍스트 생성을 위한 `text-generation`, 번역을 위한 `translation`등이 있습니다. 전체 옵션은 온라인 문서(https://bit.ly3D0BLxW)를 참고하세요.
- GPU를 사용하려면 device 매개변수를 지정해야 합니다. 이 장에서는 코랩의 T4 GPU 하나를 사용한다고 가정하므로 device=0으로 지정했습니다.

> `pipeline()` 함수는 각 작업에 연관된 파이프라인 클래스의 인스턴스 객체를 반환합니다. 예를 들어 요약 작업의 경우 `SummarizationPipeline` 클래스의 객체를 만들어 반환합니다. `transformers` 패키지에는 이렇게 작업마다 정의된 파이프라인 클래스가 있습니다. 하지만 작업에 특화된 클래스를 호출하여 사용하는 것보다 `pipeline()` 함수의 task 매개변수에 작업을 지정하는 것이 훨씬 간편합니다. 

- `pipeline()` 함수의 출력 결과를 보면 모델을 지정하지 않았기 때문에 요약 작업을 위한 기본 모델인 `sshleifer/distilbart-cnn-12-6`을 사용한다고 나타나 있습니다. 이어서 필요한 파일을 허깅페이스에서 다운로드합니다. 다른 모델이나 리비전을 지정하고 싶다면 model 매개변수와 revision 매개변수를 사용할 수 있습니다. 따라서 다음 코드는 앞의 코드와 동일한 작업을 수행합니다.
> distilbart는 전직 허깅페이스 연구원인 Sam Shleifer가 CNN 뉴스 데이터셋으로 훈련한 BART 변종 모델입니다. 이 모델은 원본 모델의 출력을 흉내내도록 더 작은 모델을 훈련하는 지식 정제(knowledge distillation) 기법을 사용한 것으로 추정됩니다.

```python
pipe = pipeline(task='summarization',
                model='sshleifer/distilbart-cnn-12-6', device=0)
```

> model 매개변수를 지정하면 자동으로 최신 리비전의 파일을 다운로드합니다.

- 이 모델 이름은 허깅페이스 웹사이트의 경로를 나타냅니다. 따라서 다음처럼 https://huggingface.co/shleifer/distilbart-cnn-12-6 에 접속하면 이 모델에 대한 상세 내용을 확인할 수 있습니다.
- 반대로 허깅 페이스 웹사이트에서 찾은 어떤 모델을 `pipeline()` 함수로 로드하고 싶다면 URL에서 https://huggingface.co/ 다음에 나오는 경로를 model 매개변수에 지정하면 됩니다.
- `pipe`객체로 텍스트를 요약하려면 8장에서 해보았던 것처럼 이 객체를 마치 함수처럼 호출하면 됩니다. 다음과 같이 반 고흐에 관련 위키백과 텍스트를 요약해 보겠습니다.

```python
sample_text = """Vincent Willem van Gogh was a Dutch Post-Impressionist painter who is among the most famous and influential figures in the history of Western art. In just over a decade, he created approximately 2100 artworks, including around 860 oil paintings, most of them in the last two years of his life. His oeuvre includes landscapes, still lifes, portraits, and self-portraits, most of which are characterised by bold colours and dramatic brushwork that contributed to the rise of expressionism in modern art. Van Gogh's work was beginning to gain critical attention before he died from a self-inflicted gunshot at age 37. During his lifetime, only one of Van Gogh's paintings, The Red Vineyard, was sold.
"""
pipe(sample_text)
```

```
[{'summary_text': " Vincent Willem van Gogh was a Dutch Post-Impressionist painter . His oeuvre includes landscapes, still lifes, portraits and self-portraits . Van Gogh's work was beginning to gain critical attention before he died from a self-inflicted gunshot at age 37 ."}]
```

- 아주 간단하군요! 출력은 딕셔너리의 리스트입니다. 이로부터 여러 개의 텍스트를 pipe 객체에 전달할 수 있다는 힌트를 얻을 수 있습니다. 예를 들어 `pipe(['...', '...'])`와 같이 여러 개의 텍스트를 리스트로 감싸서 호출할 수 있습니다.
- 딕셔너리 안에서 `summary_text` 키에 매핑된 값이 요약된 문자열입니다. 대략적으로 반 고흐의 작품과 사망에 관한 정보를 잘 정리하고 있는 것 같군요.
- 이 BART 모델은 CNN 텍스트 데이터셋에서 미세 튜닝된 모델입니다. 이 모델은 텍스트를 56\~142자 사이의 길이로 요약합니다. 만약 이 두 값의 범위를 바꾸고 싶다면 각각 `min_length`와 `max_length` 매개변수를 사용하세요.
- 아쉽게도 sshleifer/distilbart-cnn-12-6 모델은 영어 데이터셋에서 훈련되었기 때문에 한글과 같은 다국어를 지원하지 못합니다. 하지만 BART 베이스 모델을 사용해 한글 데이터셋에서 미세 튜닝한 다른 모델을 허깅페이스에서 찾아볼 수 있습니다.
- 허깅페이스 웹사이트에서 맨 위쪽 Models를 클릭합니다(https://huggingface.co/models).
- 그리고 나서 다음 그림과 같이 검색어로 `kobart`를 입력합니다. `KoBART`는 `SKT`에서 만든 `BART`기반의 한국어 인코더-디코더 모델입니다(https://github.com/SKT-AI/KoBART).
- KoBART를 요약 작업에 맞춰 미세 튜닝한 모델이 여럿 보입니다. 그 중에서 비교적 최근에 업데이트된 `EbanLee/kobart-summary-v3` 모델을 사용해 보겠습니다.

```python
kobart = pipeline(task='summarization',
                  model='EbanLee/kobart-summary-v3', device=0)
```

```python
ko_text = """하나, ‘입문자 맞춤형 7단계 구성’을 따라가며 체계적으로 반복하는 탄탄한 학습 설계!
이 책은 데이터 분석의 핵심 내용을 7단계에 걸쳐 반복 학습하면서 자연스럽게 머릿속에 기억되도록 구성했습니다. [핵심 키워드]와 [시작하기 전에]에서 각 절의 주제에 대한 대표 개념을 워밍업하고, 이론과 실습을 거쳐 마무리에서는 [핵심 포인트]와 [확인 문제]로 한번에 복습합니다. ‘혼자 공부할 수 있는’ 커리큘럼을 그대로 믿고 끝까지 따라가다 보면 데이터 분석 공부가 난생 처음인 입문자도 무리 없이 책을 끝까지 마칠 수 있습니다!
둘, 실제로 일어날 법한 흥미로운 스토리에 담긴 문제를 직접 해결하며 익히는 ‘진짜’ 데이터 분석!
현장감 넘치는 스토리를 통해 데이터를 다루는 방법을 알려 주어 ‘파이썬’과 ‘데이터’가 낯설어도 몰입감 있는 학습을 할 수 있도록 구성했습니다. 이 책에서는 API와 웹 스크래핑을 통해 실제 도서관 데이터와 온라인 서점 웹사이트에서 데이터를 가져오는 등 내 주변에 있는 데이터를 직접 수집할 수 있는 방법을 가이드합니다. 또한 판다스, 넘파이, 맷플롯립 등 데이터 분석에 유용한 각종 파이썬 라이브러리를 활용해 보며 코딩 감각을 익히고, 핵심 통계 지식으로 기본기를 탄탄하게 다질 수 있습니다. 마지막에는 분석을 바탕으로 미래를 예측하는 머신러닝까지 맛볼 수 있어 데이터 분석의 처음부터 끝까지 제대로 배울 수 있습니다.
셋, ‘혼공’의 힘을 실어줄 동영상 강의와 혼공 학습 사이트 지원!
책으로만 학습하기엔 여전히 어려운 입문자를 위해 저자 직강 동영상도 지원합니다. 또한 학습을 하며 궁금한 사항은 언제든지 저자에게 질문할 수 있도록 학습 사이트를 제공합니다. 저자가 질문 하나하나에 직접 답변을 달아 주는 것은 물론, 관련 최신 기술과 정보도 얻을 수 있습니다. 게다가 혼자 공부하고 싶지만 정작 혼자서는 자신 없는 사람들을 위해 혼공 학습단을 운영합니다. 혼공 학습단과 함께하면 마지막까지 포기하지 않고 완주할 수 있을 것입니다.
▶ https://hongong.hanbit.co.kr
▶ https://github.com/rickiepark/hg-da
넷, 언제 어디서든 가볍게 볼 수 있는 혼공 필수 [용어 노트] 제공!
꼭 기억해야 할 핵심 개념과 용어만 따로 정리한 [용어 노트]를 제공합니다. 처음 공부하는 사람들이 프로그래밍을 어려워하는 이유는 낯선 용어 때문입니다. 그러나 어려운 것이 아니라 익숙하지 않아서 헷갈리는 것이므로, 용어나 개념이 잘 생각나지 않을 때는 언제든 부담 없이 [용어 노트]를 펼쳐 보세요. 제시된 용어 외에도 새로운 용어를 추가하면서 자신만의 용어 노트를 완성해가는 과정도 또 다른 재미가 될 것입니다.
"""

kobart(ko_text)
```

```python
[{'summary_text': '이 책은 데이터 분석의 핵심 내용을 7단계에 걸쳐 반복 학습하면서 머릿속에 기억되도록 구성했습니다. 독자 공부할 수 있는 커리큘럼을 그대로 믿고 끝까지 따라가다 보면 데이터 분석 공부가 난생 처음인 입문자도 무리 없이 책을 끝까지 마칠 수 있습니다. 현장감 넘치는 스토리를 통해 데이터를 다루는 방법을 알려 주어 몰입감 있는 학습을 할 수 있도록 구성했습니다. 저자가 질문 하나하나에 직접 답변을 달아 주는 것은 물론, 최신 기술과 정보도 얻을 수 있습니다. 혼공 학습단과 함께하면 마지막까지 포기하지 않고 완주할 수 있을 것입니다. 꼭 기억해야 할 핵심 개념과 용어만 따로 정리한 [용어 노트]를 제공합니다. 새로운 용어를 추가하면서 자신만의 용어 노트를 완성해가는 과정도 재미가 될 것입니다.'}]
```

- 이 모델은 기본적으로 최대 300자까지 요약을 만들기 때문에 이전 예제보다 요약 결과가 조금 더 깁니다. 이 모델은 **빔 서치**<sup>beam search</sup>라는 방법을 사용해 텍스트를 생성하기 때문에 실행할 때마다 결과가 다를 수 있습니다. 즉, 책의 결과와 다르게 출력되는 것이 정상입니다. 

> 빔 서치는 토큰 단계마다 가장 가능성이 높은 n개의 시퀀스를 유지하면서 다음 토큰을 생성하여 이어가는 방법입니다. 

> 모델이 최대 300자 까지 출력한다는 것은 어떻게 알수 있을까요?<br>
> 허깅페이스의 모델은 웹 사이트에서 자세한 설정을 확인할 수 있습니다. EbanLee/kobart-summary-v3 모델 페이지(https://huggingface.co/EbanLee/kobart-summary-v3)에 접속한 다음 Files 탭을 선택합니다. 파일 목록에서 config.json을 클릭하면 자세한 모델 설정을 볼 수 있습니다.<br>
> 또는 모델 객체의 config 속성을 확인할 수도 있습니다. 파이프라인 함수로 로드한 모델은 파이프라인 객체인 kobart의 model 속성에 저장되어 있습니다. 따라서 이 모델의 설정을 확인하려면 다음처럼 kobart.model.config를 출력해 보면 됩니다.

```python
print(kobart.model.config)
```

```python
BartConfig {
  "_attn_implementation_autoset": true,
  "_name_or_path": "EbanLee/kobart-summary-v3",
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": false,
  "architectures": [
    "BartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "author": "EbanLee(rudwo6769@gmail.com)",
  "bos_token_id": 1,
  "classif_dropout": 0.1,
  "classifier_dropout": 0.1,
  "d_model": 768,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 3072,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 6,
  "decoder_start_token_id": 1,
  "do_blenderbot_90_layernorm": false,
  "dropout": 0.1,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 3072,
...
  "use_cache": true,
  "vocab_size": 30000
}
```

- 한국어 데이터셋으로 미세 튜닝한 모델을 파이프라인 함수를 통해 로드하여 사용하는 방법을 알아보았습니다. 그런데 모델에 입력한 텍스트를 어떻게 토큰으로 나누는 것일까요? 9장에서는 공백을 기준으로 각 단어를 정수로 나눈 것이 토큰이라고 설명했습니다. 이 방법이 가장 기본적이지만 LLM에서는 더 고급 방법들이 사용됩니다. 이어서 이에 대해 알아보겠습니다.

## 텍스트 토큰화

- **토큰화**<sup>tokenization</sup>는 텍스트를 **토큰**<sup>token</sup>라는 단위로 분할하는 과정입니다. 중요한 점은 이런 토큰화를 LLM 모델 자체가 수행하지 않는다는 것입니다. 트랜스포머 모델의 구조를 살펴보았듯 이 모델에는 텍스트를 토큰으로 분할하는 구성 요소가 없습니다. 이 모델은 이미 텍스트가 토큰으로 분할되고 각 토큰에 정수 아이디가 할당된 후 이 정수 리스트가 전달된다고 가정합니다.
- 토큰화를 수행하는 모델을 **토크나이저**<sup>tokenizer</sup>라 부르며 모델과 별도로 존재합니다. 이를 일종의 전처리 과정으로 생각할 수 있지만 토크나이저는 훈련 데이터로부터 최적의 어휘 사전을 학습하는 모델에 가깝습니다. 또한 토큰의 임베딩 벡터는 토크나이저에 있지 않고 LLM 모델의 임베딩 층에 있다는 것을 기억하세요.

