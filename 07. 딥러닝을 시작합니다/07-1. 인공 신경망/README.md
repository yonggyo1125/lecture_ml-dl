# 인공 신경망

## 키워드 정리

- **인경 신경망**
  - 생물학적 뉴런에서 영감을 받아 만든 머신러닝 알고리즘입니다. 이름이 신경망 이만 실제 우리 뇌를 모델링한 것은 아닙니다.
  - 신경망은 기존의 머신러닝 알고리즘으로 다루기 어려웠던 이미지, 음성, 텍스트 분야에서 뛰어난 성능을 발휘하면서 크게 주목받고 있습니다.
  - 인공 신경망 알고리즘을 종종 딥러닝이라고 부릅니다.
- **텐서플로**
  - 구글이 만든 딥러닝 라이브러리로 매우 인기가 높습니다. **CPU**와 **GPU**를 사용해 인공 신경망 모델을 효율적으로 훈련하여 모델 구축과 서비스에 필요한 다양한 도구를 제공합니다.
  - 텐서플로 2.0부터는 신경망 모델을 빠르게 구성할 수 있는 케라스를 핵심 API로 채택하였습니다.
  - 케라스를 사용하면 간단한 모델에서 아주 복잡한 모델한 모델까지 손쉽게 만들 수 있습니다.
- **밀집층**
  - 가장 간단한 인공 신경망의 층입니다.
  - 인공 신경망에는 여러 종류의 층이 있습니다. 밀집층에서는 뉴런들이 모여 연결되어 있기 때문에 완전 연결 층이라고도 부릅니다.
  - 특별히 출력층에 밀집층을 사용할 때는 분류하려는 클래스오 동일한 개수의 뉴런을 사용합니다.
- **원-핫 인코딩**
  - 정수값을 배열에서 해당 정수의 위치의 원소만 1이고 나머지는 모두 0으로 변환합니다.
  - 이런 변환이 필요한 이유는 다중 분류에서 출력층에서 만든 확률과 크로스 엔트로피 손실을 계산하기 위해서 입니다. 텐서플로에서는 `sparse_categorical_entropy` 손실을 지정하면 이런 변환을 수행할 필요가 없습니다.

## 핵심 패키지와 함수

### TensorFlow

- **Dense**
  - 신경망에서 가장 기본 층인 밀집층을 만드는 클래스입니다.
  - 이 층에서 첫 번째 매개변수에는 뉴런의 개수를 지정합니다.
  - `activation` 매개변수에는 사용할 활성화 함수를 지정합니다. 대표적으로 `sigmoid`, `softmax` 함수가 있습니다. 아무것도 지정하지 않으면 활성화 함수를 사용하지 않습니다.
- **Sequential**
  - 케라스에서 신경망 모델을 만드는 클래스입니다.
  - 이 클래스의 객체를 생성할 때 신경망 모델에 추가할 층을 지정할 수 있습니다.
  - 추가할 층이 1개 이상일 경우 파이썬 리스트로 전달합니다.
- **compile()**
  - 모델 객체를 만든 후 훈련하기 전에 사용할 손실 함수와 측정 지표등을 지정하는 메서드입니다.
  - `loss` 매개변수에 손실함수를 지정합니다. 이진 분류일 경우 `binary_crossentropy`, 다중 분류일 경우 `categorical_crossentropy`로 지정합니다. 회귀 모델일 경우 `mean_square_error` 등으로 지정할 수 있습니다.
  - `metrics` 매개변수에 훈련 과정에서 측정하고 싶은 지표를 지정할 수 있습니다. 측정 지표가 1개 이상일 경우 리스트로 전달합니다.
- **fit()**
  - 모델을 훈련하는 메서드입니다.
  - 첫 번째와 두 번째 매개변수에 입력하는 타깃 데이터를 전달합니다.
  - `epochs` 매개변수에 전체 데이터에 대해 반복할 에포크 횟수를 지정합니다.
- **evaluate()**
  - 모델 성능을 평가하는 메서드입니다.
  - 첫 번째와 두 번째 매개변수에 입력과 타깃 데이터를 전달합니다.
  - `compile()` 메서드에서 `loss` 매개변수에 지정한 손실 함수의 값과 `metrics` 매개변수에 지정한 측정 지표를 출력합니다.

## 패션 MNIST

- 패션 MNIST 데이터셋을 사용하겠습니다.
- 이 데이터셋은 10종류의 패션 아이템으로 구성되어 있습니다.

> MNIST
>
> 머신러닝과 딥러닝을 처음 배울 때 많이 사용하는 데이터셋이 있습니다. 머신러닝에서는 붓꽃 데이터셋이 유명합니다. 딥러닝에서는 **MNIST 데이터셋**이 유명합니다. 이 데이터는 손으로 쓴 0\~9까지의 숫자로 이루어져 있습니다. **MNIST**와 크기, 개수가 동일하지만 숫자 대신 패션 아이템으로 이루어진 데이터가 바로 **패션 MNIST**입니다.

- 패션 MNIST 데이터는 워낙 유명하기 때문에 많은 딥러닝 라이브러리에서 이 데이터를 가져올 수 있는 도구를 제공합니다. 여기에서는 **텐서플로**(TensorFlow)를 사용해 이 데이터를 불러오겠습니다.

```python
from tensorflow import keras

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()
```

- 다음 명령으로 텐서플로의 **케라스**(Keras) 패키지를 임포트하고 패션 MNIST 데이터를 다운로드 합니다.
- `keras.datasets.fashion_mnist` 모듈 아래 `load_data()` 함수는 친절하게 훈련 데이터와 테스트 데이터를 나누어 반환합니다. 이 데이터는 각각 입력과 타깃의 쌍으로 구성되어 있습니다.

```python
print(train_input.shape, train_target.shape)
```

- 전달받은 데이터의 크기를 확인해 보면

```
(60000, 28, 28) (60000,)
```

- 훈련 데이터는 60,000개의 이미지로 이루어져 있습니다. 각 이미지는 28 X 28 크기 입니다. 타깃도 60,000개의 원소가 있는 1차원 배열입니다.

<img width="522" alt="스크린샷 2024-11-15 오후 1 58 51" src="https://github.com/user-attachments/assets/9649d363-f8e3-46c5-85ee-0c8ae7d1eff3">

```python
print(test_input.shape, test_target.shape)
```

- 테스트 세트의 크기를 확인해 보면

```
(10000, 28, 28) (10000,)
```

- 테스트 세트는 10,000개의 이미지로 이루어져 있습니다.

```python
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 10, figsize=(10,10))
for i in range(10):
    axs[i].imshow(train_input[i], cmap='gray_r')
    axs[i].axis('off')
plt.show()
```

- 훈련 데이터에서 몇 개의 샘플을 그림으로 출력해 보면 어떤 이미지인지 볼 수 있으므로 문제를 이해하는데 큰 도움이 됩니다.

<img width="542" alt="스크린샷 2024-11-15 오후 2 03 22" src="https://github.com/user-attachments/assets/a16d232e-8b3f-483c-9b90-9506dbc900c7">

- 반전된 흑백 이미지 입니다.

```python
print([train_target[i] for i in range(10)])
```

- 파이썬의 리스트 내포를 사용해서 처음 10개 샘플 타깃값을 리스트로 만든 후 출력합니다.

```
[9, 0, 0, 3, 0, 2, 7, 2, 5, 5]
```

- 패션 MNIST의 타깃은 0\~9까지의 숫자 레이블로 구성됩니다. 각 숫자의 의미는 아직 모르지만 마지막 2개의 샘플이 같은 레이블 (숫자5)을 가지고 있습니다.
- 앞서 출력한 이미지를 보면 이 2개의 샘플은 같은 종류의 신발 입니다.
- 패션 MNIST에 포함된 10개 레이블의 의미는 다음과 같습니다.

<img width="616" alt="스크린샷 2024-11-15 오후 2 07 41" src="https://github.com/user-attachments/assets/98e2487e-de59-40a6-9268-0f749d47f7e7">

```python
import numpy as np

print(np.unique(train_target, return_counts=True))
```

- 넘파이 `unique()` 함수로 레이블 당 샘플의 개수를 확인합니다.

```
(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]))
```

- 0\~9까지 레이블마다 정확히 6,000개의 샘플이 들어 있는 것을 볼 수 있습니다.

## 로지스틱 회귀로 패션 아이템 분류하기

- 이 훈련 샘플은 60,000개나 되기 때문에 전체 데이터를 한꺼번에 사용하여 모델을 훈련하는 것보다 샘플을 하나씩 꺼내서 모델을 훈련하는 방법이 더 효율적입니다.
- 이런 상황에 잘 맞는 방법은 경사 하강법입니다. - **SGDClassifier**

> 넘파이 배열의 `nbytes` 속성에 실제 해당 배열이 차지하는 바이트 용량이 저장되어 있습니다.

- **SGDClassifier** 클래스의 `log_loss`로 지정하여 로지스틱 손실 함수를 최소화하는 **확률적 경사 하강법 모델**을 만듭니다.
- **SGDClassifier**를 사용할 때 **표준화 전처리된 데이터**를 사용했습니다.
- 그 이유는 확률적 경사 하강법은 여러 특성 중 기울기가 가장 가파른 방향을 따라 이동합니다. 만약 특성마다 값의 범위가 많이 다르면 올바르게 손실 함수의 경사를 내려 올 수 없습니다. 패션 MNIST의 경우 각 픽셀은 0\~255 사이의 정숫값을 가집니다. 이런 이미지의 경우 보통 255로 나누어 0\~1 사이의 값으로 정규화합니다. 이는 표준화는 아니지만 양수 값으로 이루어진 이미지를 전처리할 때 널리 사용하는 방법입니다.

```python
train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)
```

- `reshape()` 메서드를 사용해 2차원 배열인 각 샘플을 1차원 배열로 펼칩니다.
- **SGDClassifier**는 2차원 입력을 다루지 못하기 때문에 각 샘플을 1차원 배열로 만들어야 합니다.
- `reshape()` 메서드의 두 번째 매개변수를 28 X 28 이미지 크기에 맞게 지정하면 첫 번째 차원(샘플 개수)은 변하지 않고 원본 데이터의 두 번째, 세 번째 차원이 1차원으로 합쳐집니다.

```python
print(train_scaled.shape)
```

- 변환된 `train_scaled`의 크기를 확인해 봅니다.

```
(60000, 784)
```

- 기대한 대로 784개의 픽셀로 이루어진 60,000개의 샘플이 준비되었습니다.

```python
from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier

sc = SGDClassifier(loss='log_loss', max_iter=5, random_state=42)

scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)
print(np.mean(scores['test_score']))
```

- **SGDClassifier** 클래스와 `cross_validate` 함수를 사용해 이 데이터에서 교차 검증으로 성능을 확인해 봅시다.

```
0.8196000000000001
```

- 여기에서는 **SGDClassifier**의 반복 횟수(max_iter)를 5번으로 지정했습니다. 반복 횟수를 늘려도 성능이 크게 향상되지는 않습니다.
- 직접 9나 20등의 여러 숫자를 넣어서 테스트해 봅시다.

```
0.8303833333333334  # max_iter=9
0.8436999999999999  # max_iter=20
```

- 만족할 만한 수준은 아닙니다.
- 잠시 로지스틱 회귀 공식을 떠올려 보면

![스크린샷 2024-11-16 오전 12 16 49](https://github.com/user-attachments/assets/a5412935-54f6-4460-915f-b4b20553b965)

- 이 식을 패션 MNIST 데이터에 맞게 변형하면 다음과 같습니다.

![스크린샷 2024-11-16 오전 12 16 59](https://github.com/user-attachments/assets/116dce6e-1e75-4348-9976-3f8dbd1782be)

- 총 784개의 픽셀, 즉 특성이 있으므로 아주 긴 식이 만들어집니다. 가중치 개수도 많아지기 때문에 a, b, c 대신에 w1, w2, w3과 같은 식으로 바꾸었습니다. 마지막에 절편 b를 더합니다.
- 생선에 대한 특성을 픽셀 특성으로 바꾼 것뿐입니다. 대신 개수가 아주 많아졌습니다.

- 두 번째 레이블인 바지에 대한 방정식은 다음과 같습니다.

![스크린샷 2024-11-16 오전 12 17 13](https://github.com/user-attachments/assets/9e4446a4-73b3-42b4-b8d8-57b0258afd50)

- 이 식은 티셔츠에 대한 선형 방정식과 매우 비슷합니다. 동일하게 784개의 픽셀값을 그대로 사용하고 있습니다. 다만 바지에 대한 출력을 계산하기 위해 가중치와 절편은 다른 값을 사용해야 합니다.
- 티셔츠와 같은 가중치를 사용한다면 바지와 티셔츠를 구분할 수 없습니다.
- 이런 식으로 나머지 클래스에 대한 선형 방정식을 모두 생각해 볼 수 있습니다.
- **SGDClassifier** 모델은 패션 MNIST 데이터의 클래스를 가능한 잘 구분할 수 있도록 이 10개의 방정식에 대한 모델 파라미터 (가중치와 절편)을 찾습니다.

- 이 방정식의 계산을 그림으로 나타내면 다음과 같습니다.

![스크린샷 2024-11-16 오전 12 17 45](https://github.com/user-attachments/assets/0d495d4d-bb36-49f6-aec7-a1053898bfac)

- 첫 번째 픽셀1이 w1과 곱해져서 z*티셔츠에 더해집니다. 두 번째 픽셀2도 w2와 곱해져서 z*티셔츠에 더해집니다. 마지막 픽셀784도 w784와 곱해져 z*티셔츠에 절편 b를 더합니다. z*바지에 대해서도 동일한 계산 과정이 수행됩니다.
- 여기에서 중요한 점은 앞에서도 언급했듯이 티셔츠를 계산하기 위해 픽셀 784개와 곱하는 가중치 784개(w1~w784), 절편(b)이 바지를 계산하기 위해 픽셀 784개와 곱하는 가중피 784개(w1~w784), 절편(b')과 다르다는 것입니다.
- z*티셔츠, z*바지와 같이 10개의 클래스에 대해 선형 방정식을 모두 계산한 다음에는 소프트맥스 함수를 통과하여 각 클래스에 대한 확률을 얻을 수 있습니다.

## 인공신경망

- 가장 기본적인 인공 신경망은 확률적 경사 하강법을 사용하는 로지스틱 회귀와 같습니다. 어떻게 하면 인공 신경망으로 성능을 높일 수 있는 지 알아봅시다.

![스크린샷 2024-11-16 오후 9 06 44](https://github.com/user-attachments/assets/919236e5-f170-4474-9921-fd0fc7125979)

- 앞서 로지스틱 회귀를 표현한 그림과 매우 비슷합니다. 여기에서는 z*티셔츠, z*바지를 z1, z2와 같이 아래첨자를 사용하도록 바꾸었습니다. 클래스가 총 10개 이므로 z10까지 계산합니다. z1\~z10을 계산하고 이를 바탕으로 클래스를 예측하기 때문에 신경망의 최종 값을 만든다는 의미에서 **출력층**(output layer)이라고 부릅니다.
- 인공 신경망에서는 z 값을 계산하는 단위를 **뉴런**(neuron)이라고 부릅니다. 하지만 뉴런에서 일어나는 일은 선형 계산이 전부입니다. 이제는 뉴런이라는 표현 대신에 **유닛**(unit)이라고 부르는 사람이 더 많아지고 있습니다.
- 그 다음 픽셀1, 픽셀2를 x1, x2와 같이 바꾸었습니다. 역시 아래첨자를 사용해 784번째 픽셀에 해당하는 x784까지 나타냈습니다. 인공 신경망은 x1\~x784까지를 **입력층**(input layer)이라고 부릅니다. 즉 입력층은 픽셀값 자체이고 특별한 계산을 수행하지 않습니다. 하지만 많은 사람이 입력층이라 부르기 때문에 여기에서도 관례를 따르겠습니다.
- z1을 만들기 위해 픽셀1인 x1에 곱해지는 가중치는 w1,1이라고 쓰고 z2를 만들기 위해 픽셀 1인 x1에 곱해지는 가중치는 w1,2라고 씁니다. 절편은 뉴런마다 하나씩이므로 순서대로 b1, b2와 같이 나타내었습니다.

- 인공 신경망은 1943년 **워런 매컬러**(Warren McCulloch)와 **월터 피츠**(Walter Pitts)가 제안한 뉴런 모델로 거슬러 올라갑니다. 이를 **매컬러-피츠 뉴런**이라고 부릅니다.
- 이런 인공 뉴련은 다음과 같은 생물학적 뉴런에서 영감을 얻어 만들어졌습니다.

![스크린샷 2024-11-16 오후 9 16 51](https://github.com/user-attachments/assets/78160ed9-1dd1-4787-9374-f3de90103294)

- 생물학적 뉴런은 수상 돌기로부터 신호를 받아 세포체에 모읍니다. 신호가 어떤 임계값에 도달하면 축삭 돌기를 통하여 다른 세포에 신호를 전달합니다. 앞서 그렸던 인공 신경망의 출력층에 있는 인공 뉴런 하나와 비교하면 비슷합니다.
- 하지만 생물학적 뉴런이 가중치(w1,1, w2,1)와 입력을 곱하여 출력을 만드는 것은 아닙니다. 앞서 보았던 시그모이드 함수나 소프트맥스 함수를 사용하는 것은 더욱 아닙니다. 인공 뉴런은 생물학적 뉴런의 모양을 본뜬 수학 모델이 불과합니다. 즉, 생물학적 뉴런이 하는 일을 실제로 구현한 것이 아닙니다.
- 인공 신경망은 우리 뇌에 있는 뉴런과 같지 않습니다. 인공 신경망이란 말을 많이 사용할 수 밖에 없지만 정말 뇌 속에 있는 무언가를 만드는 일이 아니라는 것
- 인공 신경망은 기존의 머신러닝 알고리즘이 잘 해결하지 못했던 문제에서 높은 성능을 발휘하는 새로운 종류의 머신러닝 알고리즘일 뿐입니다.

![스크린샷 2024-11-16 오후 9 22 03](https://github.com/user-attachments/assets/e2d349b5-5c30-4f46-93bf-d445df4e9229)

> 딥러닝은 인공 신경망과 거의 동의어로 사용되는 경우가 많습니다. 혹은 심층 신경망(deep neural network, DNN)을 딥러닝이라고 부릅니다. 심층 신경망은 여러 개의 층을 가진 인공 신경망입니다.

- 확률적 경사 하강법을 사용한 로지스틱 회귀 모델이 가장 간단한 인공 신경망이라면 인공 신경망을 만들어도 성능이 좋아지지는 않을 듯 합니다. 하지만 인공 신경망 모델을 만드는 최신 라이브러리들은 **SGDClassifier**에는 없는 몇 가지 기능을 제공합니다. 이런 기능 덕택에 더 좋은 성능을 얻을 수 있습니다.
- 그럼 가장 인기가 높은 딥러닝 라이브러리인 텐서플로를 사용해 인공 신경망 모델을 만들어 보겠습니다.

### 텐서플로와 케라스

- **텐서플로**는 구글이 2015년 11월 오픈소스로 공개한 딥러닝 라이브러리 입니다. 이때를 기점으로 딥러닝에 대한 개발자의 관심이 늘어났고, 2016년 3월 **알파고**가 이세돌 9단을 이겨 대중에 알려지면서 그야말로 폭발적으로 인기가 높아졌습니다.
- 텐서플로는 그 후 많은 발전을 거듭하면서 2019년 9월 2.0 버전이 릴리즈 되었습니다.

- 다음처럼 간단히 임포트하여 사용할 수 있습니다.

```python
import tensorflow as tf
```

- 텐서플로에는 저수준 API와 고수준 API가 있습니다. 바로 **케라스**(Keras)가 텐서플로의 고수준 API입니다. 케라스는 2015년 3월 **프랑소와 숄레**(Fancois Chollet)가 만든 딥러닝 라이브러리 입니다.
- 딥러닝 라이브러리가 다른 머신러닝 라이브러리와 다른 점 중 하나는 그래픽 처리 장치인 **GPU**를 사용하여 인공 신경망을 훈련한다는 것입니다.
- **GPU**는 벡터와 행렬 연산에 매우 최적화되어 있기 때문에 곱셈과 덧셈이 많이 수행되는 인공 신경망에 큰 도움이 됩니다.
- 케라스 라이브러리는 직접 GPU 연산을 수행하지 않습니다. 대신 GPU 연산을 수행하는 다른 라이브러리를 **백엔드**(backend)로 사용합니다. 예를 들면 텐서플로가 케라스의 백엔드 중 하나입니다. 이외에도 씨아노, CNTK와 같은 여러 딥러닝 라이브러리를 케라스 백엔드로 사용할 수 있습니다. 이런 케라스를 멀티-백엔드 케라스라고 부릅니다. 케라스 API만 익히면 다양한 딥러닝 라이브러리를 입맛대로 골라서 쓸 수 있습니다. 이를 위해 케라스는 직관적이고 사용하기 편한 고수준 API를 제공합니다.
- 프랑소와가 구글에 합류한 뒤 텐서플로 라이브러리에 케라스 API가 내장되었습니다. 텐서플로 2.0 부터는 케라스 API를 남기고 나머지 고수준 API를 모두 정리했고, 케라스는 텐서플로의 핵심 API가 되었습니다. 다양한 백엔드를 지원했던 멀티-백엔드 케라스는 2.3.1 버전 이후로 더 이상 개발되지 않습니다. 이제는 케라스와 텐서플로가 거의 동의어가 된 셈입니다.

- 텐서플로에 케라스를 사용하려면 다음과 같이 임포트 합니다.

```python
from tensorflow import keras
```

- 그럼 케라스 API를 사용해 패션 아이템을 분류하는 가장 간단한 인공 신경망을 만들어봅시다.

## 인공신경망으로 모델 만들기

- 여기에서는 앞서 로지스틱 회귀에서 만든 훈련 데이터 `train_scaled`와 `train_target`을 사용합니다.
- 로지스틱 회귀에서는 교차 검증을 사용해 모델을 평가했지만, 인공 신경망엣는 교차 검증을 잘 사용하지 않고 검증 세트를 별도로 덜어내어 사용합니다.
- 이렇게 하는 이유는 다음과 같습니다.

  - 딥러닝 분야의 데이터셋은 충분히 크기 때문에 검증 점수가 안정적이다.
  - 교차 검증을 수행하기에는 훈련 시간이 너무 오래 걸리기 때문입니다. 어떤 딥러닝 모델은 훈련하는 데 몇 시간, 심지어 며칠이 걸릴 수도 있습니다.

- 패션 MNIST 데이터셋이 그만큼 크지는 않지만, 관례를 따라 검증 세트를 나누어 보겠습니다.
- 사이킷런의 `train_test_split()` 함수를 사용합니다.

```python
from sklearn.model_selection import train_test_split

train_scaled, val_scaled, train_target, val_target = train_test_split(
    train_scaled, train_target, test_size=0.2, random_state=42)
```

> 사실 패션 MNIST 데이터는 이미 잘 섞인 데이터라서 `train_test_split()` 함수를 사용하지 않고 앞이나 뒤에서 10,000개 정도의 샘플을 덜어서 검증 세트로 만들어도 됩니다. 하지만 우리는 일반적인 상황을 가정하여 섞어서 나누었습니다.

- 훈련세트에서 20%를 검증 세트로 덜어 내었습니다. 훈련 세트와 검증 세트의 크기를 알아봅시다.

```python
print(train_scaled.shape, train_target.shape)
```

```
(48000, 784) (48000,)
```

```python
print(val_scaled.shape, val_target.shape)
```

```
(12000, 784) (12000,)
```

- 60,000개 중에 12,000개가 검증 세트로 분리되었습니다. 먼저 훈련 세트(train_scaled, train_target)로 모델을 만듭니다. 그 다음 검증 세트(val_scaled, val_target)로 훈련한 모델을 평가해보겠습니다. 

- 먼저 인공 신경망 그림의 오른쪽에 놓인 층을 만들어 보겠습니다. 이 층은 다음 그림처럼 10개의 패션 아이템을 분류하기 위해 10개의 뉴런으로 구성됩니다. 

![스크린샷 2024-11-16 오후 9 55 00](https://github.com/user-attachments/assets/13da172c-a2aa-4c77-ae8d-aae473ee7c73)

- 케라스와 레이어(keras.layers) 패키지 안에는 다양한 층이 준비되어 있습니다. 가장 기본이 되는 층은 **밀집층**(dense layer)입니다. 
- 밀집이라고 부르는 이유는 다음 그림에서 왼쪽에 있는 784개의 픽셀과 오른쪽에 있는 10개의 뉴런이 모두 연결된 선을 생각해 보면 784 X 10 = 7,840개의 연결된 선이 있습니다. 정말 빽빽하게 구성되므로 밀집층이라고 합니다. 

![스크린샷 2024-11-16 오후 9 58 01](https://github.com/user-attachments/assets/5fde9a80-d685-4215-850d-34a9216a8c9f)


```python
dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,))
```

```python
model = keras.Sequential([dense])
```

## 인공신경망으로 패션 아이템 분류하기

```python
model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])
```

```python
print(train_target[:10])
```

```
[7 3 5 8 6 9 3 3 9 9]
```

```python
model.fit(train_scaled, train_target, epochs=5)
```

```python
model.evaluate(val_scaled, val_target)
```

```
[0.44444453716278076, 0.8458333611488342]
```
