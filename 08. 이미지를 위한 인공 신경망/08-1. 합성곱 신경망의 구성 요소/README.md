# 합성곱 신경망의 구성 요소

## 키워드 정리

- **합성곱** : 밀집층과 비슷하게 입력과 가중치를 곱하고 절편을 더하는 선형 계산입니다. 하지만 밀집층과 달리 각 합성곱은 입력 전체가 아니라 일부만 사용하여 선형 계산을 수행합니다.
- **필터** : 합성곱 층의 **필터**는 밀집층의 뉴런에 해당합니다. 필터의 가중치와 절편을 종종 커널이라고 부릅니다. 자주 사용되는 커널의 크기는 (3,3) 또는 (5,5)입니다. 커널의 깊이는 입력의 깊이와 같습니다.
- **특성 맵** : 합성곱 층이나 풀링 층의 출력 배열을 의미합니다. 필터 하나가 하나의 특성 맵을 만듭니다. 합성곱 층에서 5개의 필터를 적용하면 5개의 특성 맵이 만들어집니다.
- **패딩** : 합성곱 층의 입력 주위에 추가한 0으로 채워진 픽셀입니다. 패딩을 사용하지 않는 것을 밸리드 패딩이라고 합니다. 합성곱 층의 출력 크기를 입력과 동일하게 만들기 위해 입력에 패딩을 추가하는 것을 세임 패딩이라고 합니다.
- **스트라이드** : 합성곱 층에서 필터가 입력 위를 이동하는 크기입니다. 일반적으로 스트라이드는 1픽셀을 사용합니다.
- **풀링** : 풀링은 가중치가 없고 특성 맵의 가로세로 크기를 줄이는 역할을 수행합니다. 대표적으로 최대 풀링과 평균 풀링이 있으며 (2, 2) 풀링으로 입력을 절반으로 줄입니다.

## 합성곱

- **합성곱**(convolution)은 마치 입력 데이터에 마법의 도장을 찍어서 유용한 특성만 드러나게 하는 것으로 비유할 수 있습니다. 그럼 여기에서 합성곱의 동작 원리를 자세하게 알아보겠습니다.
- 7장에서 사용한 밀집층에서는 뉴런마다 입력 개수만큼 가중치가 있습니다. 즉, 모든 입력에 가중치를 곱하죠. 이 과정을 그림으로 표현하면 다음과 같습니다.

![스크린샷 2025-03-17 오후 9 23 10](https://github.com/user-attachments/assets/dde0af36-9742-4a06-ae6f-2da241611209)

- 인공 신경망은 처음에 가중치 W<sub>1</sub>\~W<sub>10</sub>와 절편 b를 랜덤하게 초기화한 다음 에포크를 반복하면서 경사 하강법 알고리즘을 사용하여 손실이 낮아지도록 최적의 가중치와 절편을 찾아갑니다. 이것이 바로 모델 훈련이죠.
- 예를 들어 밀집층에 뉴런이 3개 있다면 출력은 3개가 됩니다. 입력 개수에 상관없이 동일합니다. 7장의 예를 다시 떠올려보면 패선 MNIST 이미지에 있는 784개의 픽셀을 입력받는 은닉층의 뉴런 개수가 100개면 뉴런마다 하나씩 출력도 100개가 됩니다.
- 합성곱은 밀집층의 계산과 조금 다릅니다. 입력 데이터 전체에 가중치를 적용하는 것이 아니라 일부에 가중치를 곱하죠. 다음 그림과 이전의 밀집층 그림을 비교해 보세요. 여기에서는 이 뉴런이 3개의 가중치를 가진다고 가정했습니다.

![스크린샷 2025-03-17 오후 9 28 00](https://github.com/user-attachments/assets/a19ea1d7-13c3-41ad-b029-038c31c03e8c)

- 가중치 W<sub>1</sub>\~W<sub>3</sub>이 입력의 처음 3개 특성과 곱해져 1개의 출력을 만듭니다. 그 다음이 중요합니다. 이 뉴런이 한 칸 아래로 이동해 두 번째부터 네 번쨰 특성과 곱해져 새로운 출력을 만듭니다. 다음 그림을 참고하세요.

![스크린샷 2025-03-17 오후 9 28 09](https://github.com/user-attachments/assets/eb7e010e-8107-42f5-bf62-279e92d3665a)

- 여기에서 중요한 것은 첫 번째 합성곱에 사용된 가중치 W<sub>1</sub>\~W<sub>3</sub>과 절편 b가 두 번쨰 합성곱에도 동일하게 사용됩니다. 이렇게 한 칸씩 아래로 이동하면서 출력을 만드는 것이 합성곱입니다. 여기에서는 이 뉴런의 가중치가 3개이기 떄문에 모두 8개의 출력이 만들어집니다.

![스크린샷 2025-03-17 오후 9 33 45](https://github.com/user-attachments/assets/8ad8a37f-f4e2-4b41-919c-f08c868124ce)

- 쉽게 구분할 수 있도록 8번의 계산을 다른 색으로 나타냈지만 모두 같은 뉴런입니다. 즉 모두 같은 뉴런입니다. 즉 모두 같은 가중치 W<sub>1</sub>\~W<sub>3</sub>과 절편 b를 사용합니다.
- 밀집층의 뉴런은 입력 개수만큼 10개의 가중치를 가지고 1개의 출력을 만듭니다. 합성곱 층의 뉴런은 3개의 가중치를 가지고 8개의 출력을 만듭니다. 혹시 눈치챘을지 모르지만 합성곱 층의 뉴런에 있는 가중치 개수는 정하기 나름입니다. 즉 또 다른 하이퍼파라미터죠. 이는 마치 입력 데이터 위를 이동하면서 같은 도장(!)으로 하나씩 찍는 것처럼 생각할 수 있습니다. 도장을 찍을 때마다 출력이 하나씩 만들어지는 거죠.

![스크린샷 2025-03-17 오후 9 33 54](https://github.com/user-attachments/assets/72605437-87ff-422a-a39b-eb64e6ba5831)

- 이전에 그렸던 신경망 층의 그림은 뉴런이 길게 늘어서 있고 서로 조밀하게 연결되어 있습니다. 그런데 합성곱에서는 뉴런이 입력 위를 이동하면서 출력을 만들기 때문에 이런 식으로 표현하기가 어렵습니다. 또 뉴런이라고 부르기도 어색합니다. **합성곱 신경망**<sup>convolutional neural network - CNN</sup>에서는 완전 연결 신경망과 달리 뉴런을 **필터**<sup>filter</sup>라고 부릅니다. 혹은 **커널**<sup>kernel</sup>이라고도 부릅니다.

> 뉴런 = 필터 = 커널 모두 같은 말이라고 생각해도 됩니다.

> 완전 연결 신경망 : 7장에서 만들었던 신경망입니다. 완전 연결 층(밀집층)만 사용하여 만든 신경망을 완전 연결 신경망(밀집 신경망)이라고 부릅니다.

- 이 책에서는 케라스 API와 이름을 맞추어 뉴런 개수를 이야기할 때는 필터라 부르고, 입력에 곱해지는 가중치를 의미할 때는 커널이라고 부르겠습니다. 
- 커널은 입력에 곱해지는 가중치, 필터는 뉴런 개수를 표현할 때 사용
- 합성곱의 장점은 1차원이 아니라 2차원 입력에서도 적용할 수 있다는 것입니다. 다음 그림을 보죠.

