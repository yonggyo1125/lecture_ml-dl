# 합성곱 신경망의 구성 요소

## 키워드 정리

- **합성곱** : 밀집층과 비슷하게 입력과 가중치를 곱하고 절편을 더하는 선형 계산입니다. 하지만 밀집층과 달리 각 합성곱은 입력 전체가 아니라 일부만 사용하여 선형 계산을 수행합니다.
- **필터** : 합성곱 층의 **필터**는 밀집층의 뉴런에 해당합니다. 필터의 가중치와 절편을 종종 커널이라고 부릅니다. 자주 사용되는 커널의 크기는 (3,3) 또는 (5,5)입니다. 커널의 깊이는 입력의 깊이와 같습니다.
- **특성 맵** : 합성곱 층이나 풀링 층의 출력 배열을 의미합니다. 필터 하나가 하나의 특성 맵을 만듭니다. 합성곱 층에서 5개의 필터를 적용하면 5개의 특성 맵이 만들어집니다.
- **패딩** : 합성곱 층의 입력 주위에 추가한 0으로 채워진 픽셀입니다. 패딩을 사용하지 않는 것을 밸리드 패딩이라고 합니다. 합성곱 층의 출력 크기를 입력과 동일하게 만들기 위해 입력에 패딩을 추가하는 것을 세임 패딩이라고 합니다.
- **스트라이드** : 합성곱 층에서 필터가 입력 위를 이동하는 크기입니다. 일반적으로 스트라이드는 1픽셀을 사용합니다.
- **풀링** : 풀링은 가중치가 없고 특성 맵의 가로세로 크기를 줄이는 역할을 수행합니다. 대표적으로 최대 풀링과 평균 풀링이 있으며 (2, 2) 풀링으로 입력을 절반으로 줄입니다.

## 합성곱

- **합성곱**(convolution)은 마치 입력 데이터에 마법의 도장을 찍어서 유용한 특성만 드러나게 하는 것으로 비유할 수 있습니다. 그럼 여기에서 합성곱의 동작 원리를 자세하게 알아보겠습니다.
- 7장에서 사용한 밀집층에서는 뉴런마다 입력 개수만큼 가중치가 있습니다. 즉, 모든 입력에 가중치를 곱하죠. 이 과정을 그림으로 표현하면 다음과 같습니다.

![스크린샷 2025-03-17 오후 9 23 10](https://github.com/user-attachments/assets/dde0af36-9742-4a06-ae6f-2da241611209)

- 인공 신경망은 처음에 가중치 W<sub>1</sub>\~W<sub>10</sub>와 절편 b를 랜덤하게 초기화한 다음 에포크를 반복하면서 경사 하강법 알고리즘을 사용하여 손실이 낮아지도록 최적의 가중치와 절편을 찾아갑니다. 이것이 바로 모델 훈련이죠.
- 예를 들어 밀집층에 뉴런이 3개 있다면 출력은 3개가 됩니다. 입력 개수에 상관없이 동일합니다. 7장의 예를 다시 떠올려보면 패선 MNIST 이미지에 있는 784개의 픽셀을 입력받는 은닉층의 뉴런 개수가 100개면 뉴런마다 하나씩 출력도 100개가 됩니다.
- 합성곱은 밀집층의 계산과 조금 다릅니다. 입력 데이터 전체에 가중치를 적용하는 것이 아니라 일부에 가중치를 곱하죠. 다음 그림과 이전의 밀집층 그림을 비교해 보세요. 여기에서는 이 뉴런이 3개의 가중치를 가진다고 가정했습니다.

![스크린샷 2025-03-17 오후 9 28 00](https://github.com/user-attachments/assets/a19ea1d7-13c3-41ad-b029-038c31c03e8c)

- 가중치 W<sub>1</sub>\~W<sub>3</sub>이 입력의 처음 3개 특성과 곱해져 1개의 출력을 만듭니다. 그 다음이 중요합니다. 이 뉴런이 한 칸 아래로 이동해 두 번째부터 네 번쨰 특성과 곱해져 새로운 출력을 만듭니다. 다음 그림을 참고하세요.

![스크린샷 2025-03-17 오후 9 28 09](https://github.com/user-attachments/assets/eb7e010e-8107-42f5-bf62-279e92d3665a)

- 여기에서 중요한 것은 첫 번째 합성곱에 사용된 가중치 W<sub>1</sub>\~W<sub>3</sub>과 절편 b가 두 번쨰 합성곱에도 동일하게 사용됩니다. 이렇게 한 칸씩 아래로 이동하면서 출력을 만드는 것이 합성곱입니다. 여기에서는 이 뉴런의 가중치가 3개이기 떄문에 모두 8개의 출력이 만들어집니다.

![스크린샷 2025-03-17 오후 9 33 45](https://github.com/user-attachments/assets/8ad8a37f-f4e2-4b41-919c-f08c868124ce)

- 쉽게 구분할 수 있도록 8번의 계산을 다른 색으로 나타냈지만 모두 같은 뉴런입니다. 즉 모두 같은 뉴런입니다. 즉 모두 같은 가중치 W<sub>1</sub>\~W<sub>3</sub>과 절편 b를 사용합니다.
- 밀집층의 뉴런은 입력 개수만큼 10개의 가중치를 가지고 1개의 출력을 만듭니다. 합성곱 층의 뉴런은 3개의 가중치를 가지고 8개의 출력을 만듭니다. 혹시 눈치챘을지 모르지만 합성곱 층의 뉴런에 있는 가중치 개수는 정하기 나름입니다. 즉 또 다른 하이퍼파라미터죠. 이는 마치 입력 데이터 위를 이동하면서 같은 도장(!)으로 하나씩 찍는 것처럼 생각할 수 있습니다. 도장을 찍을 때마다 출력이 하나씩 만들어지는 거죠.

![스크린샷 2025-03-17 오후 9 33 54](https://github.com/user-attachments/assets/72605437-87ff-422a-a39b-eb64e6ba5831)

- 이전에 그렸던 신경망 층의 그림은 뉴런이 길게 늘어서 있고 서로 조밀하게 연결되어 있습니다. 그런데 합성곱에서는 뉴런이 입력 위를 이동하면서 출력을 만들기 때문에 이런 식으로 표현하기가 어렵습니다. 또 뉴런이라고 부르기도 어색합니다. **합성곱 신경망**<sup>convolutional neural network - CNN</sup>에서는 완전 연결 신경망과 달리 뉴런을 **필터**<sup>filter</sup>라고 부릅니다. 혹은 **커널**<sup>kernel</sup>이라고도 부릅니다.

> 뉴런 = 필터 = 커널 모두 같은 말이라고 생각해도 됩니다.

> 완전 연결 신경망 : 7장에서 만들었던 신경망입니다. 완전 연결 층(밀집층)만 사용하여 만든 신경망을 완전 연결 신경망(밀집 신경망)이라고 부릅니다.

- 이 책에서는 케라스 API와 이름을 맞추어 뉴런 개수를 이야기할 때는 필터라 부르고, 입력에 곱해지는 가중치를 의미할 때는 커널이라고 부르겠습니다.
- 커널은 입력에 곱해지는 가중치, 필터는 뉴런 개수를 표현할 때 사용
- 합성곱의 장점은 1차원이 아니라 2차원 입력에서도 적용할 수 있다는 것입니다. 다음 그림을 보죠.

![스크린샷 2025-03-17 오후 9 44 00](https://github.com/user-attachments/assets/4ebe7c8b-2a5c-4df3-ab6a-00cb558916be)

- 입력이 2차원 배열이면 필터(도장!)도 2차원이어야 합니다. 이 그림에서 이 필터의 커널 크기는 (3,3)으로 가정합니다(앞에서 언급했지만 커널의 크기는 우리가 지정해야 할 하이퍼파라미터입니다). 그다음 왼쪽 위 모서리에서부터 합성곱을 시작합니다. 입력의 9개 원소와 커널의 9개 가중치를 곱한 후 (물론 여기에서도 절편을 더합니다) 1개의 출력을 만듭니다.
- 그다음에는 필터가 오른쪽으로 한 칸 이동하여 합성곱을 또 수행합니다. 입력의 너비가 4이므로 더이상 오른쪽으로 한 칸 이동할 수 없습니다. 이럴 때는 아래로 한 칸 이동한 다음 다시 왼쪽에서부터 합성곱을 수행합니다. 그리고 다시 오른쪽으로 한 칸 이동하죠. 다음 그림을 참고하세요.

![스크린샷 2025-03-17 오후 9 44 23](https://github.com/user-attachments/assets/fb98b72b-f9db-44f4-8888-708cea866287)

- 합성곱은 마치 도장을 찍듯이 왼쪽 위에서 오른쪽 맨 아래까지 이동하면서 출력을 만듭니다. 계산식은 밀집층과 크게 다르지 않습니다. 입력과 가중치의 행과 열을 맞추어 곱셈하고 모두 더하는 게 전부입니다. 그림에서 필터는 모두 4번 이동할 수 있기 떄문에 4개의 출력을 만듭니다.

![스크린샷 2025-03-17 오후 9 49 50](https://github.com/user-attachments/assets/45bbf7e6-b0ea-4392-910d-33ea82029344)

- 이때 4개의 출력을 필터가 입력에 놓인 위치에 맞게 2차원으로 배치합니다. 즉 왼쪽 위, 오른쪽 위, 왼쪽 아래, 오른쪽 아래 모두 4개의 위치에 해당 값을 놓습니다. 이렇게 출력을 2차원으로 표현하면 (4,4) 크기와 입력을 (2,2) 크기로 압축한 느낌이 나죠? 합성곱 계산을 통해 얻은 출력을 특별히 **특성맵**<sup>feature map</sup>이라고 부릅니다.

- 밀집층에서 여러 개의 뉴런을 사용하듯이 합성곱 층에서도 여러 개의 필터를 사용합니다. 하나만 사용할 이유는 없죠. 다음 그림에서처럼 여러 개의 필터를 사용하면 만들어진 특성 맵은 순서대로 차곡 차곡 쌓입니다. (2,2) 크기의 특성 맵을 쌓으면 3차원 배열이 되죠. 다음 그림에서는 3개의 필터를 사용했기 때문에 (2,2,3) 크기의 3차원 배열이 됩니다.

![스크린샷 2025-03-17 오후 9 54 28](https://github.com/user-attachments/assets/daea326e-8b91-49de-b502-d515eeb14456)

- 밀집층에 있는 뉴런의 가중치가 모두 다르듯이 합성곱 층에 있는 필터의 가중치(커널)도 모두 다릅니다. 너무 당연하지만 같은 가중치를 가진 필터를 여러 개 사용할 이유가 없겠죠?
- 실제 계산은 밀집층과 동일하게 단순히 입력과 가중치를 곱하는 것이지만 2차원 형태를 유지하는 점이 다릅니다. 또 입력보다 훨씬 작은 크기의 커널을 사용하고 입력 위를 (왼쪽에서 오른쪽으로, 위에서 아래로) 이동하면서 2차원 특성 맵을 만듭니다. 이렇게 2차원 구조를 그대로 사용하기 떄문에 합성곱 신경망이 이미지 처리 분야에서 뛰어난 성능을 발휘합니다.
- 그럼 케라스에서 합성곱 층을 어떻게 만드는지 알아보겠습니다.

## 케라스 합성곱 층

- 케라스의 층은 모두 `keras.layers` 패키지 아래 클래스로 구현되어 있습니다. 합성곱 층도 마찬가지입니다. 특별히 입력 위를 (왼쪽에서 오른쪽으로, 위에서 아래로) 이동하는 합성곱은 `Conv2D` 클래스로 제공합니다.

```python
from tensorflow import keras
keras.layers.Conv2D(10, kernel_size=(3, 3), activation='relu')
```

- `Conv2D` 클래스의 첫 번쨰 매개변수는 필터(즉 도장!)의 개수입니다. `kernel_size` 매개변수는 필터에 사용할 커널의 크기를 지정합니다. 필터의 개수와 커널의 크기는 반드시 지정해야 하는 매개변수입니다.
- 마지막으로 밀집층에서처럼 활성화 함수를 지정합니다. 여거에서는 렐루 함수를 선택했습니다.

> **특성 맵은 활성화 함수를 적용하기 전인가요? 후인가요?**<br>결론부터 이야기하자면 후입니다. 완전 연결 신경망에서처럼 합성곱 신경망에서도 종종 활성화 함수를 언급하지 않습니다. 일반적으로 특성 맵은 활성화 함수를 통과한 값을 나타냅니다. 합성곱에서는 활성화 출력이란 표현을 잘 쓰지 않습니다.

> **커널의 크기는 어떻게 정하나요?**<br>앞에서 잠깐 언급했지만 커널의 크기는 하이퍼파라미터입니다. 따라서 여러 가지 값을 시도해 봐야 합니다. 하지만 보통 (3,3)이나 (5,5) 크기가 권장됩니다.

- 케라스 API를 사용하면 합성곱 층을 사용하는 것이 어렵지 않습니다. 이전에 `Dense` 층을 사용했던 자리에 대신 `Conv2D` 층을 넣으면 됩니다. 다만 `kernel_size`와 같이 추가적인 매개변수들을 고려해야 합니다.
- 그렇다면 합성곱 신경망의 정의는 무엇일까요? 일반적으로 1개 이상의 합성곱 층을 쓴 인공 신경망을 합성곱 신경망이라고 부릅니다. 즉 꼭 합성곱 층만 사용한 신경망을 합성곱 신경망이라고 부르는 것은 아닙니다. 이전 장에서 보았듯이 클래스에 대한 확률을 계산하려면 마지막 층에 클래스 개수만큼의 뉴런을 가진 밀집층을 두는 것이 일반적이니까요.
- 합성곱 층이 구현된 케라스 API를 잠시 살펴보았습니다. 그런데 합성곱 신경망을 실제 만들려면 조금 더 알아야 할 것이 있습니다. 마음이 급하겠지만 호흡을 가다듬고 패딩과 스트라이드를 알아보겠습니다.

### 패딩과 스트라이드

- 앞에서 예로 들었던 합성곱 계산은 (4,4) 크기의 입력에 (3,3) 크기의 커널을 적용하여 (2,2) 크기의 특성 맵을 만들었습니다. 그런데 만약 커널 크기는 (3,3)으로 그대로 두고 출력의 크기를 입력과 동일하게 (4,4)로 만들려면 어떻게 해야 할까요?
- (4,4) 입력과 동일한 크기의 출력을 만들려면 마치 더 큰 입력에 합성곱하는 척해야 합니다. 예를 들어 실제 입력 크기는 (4,4)이지만 (6,6)처럼 다룬다고 가정해 보겠습니다. 다음 그림과 같이 (6,6) 크기이면 (3,3) 크기의 커널로 합성곱을 했을 때 출력의 크기가 얼마나 될까요?

![스크린샷 2025-03-17 오후 10 10 26](https://github.com/user-attachments/assets/d0fdb94c-cbc4-4056-862a-37619172edda)

- (3,3) 커널로 도장을 찍어 보면 출력의 크기가 (4,4)가 되는 것을 알 수 있습니다. 다음 그림의 빨강 색 상자가 커널을 나타냅니다. 다음 그림의 빨강 색 상자가 커널을 나타냅니다. 왼쪽 위에서 오른쪽 아래까지 한 칸씩 이동하면서 합성곱을 수행하면 입력과 같은 (4,4) 크기의 출력을 만들 수 있습니다.

![스크린샷 2025-03-17 오후 10 10 37](https://github.com/user-attachments/assets/a2701310-1833-46ac-8006-ed807adb3da8)

- 이렇게 입력 배열 주위를 가상의 원소로 채우는 것을 **패딩**<sup>padding</sup>이라고 합니다. 실제 입력값이 아니기 때문에 패딩은 0으로 채웁니다. 즉 (4,4) 크기의 입력에 0을 1개 패딩 하면 다음과 같은 (6,6) 크기의 입력이 됩니다. 패딩의 역할은 순전히 커널이 도장을 찍을 횟수를 늘려주는 것밖에는 없습니다. 실제 값은 0으로 채워져 있기 때문에 계산에 영향을 미치지는 않습니다.

> 커널이 도장(필터)를 찍을 횟수를 늘려주기 위해서 입력 배열 주변을 가상의 원소로 채우는 것을 패딩이라고 합니다. 보통 패딩은 0으로 채웁니다.

![스크린샷 2025-03-17 오후 10 15 01](https://github.com/user-attachments/assets/d1539ae7-3e0a-4a48-96c9-4c12f95a2fcd)

- 이렇게 입력과 특성 맵의 크기를 동일하게 만들기 위해 입력 주위에 0으로 패딩 하는 것을 **세임 패딩**<sup>same padding</sup>이라고 부릅니다. 합성곱 신경망에서는 세임 패딩이 많이 사용됩니다. 바꿔 말하면 입력과 특성 맵의 크기를 동일하게 만드는 경우가 아주 많죠
- 패딩 없이 순수한 입력 배열에서만 합성곱을 하여 특성 맵을 만드는 경우를 **밸리드 패딩**<sup>valid padding</sup>이라고 합니다. 밸리드 패딩은 특성 맵의 크기가 줄어들 수밖에 없습니다.
- 그럼 왜 합성곱에서는 패딩을 즐겨 사용할까요? 만약 패딩이 없다면 위의 예에서 (4,4) 크기의 입력에 패딩 없이 합성곱을 한다면 왼쪽 위 모서리의 3은 커널 도장에 딱 한 번만 찍힙니다. 사실 네 모서리에 있는 다른 3개의 값도 마찬가지입니다.

![스크린샷 2025-03-17 오후 10 15 08](https://github.com/user-attachments/assets/b67af9c9-8d27-4a8c-bbd9-67a737e53c62)

- 반면 다른 원소들은 2번 이상 커널과 계산됩니다. 가운데 있는 4개 원소 4, 8, 5, 1은 4번의 합성곱 계산에 모두 포함되네요. 만약 이 입력을 이미지라고 생각하면 모서리에 있는 중요한 정보가 특성 맵으로 잘 전달되지 않을 가능성이 높습니다. 반면 가운데 있는 정보는 두드러지게 표현됩니다.
- 다음 그림을 보면 패딩을 하지 않을 경우 중앙부와 모서리 픽셀이 합성곱에 참여하는 비율은 크게 차이 납니다(4:1). 1픽셀을 패딩 하면 이 차이는 크게 줄어듭니다(9:4). 만약 2픽셀을 패딩 하면 중앙부와 모서리 픽셀이 합성곱에 참여하는 비율이 동일해집니다(1:1).

![스크린샷 2025-03-17 오후 10 28 31](https://github.com/user-attachments/assets/315d7b82-cf15-42d7-a4b4-ac4106409d77)

- 직접 손으로 그림을 그려서 계산해 보세요. 합성곱을 이해하는 데 큰 도움이 됩니다. 먼저 (4,4) 그림을 칠해 봅시다. 커널 크기가 (3,3)일 때 a, b, c가 각각 몇 번의 합성곱에 참여하나요?

![스크린샷 2025-03-17 오후 10 28 37](https://github.com/user-attachments/assets/5b60f015-c523-4371-9cb4-084abcd11ec2)

- 이번에는 패딩을 준 (6,6)을 확인해볼까요? 커널 크기가 (3,3)일 때 a, b, c는 합성곱에 몇 번 참여하나요?

![스크린샷 2025-03-17 오후 10 28 48](https://github.com/user-attachments/assets/ff83070a-9457-4467-b987-60878589c9f4)

- 정확하게 그렸다면, 첫 번째는 4, 2, 1번씩, 두 번째는 9, 6, 4번씩 참여한다고 확인할 수 있었을 겁니다.
- 적절한 패딩은 이처럼 이미지 주변에 있는 정보를 잃어버리지 않도록 도와줍니다. 앞에서도 언급했지만 일반적인 합성곱 신경망에서는 세임 패딩이 많이 사용됩니다. 케라스 `Conv2D` 클래스에서는 `padding` 매개변수로 패딩을 지정할 수 있습니다. 기본값은 `valid`로 밸리드 패딩을 나타냅니다. 세임 패딩을 사용하려면 `same`으로 지정합니다.

```python
keras.layers.Conv2D(10, kernel_size=(3,3), activation='relu', padding='same')
```

- 지금까지 본 합성곱 연산은 좌우, 위아래로 한 칸씩 이동했습니다. 하지만 두 칸씩 건너뛸 수도 있습니다. 이렇게 두 칸씩 이동하면 만들어지는 특성 맵의 크기는 더 작아지겠죠? 커널 도장을 찍는 횟수가 줄어드니까요!
- 이런 이동의 크기를 **스트라이드**<sup>stride</sup>라고 합니다. 기본적으로 스트라이드는 1입니다. 즉 한 칸씩 이동합니다. 이 값이 케라스 `Conv2D`의 `strides` 매개변수의 기본값입니다.

```python
keras.layers.Conv2D(10, kernel_size=(3,3), activation='relu', padding='same', strides=1)
```

- `strides` 매개변수는 오른쪽으로 이동하는 크기와 아래쪽으로 이동하는 크기를 (1,1)과 같이 튜플을 사용해 각각 지정할 수 있습니다. 하지만 커널의 이동 크기를 가로세로 방향으로 다르게 지정하는 경우는 거의 없습니다. 또 1보다 큰 스트라이드를 사용하는 경우도 드룹니다. 대부분 기본값을 그대로 사용하기 때문에 `stides` 매개변수는 잘 사용하지 않습니다.
- 조금 복잡해 보이지만 케라스 API를 사용하면 `Conv2D` 클래스의 옵션으로 간단히 처리할 수 있습니다. 꼭 기얻해야 할 것은 세임 패딩의 경우 입력과 만들어진 특성 맵의 가로세로 크기가 같다는 점입니다. 그럼 합성곱 신경망의 마지막 구성 요소인 풀링으로 넘어갑니다.

### 풀링

- **풀링**<sup>pooling</sup>은 합성곱 층에서 만든 특성 맵의 가로세로 크기를 줄이는 역할을 수행합니다. 하지만 특성맵의 개수는 줄이지 않습니다. 예를 들면 다음 그림처럼 (2,2,3) 크기의 특성 맵에 풀링을 적용하면 마지막 차원인 개수는 그대로 유지하고 너비와 높이만 줄어들어 (1,1,3) 크기의 특성 맵이 됩니다.

![스크린샷 2025-03-17 오후 10 42 31](https://github.com/user-attachments/assets/18f0a81c-b526-4245-95ca-0426d3be60b1)

![스크린샷 2025-03-17 오후 10 42 38](https://github.com/user-attachments/assets/0c85164f-7717-4d84-9fb1-7d30987bcd74)


