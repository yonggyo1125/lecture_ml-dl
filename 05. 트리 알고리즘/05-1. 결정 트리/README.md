# 결정 트리

## 키워드 정리 
- **결정 트리**
    - 예 / 아니오에 대한 질문을 이어나가면서 정답을 찾아 학습하는 알고리즘입니다.
    - 비교적 예측 과정을 이해하기 쉽고 성능도 뛰어납니다. 
- **불순도**
    - 결정 트리가 최적의 질문을 찾기 위한 기준입니다. 
    - 사이킷런은 지니 불순도와 엔트로피 불순도를 제공합니다.
- **정보 이득**
    - 부모 노드와 자식 노드의 불순도 차이입니다.
    - 결정 트리 알고리즘은 정보 이득이 최대화되도록 학습합니다.
- **가지치기**
    - 결정 트는 제한 없이 성장하면 훈련 세트에 과대적합되기 쉽습니다. 
    - **가지치기**는 결정트리의 성장을 제한하는 방법입니다.
    - 사이킷런의 결정 트리 알고리즘은 여러가지 가지치기 매개변수를 제공합니다.
- **특성 중요도**
    - 결정 트리에 사용된 특성이 불순도를 감소하는데 기여한 정도를 나타내는 값입니다. 
    - 특성 중요도를 계산할 수 있는 것이 결정 트리의 또다른 큰 장점입니다.

## 핵심 패키지와 함수

### pandas
- **info()**
    - 데이터프레임의 요약된 정보를 출력합니다. 
    - 인덱스와 컬럼 타입을 출력하고 널(null)이 아닌 값의 개수, 메모리 사용량을 제공합니다.
    - verbose 매개변수의 기본값 True를 False로 바꾸면 각 열에 대한 정보를 출력하지 않습니다.
- **describe()**
    - 데이터프레임 열의 통계 값을 제공합니다. 
    - 수치형일 경우 최소, 최대, 평균, 표준편차와 사분위값 등이 출력됩니다.
    - 문자열 같은 객체 타입의 열은 가장 자주 등장하는 값과 횟수 등이 출력됩니다.
    - `percentiles` 매개변수에 백분위수를 지정합니다. 기본값은 \[0.25, 0.5, 0.75\]입니다.


### scikit-learn
- **DecisionTreeClassifier** : 결정 트리 분류 클래스
    - `criterion` 매개변수 : 불순도를 지정하며 기본값은 지니 불순도를 의미하는 `gini` 이고 `entropy` 를 선택하여 엔트로피 불순도를 사용할 수 있습니다.
    - `splitter` 매개변수 : 노드를 분할하는 전략을 선택합니다. 기본값은 `best`로 정보 이득이 최대가 되도록 분할합니다. `random`이면 임의로 노드를 분할합니다. 
    - `max_depth` : 트리가 성장할 최대 깊이를 지정합니다. 기본값은 `None`으로 리프 노드가 순수하거나 `min_samples_split`보다 샘플 개수가 적을 때까지 성장합니다.
    - `min_samples_split` : 노드를 나누기 위한 최소 샘플 개수입니다. 기본값은 2입니다.
    - `max_features` 매개변수 : 최적의 분할을 위해 탐색할 특성의 개수를 지정합니다. 기본값은 `None`으로 모든 특성을 사용합니다.
- **plot_tree()** 
    - 결정 트리 모델을 시각화합니다. 첫 번째 매개변수로 결정 트리 모델 객체를 전달합니다.
    - `max_depth` 매개변수: 나타낼 트리의 깊이를 지정합니다. 기본값은 `None`으로 모든 노드를 출력합니다.
    - `feature_names` 매개변수: 특성의 이름을 지정할 수 있습니다.
    - `filled` 매개변수: True로 지정하면 타깃값에 따라 노드 안에 색을 채웁니다.

## 로지스틱 회귀로 와인 분류하기

```python
import pandas as pd

wine = pd.read_csv('https://bit.ly/wine_csv_data')
```

- 판다스를 사용해 인터넷에서 직접 불러오겠습니다. 
- 다운로드할 주소는 https://bit.ly/wine_csv_data 입니다.
 
```python
wine.head()
```

- 와인 데이터셋을 판다스 데이터프레임으로 제대로 읽어 들였는지 `head()` 메서드로 처음 5개의 샘플을 확인해 보겠습니다.

![스크린샷 2024-11-05 오전 6 33 45](https://github.com/user-attachments/assets/46e8cf34-a196-4aec-b69c-3740c468f471)

- 처음 3개의 열(alcohol, sugar, pH)은 각각 알코올 도수, 당도, pH 값을 나타냅니다.
- 네 번째의 열(class)은 타깃값으로 0이면 레드 와인, 1이면 화이트 와인이라고 합니다. 레드 와인과 화이트 와인을 구분하는 이진 분류 문제이고, 화이트 와인이 양성 클래스입니다. 
- 즉 전체 와인 데이터에서 화이트 와인을 골라내는 문제

```python
wine.info()
```

- 이 메서드는 데이터프레임의 각 열의 데이터 타입과 누락된 데이터가 있는지 확인하는 데 유용합니다. 


![스크린샷 2024-11-05 오전 6 37 50](https://github.com/user-attachments/assets/5c15ae9c-919f-4924-8249-86f9585bf574)

- 출력 결과를 보면 총 6,497개의 샘플이 있고 4개의 열은 모두 실숫값입니다.
- Non-Null Count가 모두 6497이므로 누락된 값은 없습니다.


> 누락된 값이 있다면 그 데이터를 버리거나 평균값으로 채운 후 사용할 수 있습니다. 어떤 방식이 최선인지는 미리 알기 어렵습니다. 두 가지 모두 시도해 보세요. 여기에서도 항상 훈련 세트의 통계 값으로 테스트 세트를 변환해야 합니다. 즉, 훈련 세트의 평균값으로 테스트 세트의 누락된 값을 채워야 합니다.

```python
wine.describe()
```

- 이 메서드는 열에 대한 간략한 통계를 출력해 줍니다. 
- 최소, 최대, 평균값 등을 볼 수 있습니다.

![스크린샷 2024-11-05 오전 6 45 42](https://github.com/user-attachments/assets/1832b862-5d5c-4f07-91b7-f4554910f38f)

- 평균(mean), 표준편차(std), 최소(min), 최대(max)값, 중간값(50%), 1사분위수(25%), 3사분위수(75%)를 볼 수 있습니다. 
- 여기에서 알수 있는 것은 알코올 도수와 당도, pH 값의 스케일이 다르다는 것
- 사이킷런의 `StandardScaler` 클래스를 사용해 특성을 표준화합니다.

> 사분위수는 데이터를 순서대로 4등분 한 값입니다. 예를 들어 2사분위수(중간값)는 데이터를 일렬로 늘어놓았을 때 정중앙의 값입니다. 만약 데이터 개수가 짝수개라 중앙값을 선택할 수 없다면 가운데 2개 값의 평균을 사용합니다.

```python
data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
target = wine['class'].to_numpy()
```

- 판다스 데이터프레임을 넘파이 배열로 바꾸고 훈련 세트와 테스트 세트로 나누겠습니다. 
- wine 데이터프레임에서 처음 3개의 열을 넘파이 배열로 바꿔서 data 배열에 저장하고 마지막 class 열을 넘파이 배열로 바꿔서 target 배열에 저장합니다.

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)
```

- 훈련 세트와 테스트 세트로 나눕니다.
- `train_test_split()` 함수는 설정값을 지정하지 않으면 25%를 테스트 세트로 지정합니다. 
- 샘플 개수가 충분히 많으므로 20% 정도만 테스트 세트로 나눴습니다. 코드의 `test_size=0.2`가 이런 의미 입니다. 


```python
print(train_input.shape, test_input.shape)
```

- 만들어진 훈련 세트와 테스트 세트의 크기를 확인합니다.

```
(5197, 3) (1300, 3)
```

- 훈련 세트는 5.197개이고 테스트 세트는 1,300개 입니다.

```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)

train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

- `StandardScaler` 클래스를 사용해 훈련 세트를 전처리합니다. 
- 그 다음 같은 객체를 그대로 사용해 테스트 세트를 변환하겠습니다.


```python
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()
lr.fit(train_scaled, train_target)

print(lr.score(train_scaled, train_target))
print(lr.score(test_scaled, test_target))
```

- 표준점수로 변환된 `train_scaled` 와 `test_scaled` 를 사용해 로지스틱 회귀 모델을 훈련합니다.

```
0.7808350971714451
0.7776923076923077
```

- 훈련 세트와 테스트 세트의 점수가 모두 낮으니 모델이 다소 과소적합된 것 같습니다. 

### 설명하기 쉬운 모델과 어려운 모델

```python
print(lr.coef_, lr.intercept_)
```

```
[[ 0.51270274,  1.6733911,   -0.68767781]] [1.81777902]
```

## 결정 트리
- **결정 트리**(Decision Tree)모델은 이유를 설명하기 쉽습니다.
- 결정 트리 모델은 스무고개와 같이 질문을 하나씩 던져서 정답을 맞춰가는 것
- 데이터를 잘 나눌 수 있는 질문을 찾는다면 계속 질문을 추가해서 분류 정확도를 높일 수 있습니다.
- 사이킷런은 결정 트리 알고리즘을 제공합니다. **DecisionTreeClassifier** 클래스
- `fit()` 메서드를 호출해서 모델을 훈련한 다음 `score()` 메서드로 정확도를 평가합니다. 

![스크린샷 2024-11-05 오전 7 04 18](https://github.com/user-attachments/assets/4efae792-be2f-47d9-bb13-b94d1221e9da)

```python
from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier(random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target)) # 훈련 세트
print(dt.score(test_scaled, test_target))   # 테스트 세트
```

```
0.996921300750433
0.8592307692307692
```

- 훈련 세트에 대한 점수가 매우 높으나 테스트 세트의 성능은 그에 비해 조금 낮습니다. 과대적합된 모델이라고 볼 수 있습니다. 
- 이 모델을 그림으로 표현하려면 사이킷런의 `plot_tree()`함수를 사용해 결정 트리를 이해하기 쉬운 트리 그림으로 출력해 줍니다.




```python
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
```

- 위에서 만든 결정 트리 모델 객체를 `plot_tree()` 함수에 전달해서 어떤 트리가 만들어졌는지 그려봅니다.


![스크린샷 2024-11-05 오전 7 09 08](https://github.com/user-attachments/assets/c589c11d-39ef-4e6f-840a-7e4ae1d561a8)


- 엄청난 트리가 만들어졌습니다. 진짜 나무는 밑에서 부터 하늘 위로 자라나지만, 결정 트리는 위에서 부터 아래로 거꾸로 자라납니다. 
- 맨 위의 노드(node)를 루트 노드(root node)라 부르고 맨 아래 끝에 달린 노드를 리프 노드(leaf node)라고 합니다.

> 노드는 결정 트리를 구성하는 핵심 요소입니다. 노드는 훈련 데이터의 특성에 대한 테스트를 표현합니다. 예를 들어 현재 샘플 당도가 -0.239보다 작거나 같은지 테스트 합니다. 가지(branch)는 테스트의 결과(True, False)를 나타내며 일반적으로 하나의 노드는 2개의 가지를 가집니다.

```python
plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

- 너무 복잡하므로 `plot_tree()` 함수에서 트리의 깊이를 제한해서 출력해 봅니다. 
- `max_depth` 매개변수를 1로 주면 루트 노드를 제외하고 하나의 노드를 더 확장하여 그립니다. 
- `filled` 매개변수에 클래스에 맞게 노드의 색을 칠할 수 있습니다. 
- `feature_names` 매개변수에는 특성의 이름을 전달할 수 있습니다. 
- 이렇게 하면 노드가 어떤 특성으로 나뉘는지 좀 더 잘 이해할 수 있습니다.

![스크린샷 2024-11-05 오전 7 21 15](https://github.com/user-attachments/assets/9e2b5072-66eb-4504-8a67-b86be441c6f1)


![스크린샷 2024-11-06 오전 6 05 21](https://github.com/user-attachments/assets/809c50f3-e53a-4bb2-9aa2-6d91b197edca)

![스크린샷 2024-11-06 오전 6 06 19](https://github.com/user-attachments/assets/a656d0a1-4925-493b-936b-9fcc0cf9e7ae)

- 루트 노드는 당도(sugar)가 -0.239 이하인지 질문을 합니다. 만약 어떤 샘플의 당도가 -0.239와 같거나 작으면 왼쪽 가지로 갑니다. 그렇지 않다면 오른쪽 가지로 이동합니다.
- 즉, 왼쪽이 Yes, 오른쪽이 No 입니다.
- 루트 노드의 총 샘플 수(samples)는 5,197개 입니다. 이 중에서 음성 클래스(레드 와인)는 1,258개이고, 양성 클래스(화이트 와인)는 3,939개 입니다. 이 값이 value에 나타나 있습니다.

![스크린샷 2024-11-06 오전 6 06 57](https://github.com/user-attachments/assets/f3e11c5d-68a2-40e9-82b5-e444017d4f6d)

- 이 노드는 당도가 더 낮은지를 물어봅니다. 당도가 -0.802와 같거나 낮다면 다시 왼쪽 가지로, 그렇지 않으면 오른쪽 가지로 이동합니다. 
- 이 노드에서 음성 클래스와 양성 클래스의 샘플 개수는 각각 1,177개와 1,745개 입니다. 루트 노드보다 양성 클래스, 즉 화이트 와인의 비율이 크게 줄었습니다. 

![스크린샷 2024-11-06 오전 6 07 39](https://github.com/user-attachments/assets/bd6208d5-1b94-457c-b7d7-759cff5ad28a)

- 음성 클래스가 91개, 양성 클래스가 2,194개로 대부분의 화이트 와인 샘플이 이 노드로 이동했습니다. 
- 노드의 바탕 색깔을 유심히 보면, 루트 노드보다 이 노드가 더 진하고, 왼쪽 노드는 더 연해졌습니다. 
- `plot_tree()` 함수에서 filled=True로 지정하면 클래스마다 색깔을 부여하고, 어떤 클래스의 비율이 높아지면 점점 진한 색으로 표시합니다. 

> 결정 트리에서 예측하는 방법은 간단합니다. 리프 노드에서 가장 많은 클래스가 예측 클래스가 됩니다. 앞에서 보았던 k-최근접 이웃과 매우 비슷합니다. 만약 이 결정 트리의 성장을 여기서 멈춘다면 왼쪽 노드에 도달한 샘플과 오른쪽 노드에 도달한 샘플은 모두 양성 클래스로 예측됩니다. 두 노드 모두 양성 클래스의 개수가 많기 때문입니다.
>
> 만약 결정 트리를 회귀 문제에 적용하면 리프 노드에 도달한 샘플의 타깃을 평균하여 예측값으로 사용합니다. 사이킷런의 결정 트리 회귀 모델은 **DecisionTreeRegressor** 입니다. 


### 불순도

- gini는 **지니 불순도**(Gini impurity)를 의미합니다. **DecisionTreeClassifier** 클래스의 `criterion` 매개변수의 기본값이 `gini` 입니다. `criterion` 매개변수의 용도는 노드에서 데이터를 분할할 기준을 정하는 것입니다. 
- 앞의 그린 트리에서 루트 노드는 어떻게 당도 -0.239를 기준으로 왼쪽과 오른쪽 노드로 나누었을까요? 바로 `criterion` 매개변수에 지정한 지니 불순도를 사용합니다. 
- 지니 불순도를 계산하는 방법 : 클래스의 비율을 제곱해서 더한 다음 1에서 빼면 됩니다.

![스크린샷 2024-11-06 오전 6 28 13](https://github.com/user-attachments/assets/edc99888-e215-4dde-b59f-c1fe979a7d7f)

- 다중 클래스 문제라면 클래스가 더 많겠지만 계산하는 방법은 동일합니다. 


![스크린샷 2024-11-06 오전 6 28 25](https://github.com/user-attachments/assets/88da19ec-dba9-41d2-9937-de46e5db13f9)

- 루트 노드의 지니 불순도를 계산해 봅시다. 루트 노드는 총 5,197개의 샘플이 있고 그 중에서 1,258개가 음성 클래스, 3,939개가 양성 클래스입니다. 


![스크린샷 2024-11-06 오전 6 28 32](https://github.com/user-attachments/assets/041818a7-a523-4bf5-a9bd-b844cb03e493)

- 왼쪽과 오른쪽 노드의 지니 불순도를 계산해 봅니다.
- 만약 100개의 샘플이 있는 어떤 노드의 두 클래스 비율이 정확히 1/2씩이라면 지니 불순도는 0.5가 되어 최악이 됩니다.

![스크린샷 2024-11-06 오전 6 28 38](https://github.com/user-attachments/assets/e2f3d8f7-1a43-4b38-83d7-9d9f0f1bca38)

- 노드에 하나의 클래스만 있다면 지니 불순도는 0이 되어 가장 작습니다.
- 이런 노드를 순수 노드라고도 부릅니다.

- 결정 트리 모델은 부모 노드(parent node)오 자식 노드(child node)의 불순도 차이가 가능한 크도록 트리를 성장시킵니다.
- 부모 노드와 자식 노드의 불순도 차이를 계산하는 방법을 알아봅시다. 먼저 자식 노드의 불순도를 샘플 개수에 비례하여 모두 더합니다. 그 다음 부모 노드의 불순도에서 빼면 됩니다.
- 예를 들어 앞의 트리 그림에서 루트 노드를 부모 노드라고 하면 왼쪽 노드와 오른쪽 노드가 자식 노드가 됩니다. 왼쪽 노드로는 2,922개의 샘플이 이동했고, 오른쪽 노드로는 2,275개의 샘플이 이동했습니다. 불순도의 차이는 다음과 같이 계산합니다.

![스크린샷 2024-11-06 오전 6 38 47](https://github.com/user-attachments/assets/4aee68e0-ee88-4aca-b5d8-1e0bfa62c27f)

- 이런 부모와 자식 노드 사이의 불순도 차이를 **정보 이득**(information gain)이라고 부릅니다. 
- 이 알고리즘은 정보 이득이 최대가 되도록 데이터를 나눕니다. 이때 지니 불순도를 기준으로 사용합니다. 

![스크린샷 2024-11-06 오전 6 39 03](https://github.com/user-attachments/assets/865b147f-1b44-420a-b43e-850f89e540a1)

- 그런데 사이킷런에는 또 다른 불순도 기준이 있습니다. 
- **DecisionTreeClassifier** 클래스에서 `criterion='entropy'` 를 지정하여 엔트로피 불순도를 사용할 수 있습니다. 
- 엔트로피 불순도도 노드의 클래스 비율을 사용하지만 지니 불순도처럼 제곱이 아니라 밑이 2인 로그를 사용하여 곱합니다. 
- 보통 기본값인 지니 불순도와 엔트로피 불순도가 만든 결과의 차이는 크지 않습니다. 

> 결정 트리 알고리즘은 불순도 기준을 사용해 정보 이득이 최대가 되도록 노드를 분할합니다. 노드를 순수하게 나눌수록 정보 이득이 커집니다. 새로운 샘플에 대해 예측할 때에는 노드의 질문에 따라 트리를 이동합니다. 그리고 마지막에 도달한 노드의 클래스 비율을 보고 예측을 만듭니다.
>
> 앞의 트리는 제한 없이 자라났기 때문에 훈련 세트보다 테스트 세트에서 점수가 크게 낮았았습니다.

## 가지치기

- 가지치기를 하지 않으면 무작정 끝까지 자라나는 트리가 만들어집니다. 
- 훈련 세트에는 아주 잘 맞겠지만 테스트 세트에서 점수는 그에 못 미칠 것입니다. 이를 두고 일반화가 잘 안될 것 같다고 말합니다.
- 결정 트리에서 가지치기를 하는 가장 간단한 방법은 자라날 수 있는 트리의 최대 깊이를 지정하는 것입니다. 
- **DecisionTreeClassifier** 클래스의 `max_depth` 매개변수를 3으로 지정하여 모델을 만들어 보겠습니다. 이렇게 하면 루트 노드 아래로 최대 3개의 노드까지만 성장할 수 있습니다.

```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_scaled, train_target)

print(dt.score(train_scaled, train_target))
print(dt.score(test_scaled, test_target))
```

```
0.8454877814123533
0.8415384615384616
```

- 훈련 세트의 성능은 낮아졌지만 테스트 세트의 성능은 거의 그대로입니다. 

```python
plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

- 이런 모델을 트리 그래프로 그린다면 훨씬 이해하기 쉽습니다. `plot_tree()` 함수로 그려봅니다. 

![스크린샷 2024-11-06 오전 6 52 30](https://github.com/user-attachments/assets/0f851d6e-007f-41bc-9889-d97509a91b1c)

- 그래프를 따라가면서 샘플이 어떻게 나뉘는지 확인할 수 있습니다. 루트 노드 다음에 있는 깊이 1의 노드는 모두 당도(sugar)를 기준으로 훈련 세트를 나눕니다. 하지만 깊이 2의 노드는 맨 왼쪽의 노드만 당도를 기준으로 나누고 왼쪽에서 두 번째 노드는 알코올 도수(alcohol)를 기준으로 나눕니다. 오른쪽 두 노드는 pH를 사용합니다.
- 깊이 3에 있는 노드가 최종 노드인 리프 노드 입니다. 왼쪽에서 세 번째에 있는 노드만 음성 클래스가 더 많습니다. 이 노드에 도착해야만 레드 와인으로 예측합니다. 그럼 루트 노드부터 이 노드까지 도달하려면 당도는 -0.239보다 작고 또 -0.802보다 커야 합니다. 그리고 알코올 도수는 0.454보다 작아야 합니다. 그럼 세 번째 리프 노드에 도달합니다. 즉 당도가 -0.802보다 크고 -0.239보다 작은 와인 중에 알코올 도수가 0.454와 같거나 작은 것이 레드 와인입니다. 
- 특성값의 스케일은 결정 트리 알고리즘에 아무런 영향을 미치지 않습니다. 따라서 표준화 전처리를 할 필요가 없습니다. 이것이 결정 트리 알고리즘의 또 다른 장점 중 하나입니다.

```python
dt = DecisionTreeClassifier(max_depth=3, random_state=42)
dt.fit(train_input, train_target)

print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))
```

- 앞서 전처리하기 전의 훈련 세트(train_input)와 테스트 세트(test_input)로 결정 트리 모델을 다시 훈련해 봅니다.

```
0.8454877814123533
0.8415384615384616
```

- 결과는 같습니다.

```python
plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()
```

- 트리를 그려보면 다음과 같습니다.

![스크린샷 2024-11-06 오전 7 01 13](https://github.com/user-attachments/assets/4621f4fc-b7ed-41cd-8e02-5134ce138d3b)

- 결과를 보면 같은 트리지만, 특성값을 표준점수로 바꾸지 않은 터라 이해하기가 훨씬 쉽습니다. 
- 당도가 1.625보다 크고 4.325보다 작은 와인 중에 알코올 도수가 11.025와 같거나 작은 것이 레드 와인입니다. 그 외에는 모두 화이트 와인으로 예측했습니다.

```python
print(dt.feature_importances_)
```

- 결정 트리는 어떤 특성이 가장 유용한지 나타내는 특성 중요도를 계산해 줍니다. 
- 이 트리의 루트 노드와 깊이 1에서 당도를 사용했기 때문에 아마도 당도(sugar)가 가장 유용한 특성 중 하나입니다.
- 특성 중요도는 결정 트리 모델의 `feature_importances_` 속성에 저장되어 있습니다. 


```
[0.12345626 0.86862934 0.0079144 ]
```

- 두 번째 특성인 당도가 0.87 정도로 특성 중요도가 가장 높습니다. 그 다음 알코올 도수, pH 순입니다. 이 값을 모두 더하면 1이 됩니다. 
- 특성 중요도는 각 노드의 정보 이득과 전체 샘플에 대한 비율을 곱한 후 특성별로 더하여 계산합니다. 
- 특성 중요도를 활용하면 결정 트리 모델을 특성 선택에 활용할 수 있습니다. 이것이 결정 트리 알고리즘의 장점 중에 하나입니다. 