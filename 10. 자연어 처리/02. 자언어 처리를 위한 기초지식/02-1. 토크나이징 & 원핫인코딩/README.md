# 토크나이징(Tokenizing)

- **토크나이징**(Tokenizing)은 전체 텍스트를 **원하는 구분 단위로 나누는 것**을 의미합니다.
- 전체 텍스트를 문장 단위로 나눌 수도 있고, 전체 문장을 단어 단위로 나눌 수 있습니다. 또는 형태소 단위까지 토크나이징을 진행할 수도 있습니다.
- 문장을 단어 단위로 토크나이징하는 예시를 살펴보면 아래와 같습니다.
- **Input** : “Friends, Romans, Countrymen, lend me your ears;.”
- **Output** : \[“Friends”,”Romans”,”Countrymen”,”lend”,”me”,”your”,”ears”\]

# 원핫 인코딩(One-hot Encoding)

- <b>원핫 인코딩(One-hot Encoding)</b>은 범주형 값(Categorical Value)을 이진화된 값(Binary Value)으로 바꿔서 표현하는 것을 의미합니다. 범주형 값은 예를 들어 “개”, “고양이”, “말”이라는 3개의 범주형 데이터가 있을 때 이를 \[“개”=1, “고양이”=2, “말”=3\]이라고 단순하게 Integer Label Encoding으로 변환하여 표현하는 것입니다.
- 이에 반해 One-hot Encoding을 사용하면 범주형 데이터를 “개”=\[1 0 0\], “고양이”=[0 1 0], “말”=\[0 0 1\] 형태로 **해당 레이블을 나타내는 인덱스만 1의 값을 가지고 나머지 부분은 0의 값을 가진 Binary Value**로 표현합니다.

![스크린샷 2024-12-08 오후 3 03 11](https://github.com/user-attachments/assets/c9272ec1-a62a-4346-b34d-9f198edd8372)


- 단순한 Integer Encoding의 문제점은 머신러닝 알고리즘이 정수 값으로부터 **잘못된 경향성을 학습하게 될 수도 있다는 점**입니다. 예를 들어, 위의 예시의 경우 Integer Encoding을 사용할 경우 머신러닝 알고리즘이 [“개”(=1)와 “말”(=3)의 평균(1+3/2=2)은 “고양이”(=2)이다.] 라는 지식을 학습할 수도 있는데, 이는 명백히 잘못된 학습입니다. 따라서 머신 러닝 알고리즘을 구현할 때 타겟 데이터를 One-hot Encoding 형태로 표현하는 것이 일반적입니다.

- 전통적인 자연어 처리 방법론에서는 단어 하나를 원핫 인코딩 형태로 표현합니다.
- 이때 원핫 인코딩 벡터의 크기는 <b>사용하는 어휘집합(Vocabulary Set)의 크기</b>가 됩니다.
- 어휘 집합의 크기가 일반적으로 큰 값이기 때문에 <b>단어 표현이 희박(Sprase)</b>해 지게 됩니다.
- 예를 들어 어휘집합의 크기가 10,000이라면 9999개는 0이 들어간 인코딩 행렬이 만들어지게 됩니다.
\[사람\] = \[0 0 0 0 0 0 0 …1 .. . 0 0 0 0 0 0 0 0 0\]
