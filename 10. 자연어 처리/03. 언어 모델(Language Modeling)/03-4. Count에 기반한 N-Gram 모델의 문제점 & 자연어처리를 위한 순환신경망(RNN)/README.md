## N-gram 언어모델의 문제점

- N-gram 언어모델은 다음과 같은 문제점을 갖고 있습니다.
- 단어의 조합에 대한 경우의 수가 엄청나게 많기 때문에 아무리 많은 데이터를 수집하더라도 **특정 단어 조합의 경우 데이터셋에 한번도 존재하지않아서 계산식의 분모나 분자가 0**이 될 수 있습니다.

<img width="342" alt="스크린샷 2024-12-08 오후 7 20 22" src="https://github.com/user-attachments/assets/a567f14b-0a07-404a-a1cf-9457462762f8">

- 이런 문제를 해결하기 위해 최근에는 **딥러닝에 기반한 언어모델**이 주로 사용되고 있습니다.
- 이번 시간에는 딥러닝 모델중 하나인 RNN에 기반한 <b>글자(Character) 단위 언어모델링 방법인 Char-RNN 기법</b>을 실습해봅시다.

## 순환 신경망(RNN)

- CNN이 컴퓨터 비전Computer Vision 문제에 주로 사용되는 인공신경망 구조라면 이번 장에서 배울 <b>순환신경망(Recurrent Neural Networks(RNN))</b>은 <b>자연어 처리(Natural Language Processing(NLP))</b> 문제에 주로 사용되는 인공신경망 구조입니다.
- 좀 더 정확히 말하면, RNN은 시계열 데이터를 다루기에 최적화된 인공신경망입니다.
- **시계열 데이터**란 시간축을 중심으로 현재 시간의 데이터가 앞, 뒤 시간의 데이터와 연관 관계를 가지고 있는 데이터를 의미합니다. 예를 들어, 오늘의 주식 가격은 어제의 주식 가격과 연관이 있고, 내일의 주식 가격은 오늘의 주식 가격과 연관이 있습니다. 따라서 주식 가격은 시계열 데이터로 볼 수 있습니다.
- 주식 가격 이외에도 파형으로 표현되는 음성 데이터, 앞뒤 문맥을 가진 단어들의 집합으로 표현되는 자연어 데이터 등이 대표적인 시계열 데이터입니다.
- 이제 RNN 구조를 구체적으로 살펴봅시다. 아래 그림은 RNN의 구조를 나타냅니다.
- RNN은 기본적인 ANN 구조에서 이전 시간(t-1)의 은닉층의 출력값을 다음 시간(t)에 은닉층의 입력값으로 다시 집어넣는 경로가 추가된 형태입니다. 이 구조는 <b>“recurrent(순환되는)”</b>라는 단어에서 알 수 있듯이, 현재 시간 t의 결과가 다음 시간 t+1에 영향을 미치고, 이는 다시 다음 시간 t+2에 영향을 미치는 과정이 끊임없이 반복되는 인공신경망 구조입니다.

<img width="553" alt="스크린샷 2024-12-08 오후 7 26 14" src="https://github.com/user-attachments/assets/380d7fe5-6e2a-4009-ab39-d3c6e6bea246">

<img width="1135" alt="스크린샷 2024-12-08 오후 7 29 37" src="https://github.com/user-attachments/assets/477cf936-9acd-4a50-83df-9c2a7a7b7dbd">


<img width="1143" alt="스크린샷 2024-12-08 오후 7 29 53" src="https://github.com/user-attachments/assets/240b0985-b7b6-48d4-958a-442ef1395391">


- RNN을 다른 관점으로 바라보면, 시간축에 따라 인공신경망을 펼친Unfold 형태로 생각할 수 도 있습니다.
- 예를 들어, 5개의 단어로 이루어진 문장을 RNN의 인풋으로 사용한다면, 순환 연결이 없는 인공신경망을 5층으로 쌓은 것으로 바라볼 수 있습니다.
- 아래 그림은 unfold 형태의 RNN을 나타냅니다.
  
<img width="796" alt="스크린샷 2024-12-08 오후 7 43 55" src="https://github.com/user-attachments/assets/421f7468-34d4-42f7-9f68-019102d25920">


## Char-RNN의 개념

- Char-RNN은 RNN을 처음 배울 때 가장 많이 사용되는 예제 중 하나로서 하나의 글자Character를 RNN의 입력값으로 받고, RNN은 다음에 올 글자를 예측하는 문제입니다.
- 이를 위해서 RNN의 **타겟 데이터를 인풋 문장에서 한 글자씩 뒤로 민 형태**로 구성하면 됩니다. 예를 들어서 **“HELLO”**라는 문장을 학습시키고 싶을 경우, RNN의 (인풋 데이터, 타겟 데이터) 쌍을 (H, E), (E,L), (L, L), (L,O)로 구성합니다.
- 그림은 Char-RNN을 이용해서 HELLO라는 문장을 학습시키는 예제를 보여줍니다.

<img width="678" alt="스크린샷 2024-12-08 오후 7 46 58" src="https://github.com/user-attachments/assets/007a9b5b-3a87-474e-b463-9197f69ca94d">


- 이때 Char-RNN의 출력값 형태는 학습에 사용하는 전체 문자(단어Vocabulary) 집합에 대한 소프트맥스 출력값이 됩니다. 따라서 영어 문자로만 구성된 데이터셋일 경우, 전체 문자 집합은 알파벳 글자 개수인 26이 될 것입니다.
- 즉, Char-RNN의 출력값은 다음에 올 26개의 알파벳 문자에 대한 확신의 정도를 나타내는 26×1 크기의 행렬이 될 것입니다.
- 그 중에서 argmax로 가장 확률이 높은 글자를 다음에 올 글자로 확정하고 그 글자를 이용해서 또 다음에 올 글자를 예측하는 과정을 반복합니다.
- 셰익스피어의 희곡 \<리처드 3세\>로 Char-RNN을 학습하고 샘플링한 결과 중 일부 발췌하면 그림과 같습니다.

<img width="441" alt="스크린샷 2024-12-08 오후 7 56 00" src="https://github.com/user-attachments/assets/144c628f-c314-471a-9686-b3e76bc7ace3">

- 한눈에 봐도 그럴듯한 텍스트를 생성해낸 모습을 볼 수 있습니다. 희곡은 Char-RNN의 학습 데이터로 널리 사용되는데 **RNN이 학습하는 것은 글자 배열의 패턴**이기 때문입니다.
- 희곡 같은 경우 문장의 첫 부분에 “등장인물:” 형태의 고정된 패턴이 반복해서 등장하기 때문에 Char-RNN의 학습에 적합합니다. 이전 그림 의 \<리처드 3세\> 샘플링 결과를 보면 <리처드 3세>의 실제 등장인물인 First Lord, Shepherd, KING RICHARD III, BENVOLIO, ANGELO 등의 이름을 정확히 생성해낸 모습을 볼 수 있습니다. 하지만 First MuntaSTsa와 같은 이상한 이름을 생성한 경우도 있습니다. 또한 생성한 영어 대사의 경우, 의미론적으로는 말이 안되지만 구조적으로는 그럴듯한 모습을 띄는 것을 알 수 있습니다.
- 이제 다른 학습 데이터로 재밌는 실험을 하나 더 해봅시다. 프로그래밍 언어 또한 언어의 문법에 따라 일정한 패턴으로 코드가 작성되기 때문에 Char-RNN의 학습에 적합합니다.
- 다음 그림은 C언어로 작성된 리눅스 소스 코드를 이용해서 Char-RNN을 학습시킨 후 샘플링한 결과입니다.

- 희곡의 경우와 마찬가지로 의미로는 말이 안되지만 구조적으로는 그럴 듯한 모습을 하는 것을 알 수 있습니다.
  
<img width="587" alt="스크린샷 2024-12-08 오후 7 58 57" src="https://github.com/user-attachments/assets/6afac174-3efc-4f45-985e-2d74391e4f20">


## Argmax 샘플링

<img width="743" alt="스크린샷 2024-12-08 오후 7 59 51" src="https://github.com/user-attachments/assets/c7ff681d-aba4-4640-96a6-9af1c846621a">

## Categorical Distribution 샘플링

<img width="575" alt="스크린샷 2024-12-08 오후 8 00 09" src="https://github.com/user-attachments/assets/e3af61c8-6e28-4af3-b2d1-99507970287d">


