# BERT(Bidirectional Encoder Representations from Transformers)

## ì „ì´ í•™ìŠµ(Transfer Learning)

- <b>ì „ì´ í•™ìŠµ(Transfer Learning)</b> ë˜ëŠ” Fine-Tuningì´ë¼ê³  ë¶€ë¥´ëŠ” ê¸°ë²•ì€ ì´ë¯¸ í•™ìŠµëœ Neural Networksì˜ íŒŒë¼ë¯¸í„°ë¥¼ <b>ìƒˆë¡œìš´ Taskì— ë§ê²Œ ë‹¤ì‹œ ë¯¸ì„¸ì¡°ì •(Fine-Tuning)</b>í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
- **ì»´í“¨í„° ë¹„ì „** ë¬¸ì œ ì˜ì—­ì—ì„œëŠ” **ImageNet ë“±ì˜ ë°ì´í„°ì…‹ì— ë¯¸ë¦¬ Pre-Training** ì‹œí‚¤ê³  ì´ íŒŒë¼ë¯¸í„°ë“¤ì„ **ë‚´ê°€ í’€ê³ ìí•˜ëŠ” ë¬¸ì œì— ë§ê²Œ Fine-Tuning**í•˜ëŠ” ê³¼ì •ì´ ê´‘ë²”ìœ„í•˜ê²Œ ì‚¬ìš©ë˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤.
- ìµœê·¼ì—ëŠ” **BERT, GPT ê°™ì€ ëŒ€ê·œëª¨ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸**ì´ ë“±ì¥í•˜ë©´ì„œ **ìì—°ì–´ ì²˜ë¦¬ ë¬¸ì œ ì˜ì—­ì—ì„œë„ ì „ì´ í•™ìŠµì˜ ê°œë…ì´ ê´‘ë²”ìœ„í•˜ê²Œ ì‚¬ìš©**ë˜ê³  ìˆìŠµë‹ˆë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 16 21](https://github.com/user-attachments/assets/b9193574-28b2-401e-8e4a-78413f4d732d)

## BERTì˜ í•µì‹¬ ì•„ì´ë””ì–´

- **BERTì˜ í•µì‹¬ ì•„ì´ë””ì–´** : ëŒ€ëŸ‰ì˜ ë‹¨ì–´ Corpusë¡œ <b>ì–‘ë°©í–¥ìœ¼ë¡œ(Bidirectional)</b> í•™ìŠµì‹œí‚¨ Pre-Trained ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì„ ì œê³µí•˜ê³ , ë§ˆì§€ë§‰ ë ˆì´ì–´ì— ê°„ë‹¨í•œ ANN ë“±ì˜ ì¶”ê°€ë§Œì„ í†µí•œ Fine-Tuningì„ ì´ìš©í•´ì„œ **ë‹¤ì–‘í•œ ìì—°ì–´ì²˜ë¦¬ Taskì— ëŒ€í•´ì„œ state-of-the-art ì„±ëŠ¥**ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒ

## ğ‘©ğ‘¬ğ‘¹ğ‘»<sub>ğ‘©ğ‘¨ğ‘ºğ‘¬</sub>ì™€ ğ‘©ğ‘¬ğ‘¹ğ‘»<sub>ğ‘³ğ‘¨ğ‘¹ğ‘®ğ‘¬</sub>

- **L** : number of layers(i.e., Transformer blocks)
- **H** : the hidden size
- **A** : the number of self-attention heads

- 2ê°€ì§€ ì‚¬ì´ì¦ˆì˜ BERT ëª¨ë¸ì„ ê³µê°œ
  - ğ‘©ğ‘¬ğ‘¹ğ‘»<sub>ğ‘©ğ‘¨ğ‘ºğ‘¬</sub> (L=12, H=768, A=12, Total Parameters=**110M**)
  - ğ‘©ğ‘¬ğ‘¹ğ‘»<sub>ğ‘³ğ‘¨ğ‘¹ğ‘®ğ‘¬</sub> (L=24, H=1024, A=16, Total Parameters=**340M**)

## BERT Overview

- ëŒ€ëŸ‰ì˜ corpus ë°ì´í„°ì…‹ìœ¼ë¡œ **Pre-training** -> ëª©ì ì— ë§ê²Œ **Fine-Tuning**

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 25 26](https://github.com/user-attachments/assets/5fea2877-8523-441c-a546-b1eadf02d171)

## BERT Input

- We use WordPiece embeddings (Wu et al., 2016) with a 30,000 tokenvocabulary.
