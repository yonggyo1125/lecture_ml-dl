# BERT(Bidirectional Encoder Representations from Transformers)

## ì „ì´ í•™ìŠµ(Transfer Learning)

- <b>ì „ì´ í•™ìŠµ(Transfer Learning)</b> ë˜ëŠ” Fine-Tuningì´ë¼ê³  ë¶€ë¥´ëŠ” ê¸°ë²•ì€ ì´ë¯¸ í•™ìŠµëœ Neural Networksì˜ íŒŒë¼ë¯¸í„°ë¥¼ <b>ìƒˆë¡œìš´ Taskì— ë§ê²Œ ë‹¤ì‹œ ë¯¸ì„¸ì¡°ì •(Fine-Tuning)</b>í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
- **ì»´í“¨í„° ë¹„ì „** ë¬¸ì œ ì˜ì—­ì—ì„œëŠ” **ImageNet ë“±ì˜ ë°ì´í„°ì…‹ì— ë¯¸ë¦¬ Pre-Training** ì‹œí‚¤ê³  ì´ íŒŒë¼ë¯¸í„°ë“¤ì„ **ë‚´ê°€ í’€ê³ ìí•˜ëŠ” ë¬¸ì œì— ë§ê²Œ Fine-Tuning**í•˜ëŠ” ê³¼ì •ì´ ê´‘ë²”ìœ„í•˜ê²Œ ì‚¬ìš©ë˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤.
- ìµœê·¼ì—ëŠ” **BERT, GPT ê°™ì€ ëŒ€ê·œëª¨ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸**ì´ ë“±ì¥í•˜ë©´ì„œ **ìì—°ì–´ ì²˜ë¦¬ ë¬¸ì œ ì˜ì—­ì—ì„œë„ ì „ì´ í•™ìŠµì˜ ê°œë…ì´ ê´‘ë²”ìœ„í•˜ê²Œ ì‚¬ìš©**ë˜ê³  ìˆìŠµë‹ˆë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 16 21](https://github.com/user-attachments/assets/b9193574-28b2-401e-8e4a-78413f4d732d)

## BERTì˜ í•µì‹¬ ì•„ì´ë””ì–´

- **BERTì˜ í•µì‹¬ ì•„ì´ë””ì–´** : ëŒ€ëŸ‰ì˜ ë‹¨ì–´ Corpusë¡œ <b>ì–‘ë°©í–¥ìœ¼ë¡œ(Bidirectional)</b> í•™ìŠµì‹œí‚¨ Pre-Trained ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì„ ì œê³µí•˜ê³ , ë§ˆì§€ë§‰ ë ˆì´ì–´ì— ê°„ë‹¨í•œ ANN ë“±ì˜ ì¶”ê°€ë§Œì„ í†µí•œ Fine-Tuningì„ ì´ìš©í•´ì„œ **ë‹¤ì–‘í•œ ìì—°ì–´ì²˜ë¦¬ Taskì— ëŒ€í•´ì„œ state-of-the-art ì„±ëŠ¥**ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒ

## ğ‘©ğ‘¬ğ‘¹ğ‘»<sub>ğ‘©ğ‘¨ğ‘ºğ‘¬</sub>ì™€ ğ‘©ğ‘¬ğ‘¹ğ‘»<sub>ğ‘³ğ‘¨ğ‘¹ğ‘®ğ‘¬</sub>

- **L** : number of layers(i.e., Transformer blocks)
- **H** : the hidden size
- **A** : the number of self-attention heads

- 2ê°€ì§€ ì‚¬ì´ì¦ˆì˜ BERT ëª¨ë¸ì„ ê³µê°œ
  - ğ‘©ğ‘¬ğ‘¹ğ‘»<sub>ğ‘©ğ‘¨ğ‘ºğ‘¬</sub> (L=12, H=768, A=12, Total Parameters=**110M**)
  - ğ‘©ğ‘¬ğ‘¹ğ‘»<sub>ğ‘³ğ‘¨ğ‘¹ğ‘®ğ‘¬</sub> (L=24, H=1024, A=16, Total Parameters=**340M**)

## BERT Overview

- ëŒ€ëŸ‰ì˜ corpus ë°ì´í„°ì…‹ìœ¼ë¡œ **Pre-training** -> ëª©ì ì— ë§ê²Œ **Fine-Tuning**

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 25 26](https://github.com/user-attachments/assets/5fea2877-8523-441c-a546-b1eadf02d171)

## BERT Input

- We use WordPiece embeddings (Wu et al., 2016) with a 30,000 tokenvocabulary.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 27 07](https://github.com/user-attachments/assets/ddf4ef96-cff5-493c-b21f-e09e97425a6d)

## Pre-training BERT â€’ Task 1 â€’ Masked LM(MLM)

- 2ê°€ì§€ ë¹„ì§€ë„ í•™ìŠµ ë¬¸ì œ(Unsupervised Task)ì— ëŒ€í•´ BERTë¥¼ Pre-Training í•¨
- <b>Task 1 - Masked LM(MLM)</b> : ì¸í’‹ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ Mask(\[MASK\] í† í°)ë¡œ ê°€ë¦¬ê³  ê°€ë¦° Maskì— ëŒ€í•œ Predictionì„ ìˆ˜í–‰í•˜ë„ë¡ í•™ìŠµì‹œí‚´
- ì „ì²´ ì‹¤í—˜ì—ì„œ WordPiece í† í°ì—ì„œ ëœë¤í•˜ê²Œ **15%**ë¥¼ ë§ˆìŠ¤í¬ ì²˜ë¦¬í•  ëŒ€ìƒìœ¼ë¡œ ì„ íƒí•¨
- ì„ íƒëœ ëŒ€ìƒì—ì„œ
  - **80%**ëŠ” \[MASK\] í† í°ìœ¼ë¡œ Masking ì²˜ë¦¬ í•¨
  - **10%**ëŠ” ëœë¤í•œ í† í°ìœ¼ë¡œ ë³€ê²½í•¨
  - **10%**ëŠ” ì›ë˜ ë‹¨ì–´ë¥¼ ìœ ì§€í•¨

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 35 19](https://github.com/user-attachments/assets/c6aa9b0c-da8f-43f3-8eff-bb3dc3159a43)

## Pre-training BERT â€’ Task 2 - Next Sentence Prediction (NSP)

- <b>Task 2 â€’ Next Sentence Prediction(NSP)</b> : 2ê°œì˜ ë¬¸ì¥ì´ ì´ì–´ì§€ëŠ” ë¬¸ì¥ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ì´ì§„ ë¶„ë¥˜(binary prediction)í•˜ë„ë¡ í•™ìŠµì‹œí‚´
- ë°ì´í„°ì…‹ êµ¬ì„± ê³¼ì •ì—ì„œ 50%ëŠ” ì‹¤ì œë¡œ Aì™€ Bê°€ ì´ì–´ì§€ëŠ” ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•¨ (IsNextë¼ëŠ” ë ˆì´ë¸”ë¡œ ì„¤ì •)
- 50%ëŠ” ëœë¤í•œ ë¬¸ì¥ ë¬¶ìŒìœ¼ë¡œ êµ¬ì„±í•¨ (NotNextë¼ëŠ” ë ˆì´ë¸”ë¡œ ì„¤ì •)

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 37 53](https://github.com/user-attachments/assets/05911c38-49a3-4330-a0cb-8641d47d97ab)

- ì•„ë˜ ê·¸ë¦¼ì—ì„œ C ë¶€ë¶„ì´ Next Sentence Prediction ì˜ˆì¸¡ì— ëŒ€ì‘ë˜ëŠ” ë¶€ë¶„
- í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì€ NSP íƒœìŠ¤í¬ì— ëŒ€í•´ì„œ **97%-98%**ì˜ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤Œ

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 39 58](https://github.com/user-attachments/assets/5567f1b0-41d0-4264-912c-8393c1d39f7b)

## Pre-training BERT â€’ Pre-Training Dataset

- Pre-Trainingì„ ìœ„í•´ ë‹¤ìŒì˜ ë°ì´í„° ì…‹ì„ ì‚¬ìš©
- BooksCorpus (**800M words**)
- English Wikipedia (**2,500M words**)
- Wikipedia ë°ì´í„°ì…‹ì—ì„œëŠ” í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©í•˜ê³  ë¦¬ìŠ¤íŠ¸, í…Œì´ë¸”, í—¤ë” ë“±ì€ ë¬´ì‹œí•¨

## Pre-training Procedure

- í•™ìŠµ ë°ì´í„°ì˜ ìµœëŒ€ ê¸¸ì´ëŠ” 512 í† í°ìœ¼ë¡œ ì§€ì •
- Batch size 256ìœ¼ë¡œ í•™ìŠµ (256 sequences \* 512 tokens = 128,000 tokens/batch)
- 1,000,000 steps í•™ìŠµ, ì´ëŠ” ì•½ 3.3 billion(33ì–µ) word corpusì— ëŒ€í•œ 40 epoch ì •ë„ì˜ í•™ìŠµ
- Adam Optimizer ì‚¬ìš© (lr=1e-4, ğ›½! = 0.9, ğ›½" = 0.99, L2 weight decay of 0.01,
- learning rate warmup over the first 10,000 steps, and linear decay of the learning rate.)
- We use a dropout probability of 0.1 on all layers. We use a gelu activation(Hendrycks and Gimpel, 2016) rather than the standard relu, following OpenAI GPT.
- The training loss is the sum of the mean masked LM likelihood and the mean next sentence prediction likelihood.

## Pre-training Procedure

Training of ğµğ¸ğ‘…ğ‘‡ ()\*+ was performed on <b>4 Cloud TPUs in Pod configuration (16 TPU chips total).</b>
â€¢ Training of ğµğ¸ğ‘…ğ‘‡<sub>LARGE</sub> was performed on <b>16 Cloud TPUs (64 TPU chips total).</b>
â€¢ Each pre-training took <b>4 days to complete.</b>
â€¢ Longer sequences are disproportionately expensive because attention is quadratic to the sequence length.
â€¢ To speed up pre-traing in our experiments, we pre-train the model with sequence length of 128 for 90% of the steps.
â€¢ Then, we train the rest 10% of the steps of sequence of 512 to learn the positional embeddings.

## Fine-Tuning BERT

- Compared to pre-training, **fine-tuning is relatively inexpensive.**
  â€¢ All of the results in the paper <b>can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU</b>, starting from the exact same pre-trained model.

## Experiment Result

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-14 á„‹á…©á„’á…® 11 13 30](https://github.com/user-attachments/assets/a583c0a1-27fd-4c07-9ac9-72bc0ee26a7a)



## BERT Input

- **Token Id** : í† í°í™”ëœ ì¸í’‹ ë°ì´í„°
- **Mask Id** : ì‹¤ì œ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ ë¶€ë¶„ì„ Binaryë¡œ í‘œí˜„
- **Segment Id** : ì¸í’‹ ë¬¸ì¥ì´ ì—¬ëŸ¬ê°œì¸ì§€ êµ¬ë¶„ (ì¸í’‹ ë¬¸ì¥ì´ í•˜ë‚˜ë©´ ëª¨ë‘ 0ë¡œ, 2ê°œë©´ ì²«ë¬¸ì¥ì€ 0, ë’·ë¬¸ì¥ì€ 1ë¡œ êµ¬ë¶„)

## ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œëœ BERT

- êµ¬ê¸€ì—ì„œëŠ” ëŒ€ìš©ëŸ‰ ë°ì´í„° ì…‹ì— ëŒ€í•´ í•™ìŠµì‹œí‚¨ BERT ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ëª¨ë‘ **ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œ**í•˜ì˜€ìŠµë‹ˆë‹¤.
- ë”°ë¼ì„œ ìš°ë¦¬ê°€ íŠ¹ì • ìì—°ì–´ ì²˜ë¦¬ ë¬¸ì œ ì˜ì—­ì„ í•´ê²°í•˜ê³ ì í•  ë•Œ êµ¬ê¸€ì—ì„œ ê³µê°œí•œ BERT ëª¨ë¸ì„ í† ëŒ€ë¡œ Fine-Tuningì„ ì§„í–‰í•˜ë©´ **ì ì€ ë…¸ë ¥ìœ¼ë¡œë„ ê³ ì„±ëŠ¥ì˜ ìì—°ì–´ì²˜ë¦¬ ëª¨ë¸**ì„ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤

## BERT ê³µì‹ êµ¬í˜„ì²´

- https://github.com/google-research/bert
