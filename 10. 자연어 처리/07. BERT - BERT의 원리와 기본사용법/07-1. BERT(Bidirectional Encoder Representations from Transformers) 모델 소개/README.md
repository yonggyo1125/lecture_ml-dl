# BERT(Bidirectional Encoder Representations from Transformers)

## ì „ì´ í•™ìŠµ(Transfer Learning)

- <b>ì „ì´ í•™ìŠµ(Transfer Learning)</b> ë˜ëŠ” Fine-Tuningì´ë¼ê³  ë¶€ë¥´ëŠ” ê¸°ë²•ì€ ì´ë¯¸ í•™ìŠµëœ Neural Networksì˜ íŒŒë¼ë¯¸í„°ë¥¼ <b>ìƒˆë¡œìš´ Taskì— ë§ê²Œ ë‹¤ì‹œ ë¯¸ì„¸ì¡°ì •(Fine-Tuning)</b>í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
- **ì»´í“¨í„° ë¹„ì „** ë¬¸ì œ ì˜ì—­ì—ì„œëŠ” **ImageNet ë“±ì˜ ë°ì´í„°ì…‹ì— ë¯¸ë¦¬ Pre-Training** ì‹œí‚¤ê³  ì´ íŒŒë¼ë¯¸í„°ë“¤ì„ **ë‚´ê°€ í’€ê³ ìí•˜ëŠ” ë¬¸ì œì— ë§ê²Œ Fine-Tuning**í•˜ëŠ” ê³¼ì •ì´ ê´‘ë²”ìœ„í•˜ê²Œ ì‚¬ìš©ë˜ê³  ìˆì—ˆìŠµë‹ˆë‹¤.
- ìµœê·¼ì—ëŠ” **BERT, GPT ê°™ì€ ëŒ€ê·œëª¨ ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸**ì´ ë“±ì¥í•˜ë©´ì„œ **ìì—°ì–´ ì²˜ë¦¬ ë¬¸ì œ ì˜ì—­ì—ì„œë„ ì „ì´ í•™ìŠµì˜ ê°œë…ì´ ê´‘ë²”ìœ„í•˜ê²Œ ì‚¬ìš©**ë˜ê³  ìˆìŠµë‹ˆë‹¤.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 16 21](https://github.com/user-attachments/assets/b9193574-28b2-401e-8e4a-78413f4d732d)

## BERTì˜ í•µì‹¬ ì•„ì´ë””ì–´

- **BERTì˜ í•µì‹¬ ì•„ì´ë””ì–´** : ëŒ€ëŸ‰ì˜ ë‹¨ì–´ Corpusë¡œ <b>ì–‘ë°©í–¥ìœ¼ë¡œ(Bidirectional)</b> í•™ìŠµì‹œí‚¨ Pre-Trained ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì„ ì œê³µí•˜ê³ , ë§ˆì§€ë§‰ ë ˆì´ì–´ì— ê°„ë‹¨í•œ ANN ë“±ì˜ ì¶”ê°€ë§Œì„ í†µí•œ Fine-Tuningì„ ì´ìš©í•´ì„œ **ë‹¤ì–‘í•œ ìì—°ì–´ì²˜ë¦¬ Taskì— ëŒ€í•´ì„œ state-of-the-art ì„±ëŠ¥**ì„ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒ

## ğ‘©ğ‘¬ğ‘¹ğ‘»<sub>ğ‘©ğ‘¨ğ‘ºğ‘¬</sub>ì™€ ğ‘©ğ‘¬ğ‘¹ğ‘»<sub>ğ‘³ğ‘¨ğ‘¹ğ‘®ğ‘¬</sub>

- **L** : number of layers(i.e., Transformer blocks)
- **H** : the hidden size
- **A** : the number of self-attention heads

- 2ê°€ì§€ ì‚¬ì´ì¦ˆì˜ BERT ëª¨ë¸ì„ ê³µê°œ
  - ğ‘©ğ‘¬ğ‘¹ğ‘»<sub>ğ‘©ğ‘¨ğ‘ºğ‘¬</sub> (L=12, H=768, A=12, Total Parameters=**110M**)
  - ğ‘©ğ‘¬ğ‘¹ğ‘»<sub>ğ‘³ğ‘¨ğ‘¹ğ‘®ğ‘¬</sub> (L=24, H=1024, A=16, Total Parameters=**340M**)

## BERT Overview

- ëŒ€ëŸ‰ì˜ corpus ë°ì´í„°ì…‹ìœ¼ë¡œ **Pre-training** -> ëª©ì ì— ë§ê²Œ **Fine-Tuning**

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 25 26](https://github.com/user-attachments/assets/5fea2877-8523-441c-a546-b1eadf02d171)

## BERT Input

- We use WordPiece embeddings (Wu et al., 2016) with a 30,000 tokenvocabulary.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 27 07](https://github.com/user-attachments/assets/ddf4ef96-cff5-493c-b21f-e09e97425a6d)

## Pre-training BERT â€’ Task 1 â€’ Masked LM(MLM)

- 2ê°€ì§€ ë¹„ì§€ë„ í•™ìŠµ ë¬¸ì œ(Unsupervised Task)ì— ëŒ€í•´ BERTë¥¼ Pre-Training í•¨
- <b>Task 1 - Masked LM(MLM)</b> : ì¸í’‹ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ Mask(\[MASK\] í† í°)ë¡œ ê°€ë¦¬ê³  ê°€ë¦° Maskì— ëŒ€í•œ Predictionì„ ìˆ˜í–‰í•˜ë„ë¡ í•™ìŠµì‹œí‚´
- ì „ì²´ ì‹¤í—˜ì—ì„œ WordPiece í† í°ì—ì„œ ëœë¤í•˜ê²Œ **15%**ë¥¼ ë§ˆìŠ¤í¬ ì²˜ë¦¬í•  ëŒ€ìƒìœ¼ë¡œ ì„ íƒí•¨
- ì„ íƒëœ ëŒ€ìƒì—ì„œ
  - **80%**ëŠ” \[MASK\] í† í°ìœ¼ë¡œ Masking ì²˜ë¦¬ í•¨
  - **10%**ëŠ” ëœë¤í•œ í† í°ìœ¼ë¡œ ë³€ê²½í•¨
  - **10%**ëŠ” ì›ë˜ ë‹¨ì–´ë¥¼ ìœ ì§€í•¨

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 35 19](https://github.com/user-attachments/assets/c6aa9b0c-da8f-43f3-8eff-bb3dc3159a43)

## Pre-training BERT â€’ Task 2 - Next Sentence Prediction (NSP)
- <b>Task 2 â€’ Next Sentence Prediction(NSP)</b> : 2ê°œì˜ ë¬¸ì¥ì´ ì´ì–´ì§€ëŠ” ë¬¸ì¥ì¸ì§€ ì•„ë‹Œì§€ë¥¼ ì´ì§„ ë¶„ë¥˜(binary prediction)í•˜ë„ë¡ í•™ìŠµì‹œí‚´
- ë°ì´í„°ì…‹ êµ¬ì„± ê³¼ì •ì—ì„œ 50%ëŠ” ì‹¤ì œë¡œ Aì™€ Bê°€ ì´ì–´ì§€ëŠ” ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±í•¨ (IsNextë¼ëŠ” ë ˆì´ë¸”ë¡œ ì„¤ì •)
- 50%ëŠ” ëœë¤í•œ ë¬¸ì¥ ë¬¶ìŒìœ¼ë¡œ êµ¬ì„±í•¨ (NotNextë¼ëŠ” ë ˆì´ë¸”ë¡œ ì„¤ì •)

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 37 53](https://github.com/user-attachments/assets/05911c38-49a3-4330-a0cb-8641d47d97ab)

- ì•„ë˜ ê·¸ë¦¼ì—ì„œ C ë¶€ë¶„ì´ Next Sentence Prediction ì˜ˆì¸¡ì— ëŒ€ì‘ë˜ëŠ” ë¶€ë¶„
- í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì€ NSP íƒœìŠ¤í¬ì— ëŒ€í•´ì„œ **97%-98%**ì˜ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤Œ


![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2024-12-13 á„‹á…©á„’á…® 10 39 58](https://github.com/user-attachments/assets/5567f1b0-41d0-4264-912c-8393c1d39f7b)


