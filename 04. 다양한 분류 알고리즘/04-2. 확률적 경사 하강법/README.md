# 확률적 경사 하강법

## 키워드 정리

- **확률적 경사 하강법**
  - 훈련 세트에서 샘플 하나씩 꺼내 손실 함수의 경사를 따라 최적의 모델을 찾는 알고리즘입니다.
  - 샘플을 하나씩 사용하지 않고 여러 개를 사용하면 미니배치 경사 하강법이 됩니다.
  - 한 번에 전체 샘플을 사용하면 배치 경사 하강법이 됩니다.
- **손실 함수**
  - 확률적 경사 하강법이 최적화할 대상입니다.
  - 대부분의 문제에 잘 맞는 손실 함수가 이미 정의되어 있습니다.
  - 이진 분류에는 로지스틱 회귀(또는 이진 크로스엔트로피) 손실 함수를 사용합니다.
  - 다중 분류에는 크로스엔트로피 손실 함수를 사용합니다.
  - 회귀 문제에는 평균 제곱 오차 손실 함수를 사용합니다.
- **에포크**
  - 확률적 경사 하강법에서 전체 샘플을 모두 사용하는 한 번 반복을 의미합니다.
  - 일반적으로 경사 하강법 알고리즘은 수십에서 수백 번의 에포크를 반복합니다.

## 핵심 패키지와 함수

### scikit-learn

- **SGDClassifier**

  - 확률적 경사 하강법을 사용한 분류 모델을 만듭니다.
  - loss 매개변수는 확률적 경사 하강법으로 최적화할 손실 함수를 지정합니다. 기본값은 서포트 백터 머신을 위한 `hinge` 손실 함수입니다. 로지스틱 회귀를 위해서는 `log_loss`로 지정합니다.
  - `penalty` 매개변수에서 규제의 종류를 지정할 수 있습니다. 기본값은 L2 규제를 위한 'l2' 입니다. L1 규제를 적용하라면 'l1'으로 지정합니다. 규제 강도는 alpha 매개변수에서 지정합니다. 기본값은 0.0001 입니다.
  - `max_iter` 매개변수는 에포크 횟수를 지정합니다. 기본값은 1000입니다.
  - `tol` 매개변수는 반복을 멈출 조건입니다. `n_iter_no_charge` 매개변수에서 지정한 에포크 동안 손실이 tol 만큼 줄어들지 않으면 알고리즘이 중단됩니다. tol 매개변수의 기본값은 0.001 이고 `n_iter_no_charge` 매개변수의 기본값은 5입니다.

- **SGDRegressor**
  - 확률적 경사 하강법을 사용한 회귀모델을 만듭니다.
  - `loss` 매개변수에서 손실 함수를 지정합니다. 기본값은 제곱 오차를 나타내는 `squared_loss` 입니다.
  - 앞의 `SGDClassifier`에서 설명한 매개변수는 모두 `SGDRegressor`에서 동일하게 사용됩니다.

## 점진적인 학습

- 앞서 훈련한 모델을 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하는 방법
- 이런 식의 훈련방법을 **점진적 학습** 또는 온라인 학습이라고 부릅니다.
- 대표적인 학습 알고리즘은 **확률적 경사 하강법**(Stochastic Gradient Descent)입니다.

## 확률적 경사 하강법

- 확률적 경사 하강법에서 확률적이란 말은 **무작위하기** 혹은 **랜덤하게**의 기술적인 표현입니다.
- 경사 하강법은 경사를 따라 내려가는 방법을 말합니다.
- 가장 빠른 길은 경사가 가장 가파른 길이며, 경사 하강법이 바로 이런 방식입니다. 가장 가파른 경사를 따라 원하는 지점에 도달하는 것이 목표입니다.
- 가장 가파른 길을 찾아 내려오지만 조금씩 내려오는 것이 중요합니다. 이렇게 내려오는 과정이 바로 경사 하강법 모델을 훈련하는 것입니다.
- 확률적이라는 말을 이해해보려면 경사 하강법으로 내려올 때 가장 가파른 길을 찾는 방법이 무엇인지 생각해보는 것
- 훈련 세트의 전체 샘플을 사용하지 않고 딱 하나의 샘플을 훈련 세트에서 랜덤하게 골라 가장 가파른 길을 찾습니다. 이처럼 훈련 세트에서 랜덤하게 하나의 샘플을 고르는 것이 바로 **확률적 경사 하강법** 입니다.

> 확률적 경사 하강법은 훈련 세트에서 랜덤하게 하나의 샘플을 선택하여 가파른 경사를 조금 내려갑니다. 그다음 훈련 세트에서 랜덤하게 또 다른 샘플을 하나 선택하여 가파른 경사를 조금 내려갑니다. 그다음 훈련 세트에서 랜덤하게 또 다른 샘플을 하나 선택하여 경사를 조금 내려갑니다. 이런 식으로 모두 사용할 때까지 계속합니다.
>
> 모든 샘플을 다 사용했습니다. 그래도 산을 다 내려오지 못하였다면 다시 처음부터 시작하게 됩니다. 훈련 세트에 모든 샘플을 다시 채워 넣습니다. 그다음 다시 랜덤하게 하나의 샘플을 선택해 이어서 경사를 내려갑니다. 이렇게 만족할 만한 위치에 도달할 때까지 계속 내려가면 됩니다.
>
> 확률적 경사하강법에서 훈련 세트를 한 번 모두 사용하는 과정을 **에포크**(epoch)라고 부릅니다. 일반적으로 경사 하강법은 수십, 수백 번 이상 에포크를 수행합니다.

- **미니배치 경사 하강법**(minibatch gradient descent): 여러개의 샘플을 사용해 경사 하강법을 수행하는 방식
- **배치 경사 하강법**(batch gradient descent)
  - 한 번 경사로를 따라 이동하기 위해 전체 샘플을 사용하는 것
  - 전체 데이터를 사용하기 때문에 가장 안정적인 방법이 될 수 있습니다.
  - 하지만 전체 데이터를 사용하면 그만큼 컴퓨터의 자원을 많이 사용하게 됩니다.
  - 어떤 경우는 데이터가 너무너무 많아 한 번에 전체 데이터를 모두 읽을 수 없을지도 모릅니다.

![스크린샷 2024-11-03 오후 9 36 26](https://github.com/user-attachments/assets/bda256c2-60ea-4642-b0d7-4262cafe3291)

- 확률적 경사 하강법은 훈련 세트를 사용해 산 아래에 있는 최적의 장소로 조금씩 이동하는 알고리즘 입니다.
- 이 때문에 훈련 데이터가 모두 준비되어 있지 않고 매일매일 업데이트되어도 학습을 계속 이어나갈 수 있습니다.

> 확률적 경사 하강법을 꼭 사용하는 알고리즘이 있습니다. 바로 신경망 알고리즘 입니다. 신경망은 일반적으로 많은 데이터를 사용하기 때문에 한 번에 모든 데이터를 사용하기 어렵습니다. 또 모델이 매우 복잡하기 때문에 수학적인 방법으로 해답을 얻기 어렵습니다. 신경망 모델이 확률적 경사하강법이나 미니배치 경사 하강법을 사용합니다.

- 가장 빠른 길을 찾아 내려가려고 하는 산 -> 손실함수

## 손실 함수

- **손실함수**(loss function)는 어떤 문제에서 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준입니다.
- 손실 함수의 값이 작을 수록 좋으나 어떤 값이 최솟값인지는 알지 못합니다. 가능한 많이 찾아보고 만족할만한 수준이라면 산을 다 내려왔다고 인정해야 합니다.
- 이 값을 찾아서 조금씩 이동하려면 확률적 경사 하강법이 잘 맞습니다.
- 다행히도 우리가 다루는 많은 문제에 필요한 손실 함수는 이미 정의되어 있습니다.

- 분류에서 손실은 정답을 못 맞히는 것 입니다.
- 도미와 빙어를 구분하는 이진 문제를 예로 들면 도미는 양성 클래스(1), 빙어는 음성 클래스(0)라고 가정해 봅시다.

![스크린샷 2024-11-04 오전 6 02 32](https://github.com/user-attachments/assets/87cabd9c-a73f-4add-9554-ee6f9966d7a3)

- 정확도는 4개의 예측 중에 2개만 맞았으므로 정확도는 1/2 = 0.5 입니다.
- 정확도를 손실함수로 사용할 수 있습니다. 정확도에 음수를 취하면 -1.0이 가장 낮고, -0.0이 가낭 높습니다.
- 하지만 정확도는 치명적인 단점이 있습니다. 예를 들어 앞의 그림과 같이 4개의 샘플만 있다면 가능한 정확도는 0, 0.25, 0.5, 0.75, 1 다섯 가지 뿐입니다.
- 정확도가 이렇게 듬성듬성하다면 경사 하강법을 이용해 조금씩 움질일 수 없습니다. 산의 경사면은 확실히 연속적이어야 합니다.
- 기술적으로 말하면 손실 함수는 미분 가능해야 합니다.
- 예측은 0 또는 1이지만 확률은 0\~1 사이의 어떤 값도 될 수 있습니다. 즉 연속적입니다.
- 예를 들어 위 샘플 4개의 예측 확률을 각각 0.9, 0.3, 0.2, 0.8 이라고 가정해 보겠습니다.

![스크린샷 2024-11-04 오전 6 07 22](https://github.com/user-attachments/assets/6c426715-1a18-4fa4-9be4-bc3734f7e97c)

## 로지스틱 손실 함수

- 첫 번째 샘플의 예측은 0.9이므로 양성 클래스의 타깃인 1과 곱한 다음 음수로 바꿀 수 있습니다. 이 경우 예측이 1에 가까울수록 좋은 모델입니다.
- 예측이 1에 가까울수록 예측과 타깃의 곱의 음수는 점점 작아집니다. 이 값을 손실 함수로 사용해도 괜찮습니다.

![스크린샷 2024-11-04 오전 6 27 59](https://github.com/user-attachments/assets/c0c5d6d6-40de-4307-af1d-7c5518ff7b99)

- 두 번째 샘플의 예측은 0.3입니다. 타깃이 양성 클래스(1) 인데 거리가 멉니다. 위에서와 마찬가지로 예측과 타깃을 곱해 음수로 바꿔봅니다.
- 이 값은 -0.3이 되기 때문에 확실히 첫 번째 샘플보다 높은 손실이 됩니다.

![스크린샷 2024-11-04 오전 6 13 20](https://github.com/user-attachments/assets/6433ca55-6987-4096-a712-ed22799d1dc7)

- 세 번째 샘플의 타깃은 음성 클래스가 0이군요. 이 값을 예측 확률인 0.2와 그대로 곱해서는 곤란합니다. 무조건 0이 되므로.
- 한 가지 방법은 타깃을 마치 양성 클래스처럼 바꾸어 1로 만드는 것, 대신 예측값도 양성 클래스에 대한 예측으로 바뀝니다. 즉 1 - 0.2 = 0.8로 사용합니다. 그 다음 곱하고 음수로 바꾸는 것은 위와 동일

![스크린샷 2024-11-04 오전 6 23 00](https://github.com/user-attachments/assets/01e3c426-ccf1-4317-a7c4-d29525f07c76)

- 세 번째 샘플은 음성 클래스인 타깃을 맞추었으므로 손실이 낮아야 합니다. -0.8은 꽤 낮은 손실입니다.
- 네 번째 샘플을 보면 네 번째 샘플도 타깃은 음성 클래스입니다. 하지만 정답을 맞히지 못하였습니다. 타깃을 1로 바꾸고 예측 확률을 1에서 뺀 다음 곱해서 음수로 바꿔 봅니다.

![스크린샷 2024-11-04 오전 6 23 11](https://github.com/user-attachments/assets/023b0055-8589-4383-8e42-82dfa9c7d3d7)

- 네 번째 샘플의 손실이 높습니다. 예측 확률을 사용해 이런 방식으로 계산하면 연속적인 손실 함수를 얻을 수 있습니다. 여기에는 예측 확률에 로그 함수를 적용하면 좋습니다
- 예측 확률의 범위는 0\~1 사이인데, 로그 함수는 이 사이에서 음수가 되므로 최종 손실 값은 양수가 됩니다. 손실이 양수가 되면 이해하기 더 쉽습니다.
- 로그 함수는 0에 가까울수록 큰 음수가 되기 때문에 손실을 아주 크게 만들어 모델에 큰 영향을 미칠 수 있습니다.

![스크린샷 2024-11-04 오전 6 23 23](https://github.com/user-attachments/assets/2ef1411a-3cbc-4fce-9199-4303e385aee5)

- 양성 클래스(타깃 = 1)일 때 손실은 -log(예측 확률)로 계산합니다.
- 확률이 1에서 멀어져 0에 가까워질수록 손실은 아주 큰 양수가 됩니다. 음성 클래스(타깃 = 0)일 떄 손실은 -log(1 - 예측 확률)로 계산합니다.
- 이 예측 확률이 0에서 멀어져 1에 가까워질수록 손실은 아주 큰 양수가 됩니다.
- 이 손실 함수를 **로지스틱 손실 함수**(logistic loss function)라고 부릅니다. 또는 **이진 크로스엔트로피 손실 함수**(binary cross-entropy loss function)라고도 부릅니다.
- 이 손실 함수를 사용하면 로지스틱 회귀 모델이 만들어집니다.
- 다중 분류도 매우 비슷한 손실 함수를 사용합니다. 다중 분류에서 사용하는 손실함수를 **크로스엔트로피 손실함수**(cross-entropy loss function)라고 부릅니다.
- 이진 분류는 로지스틱 손실 함수를 사용하고 다중 분류는 크로스엔트로피 손실 함수를 사용합니다.

> 회귀의 손실 함수로 평균 절대값 오차를 사용할 수 있습니다. 타깃에서 예측을 뺀 절대값을 모든 샘플에 평균한 값입니다. 또는 **평균 제곱 오차**(mean squared error)를 많이 사용합니다. 타깃에서 예측을 뺀 값을 제곱한 다음 모든 샘플에 평균한 값, 확실히 이 값이 작을수록 좋은 모델입니다.

## SGDClassifier

```python
import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')
```

- fish_csv_data 파일에서 판다스 데이터프레임을 만듭니다.

```python
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()
```

- Species 열을 제외한 나머지 5개는 입력데이터로 사용합니다. Species 열은 타깃 데이터 입니다.

```python
from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
```

- 사이킷런의 `train_test_split()` 함수를 사용해 이 데이터를 훈련 세트와 테스트 세트로 나눕니다.

```python
from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)
```

- 훈련 세트와 테스트 세트의 특성을 표준화 전처리합니다.
- 반드시 훈련 세트에서 학습한 통계 값으로 테스트 세트로 변환해야 합니다.

```python
from sklearn.linear_model import SGDClassifier
```

- 사이킷 런에서 확률적 경사 하강법을 제공하는 대표적인 분류용 클래스는 `SGDClassifier` 입니다.
- `sklearn.linear_model` 패키지 아래에 임포트해 봅시다.

```python
sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

- `SGDClassifier`의 객체를 만들 때 2개의 매개변수를 지정합니다.
- `loss`는 손실 함수의 종류를 지정합니다. 여기에서는 `loss='log_loss'` 로 지정하여 로지스틱 손실 함수를 지정했습니다.
- `max_iter`는 수행할 에포크 횟수를 지정합니다. 10으로 지정하여 전체 훈련 세트를 10회 반복하겠습니다.
- 그 다음 훈련 세트와 테스트 세트에서 정확도 점수를 출력합니다.

> 다중 분류일 경우 `SGDClassifier`에 `loss='log_loss'`로 지정하면 클래스마다 이진 분류 모델을 만듭니다. 즉, 도메는 양성 클래스를 두고 나머지를 모두 음성 클래스로 두는 방식입니다. 이런 방식을 OvR(One Versus Rest)이라고 부릅니다.

```
0.773109242697479
0.775
```

- 출력된 훈련 세트와 테스트 세트의 정확도가 낮습니다. 지정한 반복 횟수 10번이 부족한 것으로 보입니다.

```python
sc.partial_fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

- `SGDClassifier` 객체를 다시 만들지 않고 훈련한 모델 sc를 추가로 더 훈련할 수 있습니다.
- 모델을 이어서 훈련할 때는 `partial_fit()` 메서드를 사용합니다.
- 이 메서드는 `fit()`메서드와 사용법이 같지만 호출할 떄마다 1 에포크씩 이어서 훈련할 수 있습니다.
- `partial_fit()` 메서드를 호출하고 다시 훈련 세트와 테스트 세트의 점수를 확인합니다.

```
0.8151260504201681
0.85
```

- 아직 점수가 낮지만 에포크를 한번 더 실행하니 정확도가 향상되었습니다. 이 모델을 여러 에포크에서 더 훈련해 볼 필요가 있습니다. 즉 어떤 기준에 의해서 반복하여 훈련합니다.

## 에포크와 과대/과소접합

- 확률적 경사 하강법을 사용한 모델은 에포크 횟수에 따라 과소적합이나 과대적합이 될 수 있습니다..
- 에포크가 횟수가 적으면 모델이 훈련 세트를 덜 학습합니다. 마치 산을 다 내려오지 못하고 훈련을 마치는 셈
- 에포크 횟수가 충분히 많으면 훈련 세트를 완전히 학습할 것입니다. 훈련 세트에 아주 잘 맞는 모델이 만들어집니다.
- 적은 에포크 횟수 동안에 훈련한 모델은 훈련 세트와 테스트 세트에 잘 맞지 않는 과소적합된 모델일 가능성이 높습니다.
- 반대로 많은 에포크 횟수 동안 훈련한 모델은 훈련 세트에 너무 잘 맞아 테스트 세트에는 오히려 점수가 나쁜 과대적합된 모델일 가능성이 높습니다.

![스크린샷 2024-11-04 오전 7 09 38](https://github.com/user-attachments/assets/0429290c-793b-4ff6-81ba-c1201266722c)

- 이 그래프는 에포크가 진행됨에 따라 모델의 정확도를 나타낸 것입니다. 훈련 세트 점수는 에포크가 진행될수록 꾸준히 증가하지만 테스트 세트 점수는 어느 순간 감소하기 시작합니다. 바로 이 지점이 모델이 과대적합되기 시작하는 곳입니다. 
- 과대적합이 시작하기 전에 훈련을 멈추는 것을 **조기 종료**(early stopping)라고 합니다. 그럼 준비한 데이터셋으로 위와 같은 그래프를 만들어 봅니다.
- 이 예제에서는 `fit()` 메서드를 사용하지 않고 `partial_fit()` 메서드만 사용합니다. `partial_fit()` 메서드만 사용하려면 훈련 세트에 있는 전체 클래스의 레이블을 `partial_fit()` 메서드에 전달해 주어야 합니다. 

```python
import numpy as np

sc = SGDClassifier(loss='log_loss', random_state=42)

train_score = []
test_score = []

classes = np.unique(train_target)
```

- 이를 위해 `np.unique()` 함수로 train_target에 있는 7개 생선 목록을 만듭니다.
- 에포크마다 훈련 세트와 테스트 세트에 대한 점수를 기록하기 위해 2개의 리스트를 준비합니다.  

```python
for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)

    train_score.append(sc.score(train_scaled, train_target))
    test_score.append(sc.score(test_scaled, test_target))
```

- 300번의 에포크 동안 훈련을 반복하여 진행해 보겠습니다. 
- 반복마다 훈련 세트와 테스트 세트의 점수를 계산하여 `train_score`, `test_score` 리스트에 추가

```python
import matplotlib.pyplot as plt

plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()
```

![스크린샷 2024-11-04 오전 7 23 17](https://github.com/user-attachments/assets/afdb26b1-11c0-44c7-927a-d7b3037f754a)



- 300번의 에포크 동안 기록한 훈련 세트와 테스트 세트의 점수를 그래프로 그려 봅니다.
- 데이터가 작기 때문에 아주 잘 드러나지는 않지만, 백 번째 에포크 이후에는 훈련 세트와 테스트 세트의 점수가 조금씩 벌어지고 있습니다.
- 확실히 에포크 초기에는 과소적합되어 훈련 세트와 테스트 세트의 점수가 낮습니다. 이 모델의 경우 백 번째 에포크가 적절한 반복 횟수로 보입니다.

```python
sc = SGDClassifier(loss='log_loss', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

- `SGDClassifier` 의 반복 횟수를 100에 맞추고 모델을 다시 훈련해 보겠습니다. 그리고 최종적으로 훈련 세트와 테스트 세트에서 점수를 출력합니다.

```
0.957983193277311
0.925
```

- `SGDClassifier` 는 일정 에포크 동안 성능이 향상되지 않으면 더 훈련하지 않고 자동으로 멈춥니다. 
- `tol` 매개변수에서 향상될 최소값을 지정합니다. 앞의 코드에서는 tol 매개변수를 None으로 지정하여 자동으로 멈추지 않고 max_iter=100만큼 무조건 반복하도록 하였습니다. 

> `SGDRegressor`가 확률적 경사 하강법을 사용한 회귀 알고리즘을 제공합니다. 사용하는 방법은 `SGDClassifier`와 동일합니다.

```python
sc = SGDClassifier(loss='hinge', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled, train_target)

print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))
```

- `loss` 매개변수의 기본값은 'hinge' 입니다. **힌지 손실**(hinge loss)은 **서포트 벡터 머신**(support vector machine)이라고 불리는 또 다른 머신러닝 알고리즘을 위한 손실 함수입니다.

```
0.9495798319327731
0.925
```
