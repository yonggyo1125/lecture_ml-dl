# 순차 데이터와 순환 신경망

## 키워드 정리

- **순차 데이터** : 텍스트나 시계열 데이터와 같이 순서에 의미가 있는 데이터입니다. 대표적인 순차 데이터로는 글, 대화, 일자별 날씨, 일자별 판매 실적 등을 예로 들 수 있습니다.
- **순환 신경망** : 순차 데이터에 잘 맞는 인공 신경망의 한 종류입니다. 순차 데이터를 처리하기 위해 고안된 순환층을 1개 이상 사용한 신경망을 순환 신경망이라고 부릅니다.
- 순환 신경망에서는 종종 순환층을 **셀**이라고 부릅니다. 일반적인 인공 신경망과 마찬가지로 하나의 셀은 여러 개의 뉴런으로 구성됩니다.
- 순환 신경망에서는 셀의 출력을 특별히 **은닉 상태**라고 부릅니다. 은닉 상태는 다음 층으로 전달될 뿐만 아니라 셀이 다음 타임스텝의 데이터를 처리할 때 재사용됩니다.

## 순차 데이터

- **순차 데이터**<sup>sequential data</sup>는 텍스트나 **시계열 데이터**<sup>time series data</sup>와 같이 순서에 의미가 있는 데이터를 말합니다. 예를 들어 "I am a boy"는 쉽게 이해할 수 있지만 "boy am a I"는 말이 되지 않습니다. 또 일별 온도를 기록한 데이터에서 날짜 순서를 뒤죽박죽 섞는다면 내일의 온도를 쉽게 예상하기 어렵습니다.

![스크린샷 2025-03-18 오후 8 01 03](https://github.com/user-attachments/assets/6a00cd08-736b-4adc-8ea1-be97ea4a5d99)

- 지금까지 우리가 보았던 데이터는 순서와 상관이 없었습니다. 예로 패션 MNIST 데이터로 생각해 보죠. 이 데이터를 신경망 모델에 전달할 때 샘플을 랜덤하게 섞은 후 훈련 세트로 나누었습니다. 즉 샘플의 순서와 전혀 상관이 없었죠. 심지어 골고루 섞는 편이 결과가 더 좋았습니다.
- 이는 딥러닝뿐만 아니라 일반적인 머신러닝 모델에서도 마찬가지입니다. 4장에서 봤던 생선 데이터나 패션 MNIST 데이터는 어떤 샘플이 먼저 주입되어도 모델의 학습에 큰 영향을 미치지 않습니다.

![스크린샷 2025-03-18 오후 8 01 10](https://github.com/user-attachments/assets/d6ba648a-7855-4f82-8f27-cc44f7de760b)

- 이 장에서 사용하는 댓글, 즉 텍스트 데이터는 단어의 순서가 중요한 순차 데이터입니다. 이런 데이터는 순서를 유지하며 신경망에 주입해야 합니다. 단어의 순서를 마구 섞어서 주입하면 안 됩니다.
- 따라서 순차 데이터를 다룰 때는 이전에 입력한 데이터를 기억하는 기능이 필요합니다. 예를 들어 "별로지만 추천해요"에서 "추천해요"가 입력될 때 "별로지만"을 기억하고 있어야 이 댓글을 무조건 긍정적이라고 판단하지 않을 것입니다.

> 시계열 데이터는 일정한 시간 간격으로 기록된 데이터를 말합니다.

- 완전 연결 신경망이나 합성곱 신경망은 이런 기억 장치가 없습니다. 하나의 샘플(또는 하나의 배치)을 사용하여 정방향 계산을 수행하고 나면 그 샘플은 버려지고 다음 샘플을 처리할 떄 재사용하지 않습니다.

![스크린샷 2025-03-18 오후 8 06 40](https://github.com/user-attachments/assets/80746ab8-a292-4878-b799-b6f0f3adc510)

- 이렇게 입력 데이터의 흐름이 앞으로만 전달되는 신경망을 **피드포워드 신경망**<sup>feedforward neural network, FFNN</sup>이라고 합니다. 이전 장에서 배웠던 완전 연결 신경망과 합성곱 신경망이 모두 피드포워드 신경망에 속합니다.

![스크린샷 2025-03-18 오후 8 06 50](https://github.com/user-attachments/assets/a4a06f5b-a224-46ea-afeb-efdcc5d58de7)

- 신경망이 이전에 처리했던 샘플을 다음 샘플을 처리하는데 재사용하기 위해서는 이렇게 데이터의 흐름이 앞으로만 전달되어서는 곤란합니다. 다음 샘플을 위해서 이전 데이터가 신경망 층에 순환될 필요가 있죠. 이런 신경망이 바로 순환 신경망입니다.

## 순환 신경망

- **순환 신경망**<sup>recurrent neural network, RNN</sup>은 일반적인 완전 연결 신경망과 거의 비슷합니다. 완전 연결 신경망에 이전 데이터의 처리 흐름을 순환하는 고리 하나만 추가하면 됩니다. 다음 그림에서 은닉층에 있는 붉은 고리를 눈여겨보세요.

![스크린샷 2025-03-18 오후 8 12 31](https://github.com/user-attachments/assets/35f4ce28-4db9-4bc9-a09d-62dbb35883f1)

- 뉴런의 출력이 다시 자기 자신으로 전달됩니다. 즉 어떤 샘플을 처리할 떄 바로 이전에 사용했던 데이터를 재사용하는 셈이죠. 조금 더 간단한 예를 들어 설명해 보겠습니다. 다음처럼 A, B, C, 3개의 샘플을 처리하는 순환 신경망의 뉴런이 있다고 가정해 보죠. O는 출력된 결과입니다. 첫 번째 샘플 A를 처리하고 난 출력(O<sub>A</sub>)이 다시 뉴런으로 들어갑니다.

![스크린샷 2025-03-18 오후 8 12 38](https://github.com/user-attachments/assets/cf30b427-e307-4a3d-bc26-61817c9ce154)

- 이 출력에는 A에 대한 정보가 다분히 들어 있겠죠. 그 다음 B를 처리할 때 앞에서 A를 사용해 만든 출력 O<sub>A</sub>를 함께 사용합니다.

![스크린샷 2025-03-18 오후 8 12 44](https://github.com/user-attachments/assets/710eb0a8-c172-4e84-9b03-e1a093739670)

- 따라서 O<sub>A</sub>와 B를 사용해서 만든 O<sub>B</sub>에는 A에 대한 정보가 어느 정도 포함되어 있을 것입니다. 그다음 C를 처리할 떄는 O<sub>B</sub>를 함께 사용합니다.

![스크린샷 2025-03-18 오후 8 17 29](https://github.com/user-attachments/assets/88590dad-4be3-42e2-9c23-be4ddea0f729)

- O<sub>B</sub>와 C를 사용해 만든 O<sub>c</sub>에는 어떤 정보들이 포함되어 있을까요? O<sub>B</sub>를 사용했으므로 당연히 B에 대한 정보가 어느정도 포함되어 있을 것입니다. 또 O<sub>B</sub>에는 A에 대한 정보도 포함되어 있습니다. 따라서 O<sub>C</sub>에 B와 A에 대한 정보가 담겨 있다고 말할 수 있습니다.
- 물론 O<sub>C</sub>에는 A에 대한 정보보다는 B에 대한 정보가 더 많을 것입니다. 그래서 순환 신경망에서는 '이전 샘플에 대한 기억을 가지고 있다'고 종종 말합니다. 이렇게 샘플을 처리하는 한 단계를 **타임스텝**<sup>timestep</sup>이라고 말합니다.
- 순환 신경망은 이전 타임스텝의 샘플을 기억하지만 타임스텝이 오래될수록 순환되는 정보는 희미해집니다. 나중에 여기에 대해서 다시 자세히 언급하겠습니다.
- 순환 신경망에서는 특별히 층을 **셀**<sup>cell</sup>이라고 부릅니다. 한 셀에는 여러 개의 뉴런이 있지만 완전 연결 신경망과 달리 뉴런을 모두 표시하지 않고 하나의 셀로 층을 표현합니다. 또 셀의 출력을 **은닉 상태**<sup>hidden state</sup>라고 부릅니다.
- 합성곱 신경망에서처럼 신경망의 구조마다 조금씩 부르는 이름이 다를 수 있습니다. 하지만 기본 구조는 같습니다. 입력에 어떤 가중치를 곱하고 활성화 함수를 통과시켜 다음 층으로 보내는 거죠. 달라지는 것은 층의 출력(즉, 은닉 상태)을 다음 타임 스텝에 재사용한다는 것뿐입니다.

![스크린샷 2025-03-18 오후 8 17 54](https://github.com/user-attachments/assets/073f3365-c966-4ebb-a44a-3d919077f437)

- 일반적으로 은닉층의 활성화 함수로는 하이퍼볼릭 탄젠트<sup>hyperbolic tangent</sup>함수인 tanh<sup>2</sup>가 많이 사용됩니다. tanh 함수도 S자 모양을 띠기 때문에 종종 시그모이드 함수라고 부르기도 합니다. 하지만 헷갈릴 수 있으니 여기에서는 이렇게 부르지 않겠습니다. tanh 함수는 시그모이드 함수와는 달리 -1\~1 사이의 범위를 가집니다.

![스크린샷 2025-03-18 오후 8 26 40](https://github.com/user-attachments/assets/5e71ec43-2555-4180-bb9b-c16aecfa3184)

- 다른 신경망과 마찬가지로 순환 신경망 그림에도 번거로움을 피하기 위해 활성화 함수를 표시하지 않는 경우가 많습니다. 하지만 순환 신경망에도 활성화 함수가 반드시 필요하다는 것을 꼭 기억해 주세요.
- 합성곱 신경망과 같은 피드포워드 신경망에서 뉴런은 입력과 가중치를 곱합니다. 순환 신경망에서도 동일합니다. 다만 순환 신경망의 뉴런은 가중치가 하나 더 있습니다. 바로 이전 타임스텝의 은닉 상태에 곱해지는 가중치입니다. 셀은 입력과 이전 타임스텝의 은닉 상태를 사용하여 현재 타임스텝의 은닉 상태를 만듭니다.
- 다음 그림에서 2개의 가중치를 셀 안에 구분해서 표시했습니다. W<sub>x</sub>는 입력에 곱해지는 가중치이고 W<sub>h</sub>는 이전 타임스텝의 은닉 상태에 곱해지는 가중치입니다. 피드포워드 신경망과 마찬가지로 뉴런마다 하나의 절편이 포함됩니다. 하지만 여기에서는 따로 표시하지 않겠습니다.

![스크린샷 2025-03-18 오후 8 30 43](https://github.com/user-attachments/assets/88085c5a-c90f-4b6a-ac1a-23e57db5db08)

- 이 그림을 조금 변형해서 그려보죠. 셀의 출력(은닉 상태)이 다음 타임스텝에 재사용되기 떄문에 타임스텝으로 셸을 나누어 그릴 수 있습니다. 다음 그림처럼 순환 신경망을 타임스텝마다 그릴 수 있는데요, 이런 그림을 보고 셀을 타임스텝으로 펼쳤다고 말합니다.

![스크린샷 2025-03-18 오후 8 30 50](https://github.com/user-attachments/assets/54a27f27-ae61-4eab-a915-78928ce8bbf5)

- 타임스텝 1에서 셀의 출력 h<sub>1</sub>이 타임스텝 2의 셀로 주입됩니다. 이때 W<sub>h</sub>와 곱해집니다. 마찬가지로 타임스텝 2에 셀의 출력 h<sub>2</sub>가 타임스텝 3의 셀로 주입됩니다. 이때에도 W<sub>h</sub>와 곱해집니다.
- 여기에서 알 수 있는 것은 모든 타임스텝에서 사용되는 가중치는 W<sub>h</sub> 하나라는 점입니다. 가중치 W<sub>h</sub>는 타임스텝에 따라 변화되는 뉴런의 출력을 학습합니다. 이런 능력이 이 절의 시작 부분에 언급했던 순차 데이터를 다루는 데 필요합니다.
- 그럼 맨 처음 타임스텝 1에서 사용되는 이전 은닉 상태 h<sub>0</sub>은 어떻게 구할 수 있을까요? 맨 처음 샘플을 입력할 때는 이전 타임스텝이 없습니다. 따라서 간단히 h<sub>0</sub>은 모두 0으로 초기화합니다.

## 셀의 가중치와 입출력

- 순환 신경망의 셀에서 필요한 가중치 크기를 계산해 보겠습니다. 복잡한 모델을 배울수록 가중치 개수를 계산해 보면 잘 이해하고 있는지 알 수 있습니다. 예를 들어 다음 그림처럼 순환층에 입력되는 특성의 개수가 4개이고 순환층의 뉴런이 3개라고 가정해 보겠습니다.
- 먼저 W<sub>x</sub>의 크기를 구해봅시다. 입력층과 순환층의 뉴런이 모두 완전 연결되기 떄문에 가중치 W<sub>x</sub>의 크기는 4 X 3 = 12개가 됩니다. 7장에서 본 완전 연결 신경망의 입력층과 은닉층의 연결과 같죠.

![스크린샷 2025-03-18 오후 8 39 22](https://github.com/user-attachments/assets/cb842f5d-e2cb-4887-9ec5-e42030029140)

- 그럼 순환층에서 다음 타임스텝에 재사용되는 은닉 상태를 위한 가중치 W<sub>h</sub>의 크기는 어떻게 될까요?
- 다음 그림을 먼저 보겠습니다.

![스크린샷 2025-03-18 오후 8 39 28](https://github.com/user-attachments/assets/038e3c70-7598-43e5-a9ab-a1417c2531f8)

- 순환층에 있는 첫 번째 뉴련(r<sub>1</sub>)의 은닉 상태가 다음 타임스텝에 재사용될 떄 첫 번째 뉴런과 두 번쨰 뉴런, 세 번째 뉴런에 모두 전달됩니다. 위 그림에서 붉은색으로 표시했습니다. 즉 이전 타임스텝의 은닉 상태는 다음 타임스텝의 뉴런에 완전히 연결됩니다.
- 두 번째 뉴런의 은닉 상태도 마찬가지로 첫 번째 뉴런과 두 번쨰 뉴런, 세 번째 뉴런에 모두 전달되고(파란 화살표). 세 번째 뉴런의 은닉 상태도 동일합니다(검은 화살표). 따라서 이 순환층에서 은닉 상태를 위한 가중치 W<sub>h</sub>는 3 X 3 = 9개 입니다.
- 가중치는 모두 구했으니 모델 파라미터 개수를 계산해볼까요? 가중치에 절편을 더하면 되죠. 여기엔 각 뉴런마다 하나의 절편이 있습니다. 따라서 이 순환층은 모두 12 + 9 + 3 = 24개의 모델 파라미터를 가지고 있습니다. 이제 왜 순환층을 셀 하나로 표시할 수밖에 없는지 이해되셨나요? 은닉 상태가 모든 뉴런에 순환되기 떄문에 완전 연결 신경망처럼 그림으로 표현하기는 너무 어렵습니다.

![스크린샷 2025-03-18 오후 8 45 51](https://github.com/user-attachments/assets/2425cfd2-983d-45e4-b72e-3904563de56e)

- 순환층의 가중치 크기를 알아보았으므로 이번에는 순환층의 입력과 출력에 대해 생각해 보죠. 이전장에서 배웠던 합성곱 층의 입력은 전형적으로 하나의 샘플이 3개의 차원을 가집니다. 너비, 높이, 채널입니다. 입력이 합성곱 층과 풀링 층을 통과하면 너비, 높이, 채널(혹은 깊이)의 크기가 달라지지만 차원의 개수는 그대로 유지되었습니다.
- 순환층은 일반적으로 샘플마다 2개의 차원을 가집니다. 보통 하나의 샘플을 하나의 시퀀스<sup>sequence</sup>라고 말합니다. 시퀀스 안에는 여러 개의 아이템이 들어 있습니다. 여기에서 시퀀스의 길이가 바로 타임스텝 길이가 됩니다.
- 예를 들어 어떤 샘플에 "I am a boy"란 문장이 들어 있다고 가정해 보죠. 이 샘플은 4개의 단어로 이루어져 있습니다. 각 단어를 3개의 어떤 숫자로 표현한다고 가정해 봅시다(이 숫자 표현에 대해서는 다음 절에 자세히 다루겠습니다).

![스크린샷 2025-03-18 오후 8 46 01](https://github.com/user-attachments/assets/7d4b2737-d0d2-4496-9f04-a14144ac9e94)

- 이런 입력이 순환층을 통과하면 두 번쨰, 세 번째 차원이 사라지고 순환층의 뉴런 개수만틈 출력됩니다. 이를 차근차근 설명해 보겠습니다. 먼저 방금 한 설명을 그림으로 나타내어 보겠습니다.

![스크린샷 2025-03-18 오후 8 46 07](https://github.com/user-attachments/assets/e15204f1-e2ff-4c7d-9519-edf1340a2729)

- 하나의 샘플은 시퀀스 길이(여기에서는 단어 개수)와 단어 표현(다음 절에서 자세히 다룹니다)의 2차원 배열입니다. 순환층을 통과하면 1차원 배열로 바뀝니다. 이 1차원 배열의 크기는 순환층의 뉴런 개수에 의해 결정됩니다.
- 혹시 이 설명이 의아하게 느껴지지 않나요? 앞에서 셀의 출력을 설명할 때 빠르린 것이 있기 때문입니다. 앞에서는 셀이 모든 타임스텝에서 출력을 만든 것 처럼 표현했습니다. 사실 순환층은 기본적으로 마지막 타임스텝의 은닉 상태만 출력으로 내보냅니다.
- 다음 그림에서 이런 특징을 그림으로 표현하기 위해 셀의 출력을 점선으로 표시했습니다. 또 마지막 타임스텝의 은닉 상태임을 나타내기 위해 아랫첨자 f를 사용했습니다.

![스크린샷 2025-03-18 오후 8 58 17](https://github.com/user-attachments/assets/e4928f7f-c417-40f7-8074-b724659755ad)

- 이는 마치 입력된 시퀀스 길이를 모두 읽어서 정보를 마지막 은닉 상태에 압축하여 전달하는 것처럼 볼 수 있습니다. 이제 순환 신경망이 정보를 기억하는 메모리를 가진다고 표현하는지 이해할 수 있습니다. 또 순환 신경망이 순차 데이터에 잘 맞는 이유를 파악할 수 있습니다.
- 순환 신경망도 완전 연결 신경망이나 합성곱 신경망처럼 여러 개의 층을 쌓을 수 있습니다. 순환층을 여러 개 쌓았을 때는 셀의 출력은 어떻게 달라질까요? 셀의 입력은 샘플마다 타임스텝과 단어 표현으로 이루어진 2차원 배열이어야 합니다. 따라서 첫 번째 셀이 마지막 타임스텝의 은닉 상태만 출력해서는 안 되겠죠. 이런 경우에는 마지막 셀을 제외한 다른 모든 셀은 모든 타임스텝의 은닉 상태를 출력합니다. 예를 들어 2개의 순환층을 쌓은 경우에는 다음 그림과 같습니다.

![스크린샷 2025-03-18 오후 8 58 24](https://github.com/user-attachments/assets/b5ab5310-6545-4fb6-800d-ac0860a7437c)

- 첫 번째 셀은 모든 타임스텝의 은닉 상태를 출력하고, 두 번째 셀은 마지막 타임스텝의 은닉 상태만 출력합니다. 다음 절에서 이런 예를 직접 다루어 보겠습니다.
- 마지막으로 출력층의 구성에 대해 알아보겠습니다. 합성곱 신경망과 마찬가지로 순환 신경망도 마지막에는 밀집층을 두어 클래스를 분류합니다. 다중 분류일 경우에는 출력층에 클래스 개수만큼 뉴런을 두고 소프트맥스 활성화 함수를 사용합니다. 이진 분류일 경우에는 하나의 뉴런을 두고 시그모이드 활성화 함수를 사용합니다.
- 합성곱 신경망과 다른 점은 마지막 셀의 출력이 1차원이기 때문에 `Flatten` 클래스로 펼칠 필요가 없습니다. 셀의 출력을 그대로 밀집층에 사용할 수 있죠. 예를 들어 다중 분류 문제에서 입력 샘플의 크기가 (20, 100)일 경우 하나의 순환층을 사용하는 순환 신경망의 구조는 다음과 같이 나타낼 수 있습니다.

![스크린샷 2025-03-18 오후 9 05 56](https://github.com/user-attachments/assets/4cfb327e-252b-47d0-b02b-caf843c25dee)

- 이 예에서 샘플은 20개의 타임스텝으로 이루어져 있습니다. 또 각 타임스텝은 100개의 표현 또는 특성으로 이루어져 있죠. 이 샘플이 순환층의 셀을 통과하면 모든 타임스텝을 처리하고 난 후의 은닉상태만 출력됩니다. 이 은닉 상태의 크기는 셀에 있는 뉴런의 개수가 되므로 (10,) 입니다.
- 샘플마다 셀이 1차원 배열을 출력하기 떄문에 합성곱 신경망처럼 `Flatten` 클래스로 펼칠 필요 없이 바로 출력층에 연결할 수 있습니다. 앞의 그림은 3개의 클래스를 가진 다중 분류일 경우를 위해 출력층에서 3개의 뉴런과 소프트맥스 활성화 함수를 사용한 예입니다.
