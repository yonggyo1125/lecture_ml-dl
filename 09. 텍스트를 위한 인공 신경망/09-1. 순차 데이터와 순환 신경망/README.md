# 순차 데이터와 순환 신경망

## 키워드 정리

- **순차 데이터** : 텍스트나 시계열 데이터와 같이 순서에 의미가 있는 데이터입니다. 대표적인 순차 데이터로는 글, 대화, 일자별 날씨, 일자별 판매 실적 등을 예로 들 수 있습니다.
- **순환 신경망** : 순차 데이터에 잘 맞는 인공 신경망의 한 종류입니다. 순차 데이터를 처리하기 위해 고안된 순환층을 1개 이상 사용한 신경망을 순환 신경망이라고 부릅니다.
- 순환 신경망에서는 종종 순환층을 **셀**이라고 부릅니다. 일반적인 인공 신경망과 마찬가지로 하나의 셀은 여러 개의 뉴런으로 구성됩니다.
- 순환 신경망에서는 셀의 출력을 특별히 **은닉 상태**라고 부릅니다. 은닉 상태는 다음 층으로 전달될 뿐만 아니라 셀이 다음 타임스텝의 데이터를 처리할 때 재사용됩니다.

## 순차 데이터

- **순차 데이터**<sup>sequential data</sup>는 텍스트나 **시계열 데이터**<sup>time series data</sup>와 같이 순서에 의미가 있는 데이터를 말합니다. 예를 들어 "I am a boy"는 쉽게 이해할 수 있지만 "boy am a I"는 말이 되지 않습니다. 또 일별 온도를 기록한 데이터에서 날짜 순서를 뒤죽박죽 섞는다면 내일의 온도를 쉽게 예상하기 어렵습니다.

![스크린샷 2025-03-18 오후 8 01 03](https://github.com/user-attachments/assets/6a00cd08-736b-4adc-8ea1-be97ea4a5d99)

- 지금까지 우리가 보았던 데이터는 순서와 상관이 없었습니다. 예로 패션 MNIST 데이터로 생각해 보죠. 이 데이터를 신경망 모델에 전달할 때 샘플을 랜덤하게 섞은 후 훈련 세트로 나누었습니다. 즉 샘플의 순서와 전혀 상관이 없었죠. 심지어 골고루 섞는 편이 결과가 더 좋았습니다.
- 이는 딥러닝뿐만 아니라 일반적인 머신러닝 모델에서도 마찬가지입니다. 4장에서 봤던 생선 데이터나 패션 MNIST 데이터는 어떤 샘플이 먼저 주입되어도 모델의 학습에 큰 영향을 미치지 않습니다.

![스크린샷 2025-03-18 오후 8 01 10](https://github.com/user-attachments/assets/d6ba648a-7855-4f82-8f27-cc44f7de760b)

- 이 장에서 사용하는 댓글, 즉 텍스트 데이터는 단어의 순서가 중요한 순차 데이터입니다. 이런 데이터는 순서를 유지하며 신경망에 주입해야 합니다. 단어의 순서를 마구 섞어서 주입하면 안 됩니다.
- 따라서 순차 데이터를 다룰 때는 이전에 입력한 데이터를 기억하는 기능이 필요합니다. 예를 들어 "별로지만 추천해요"에서 "추천해요"가 입력될 때 "별로지만"을 기억하고 있어야 이 댓글을 무조건 긍정적이라고 판단하지 않을 것입니다.

> 시계열 데이터는 일정한 시간 간격으로 기록된 데이터를 말합니다.

- 완전 연결 신경망이나 합성곱 신경망은 이런 기억 장치가 없습니다. 하나의 샘플(또는 하나의 배치)을 사용하여 정방향 계산을 수행하고 나면 그 샘플은 버려지고 다음 샘플을 처리할 떄 재사용하지 않습니다.

![스크린샷 2025-03-18 오후 8 06 40](https://github.com/user-attachments/assets/80746ab8-a292-4878-b799-b6f0f3adc510)

- 이렇게 입력 데이터의 흐름이 앞으로만 전달되는 신경망을 **피드포워드 신경망**<sup>feedforward neural network, FFNN</sup>이라고 합니다. 이전 장에서 배웠던 완전 연결 신경망과 합성곱 신경망이 모두 피드포워드 신경망에 속합니다.

![스크린샷 2025-03-18 오후 8 06 50](https://github.com/user-attachments/assets/a4a06f5b-a224-46ea-afeb-efdcc5d58de7)

- 신경망이 이전에 처리했던 샘플을 다음 샘플을 처리하는데 재사용하기 위해서는 이렇게 데이터의 흐름이 앞으로만 전달되어서는 곤란합니다. 다음 샘플을 위해서 이전 데이터가 신경망 층에 순환될 필요가 있죠. 이런 신경망이 바로 순환 신경망입니다.

## 순환 신경망

- **순환 신경망**<sup>recurrent neural network, RNN</sup>은 일반적인 완전 연결 신경망과 거의 비슷합니다. 완전 연결 신경망에 이전 데이터의 처리 흐름을 순환하는 고리 하나만 추가하면 됩니다. 다음 그림에서 은닉층에 있는 붉은 고리를 눈여겨보세요.

![스크린샷 2025-03-18 오후 8 12 31](https://github.com/user-attachments/assets/35f4ce28-4db9-4bc9-a09d-62dbb35883f1)

- 뉴런의 출력이 다시 자기 자신으로 전달됩니다. 즉 어떤 샘플을 처리할 떄 바로 이전에 사용했던 데이터를 재사용하는 셈이죠. 조금 더 간단한 예를 들어 설명해 보겠습니다. 다음처럼 A, B, C, 3개의 샘플을 처리하는 순환 신경망의 뉴런이 있다고 가정해 보죠. O는 출력된 결과입니다. 첫 번째 샘플 A를 처리하고 난 출력(O<sub>A</sub>)이 다시 뉴런으로 들어갑니다.

![스크린샷 2025-03-18 오후 8 12 38](https://github.com/user-attachments/assets/cf30b427-e307-4a3d-bc26-61817c9ce154)

- 이 출력에는 A에 대한 정보가 다분히 들어 있겠죠. 그 다음 B를 처리할 때 앞에서 A를 사용해 만든 출력 O<sub>A</sub>를 함께 사용합니다.

![스크린샷 2025-03-18 오후 8 12 44](https://github.com/user-attachments/assets/710eb0a8-c172-4e84-9b03-e1a093739670)

- 따라서 O<sub>A</sub>와 B를 사용해서 만든 O<sub>B</sub>에는 A에 대한 정보가 어느 정도 포함되어 있을 것입니다. 그다음 C를 처리할 떄는 O<sub>B</sub>를 함께 사용합니다.

![스크린샷 2025-03-18 오후 8 17 29](https://github.com/user-attachments/assets/88590dad-4be3-42e2-9c23-be4ddea0f729)

- O<sub>B</sub>와 C를 사용해 만든 O<sub>c</sub>에는 어떤 정보들이 포함되어 있을까요? O<sub>B</sub>를 사용했으므로 당연히 B에 대한 정보가 어느정도 포함되어 있을 것입니다. 또 O<sub>B</sub>에는 A에 대한 정보도 포함되어 있습니다. 따라서 O<sub>C</sub>에 B와 A에 대한 정보가 담겨 있다고 말할 수 있습니다.
- 물론 O<sub>C</sub>에는 A에 대한 정보보다는 B에 대한 정보가 더 많을 것입니다. 그래서 순환 신경망에서는 '이전 샘플에 대한 기억을 가지고 있다'고 종종 말합니다. 이렇게 샘플을 처리하는 한 단계를 **타임스텝**<sup>timestep</sup>이라고 말합니다.
- 순환 신경망은 이전 타임스텝의 샘플을 기억하지만 타임스텝이 오래될수록 순환되는 정보는 희미해집니다. 나중에 여기에 대해서 다시 자세히 언급하겠습니다.
- 순환 신경망에서는 특별히 층을 **셀**<sup>cell</sup>이라고 부릅니다. 한 셀에는 여러 개의 뉴런이 있지만 완전 연결 신경망과 달리 뉴런을 모두 표시하지 않고 하나의 셀로 층을 표현합니다. 또 셀의 출력을 **은닉 상태**<sup>hidden state</sup>라고 부릅니다.
- 합성곱 신경망에서처럼 신경망의 구조마다 조금씩 부르는 이름이 다를 수 있습니다. 하지만 기본 구조는 같습니다. 입력에 어떤 가중치를 곱하고 활성화 함수를 통과시켜 다음 층으로 보내는 거죠. 달라지는 것은 층의 출력(즉, 은닉 상태)을 다음 타임 스텝에 재사용한다는 것뿐입니다.

![스크린샷 2025-03-18 오후 8 17 54](https://github.com/user-attachments/assets/073f3365-c966-4ebb-a44a-3d919077f437)

- 일반적으로 은닉층의 활성화 함수로는 하이퍼볼릭 탄젠트<sup>hyperbolic tangent</sup>함수인 tanh<sup>2</sup>가 많이 사용됩니다. tanh 함수도 S자 모양을 띠기 때문에 종종 시그모이드 함수라고 부르기도 합니다. 하지만 헷갈릴 수 있으니 여기에서는 이렇게 부르지 않겠습니다. tanh 함수는 시그모이드 함수와는 달리 -1\~1 사이의 범위를 가집니다.

